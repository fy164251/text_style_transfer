{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fy164251/text_style_transfer/blob/master/RPS_AWS_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6o7cfFrJqOn"
   },
   "outputs": [],
   "source": [
    "# AWS notebook v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHn_26BHO5C5"
   },
   "source": [
    "This jupyter sheet is an AWS-based RPS generator. \n",
    "For it to work, runtime has to be supported by GPU with suffiicent memory (does not happen often on free tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyVYZnZW7JVo"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @kernel-restart\n",
    "# Ayne Rand \"Atlas shrugged\" short 1\n",
    "\n",
    "input = \"\"\"She sat at the window of the train, her head thrown back, one leg stretched across to the empty seat\n",
    "before her. The window frame trembled with the speed of the motion, the pane hung over empty\n",
    "darkness, and dots of light slashed across the glass as luminous streaks, once in a while.\n",
    "\n",
    "Her leg, sculptured by the tight sheen of the stocking, its long line running straight, over an arched\n",
    "instep, to the tip of a foot in a high-heeled pump, had a feminine elegance that seemed out of place in\n",
    "the dusty train car and oddly incongruous with the rest of her. She wore a battered camel's hair coat\n",
    "that had been expensive, wrapped shapelessly about her slender, nervous body. The coat collar was\n",
    "raised to the slanting brim of her hat. A sweep of brown hair fell back, almost touching the line of her\n",
    "shoulders. Her face was made of angular planes, the shape of her mouth clear-cut, a sensual mouth\n",
    "held closed with inflexible precision. She kept her hands in the coat pockets, her posture taut, as if\n",
    "she resented immobility, and unfeminine, as if she were unconscious of her own body and that it was\n",
    "a woman's body. She sat listening to the music. It was a symphony of triumph. The notes flowed up,\n",
    "they spoke of rising and they were the rising itself, they were the essence and the form of upward\n",
    "motion, they seemed to embody every human act and thought that had ascent as its motive. It was a\n",
    "sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the\n",
    "tension of purpose. It swept space clean, and left nothing but the joy of an unobstructed effort. Only a\n",
    "faint echo within the sounds spoke of that from which the music had escaped, but spoke in laughing\n",
    "astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was\n",
    "the song of an immense deliverance.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ayn Rand short 2\n",
    "\n",
    "input = \"\"\"Man, at his head might, is known and fulfilled within aside maker\n",
    "himself. Whether the creator is only, or finds but a little of others like him,\n",
    "or is among the favor of mankind, is of not enchantment or say whatever;\n",
    "many have nothing to range with it. He alone or he and a small others thus him are\n",
    "mankind, in the even scope of head the strong of what one actually is, heat at his\n",
    "utmost, the occasion his, his at his good possibility.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare Sonnet II\n",
    "\n",
    "input = \"\"\"When forty winters shall beseige thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery, so gazed on now,\n",
    "Will be a tatter'd weed, of small worth held:\n",
    "Then being ask'd where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days,\n",
    "To say, within thine own deep-sunken eyes,\n",
    "Were an all-eating shame and thriftless praise.\n",
    "How much more praise deserved thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twain \"The Adventures of Huckleberry Finn\"\n",
    "\n",
    "input = \"\"\"At first I hated the school, but by and by I got so I could stand it.\n",
    "Whenever I got uncommon tired I played hookey, and the hiding I got next\n",
    "day done me good and cheered me up.  So the longer I went to school the\n",
    "easier it got to be.  I was getting sort of used to the widow's ways,\n",
    "too, and they warn't so raspy on me.  Living in a house and sleeping in\n",
    "a bed pulled on me pretty tight mostly, but before the cold weather I\n",
    "used to slide out and sleep in the woods sometimes, and so that was a\n",
    "rest to me.  I liked the old ways best, but I was getting so I liked the\n",
    "new ones, too, a little bit. The widow said I was coming along slow but\n",
    "sure, and doing very satisfactory.  She said she warn't ashamed of me. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twain \"A CONNECTICUT YANKEE IN KING ARTHUR'S COURT\"\n",
    "\n",
    "input = \"\"\"The question as to whether there is such a thing as divine right\n",
    "of kings is not settled in this book.  It was found too difficult.\n",
    "That the executive head of a nation should be a person of lofty\n",
    "character and extraordinary ability, was manifest and indisputable;\n",
    "that none but the Deity could select that head unerringly, was\n",
    "also manifest and indisputable; that the Deity ought to make that\n",
    "selection, then, was likewise manifest and indisputable; consequently,\n",
    "that He does make it, as claimed, was an unavoidable deduction.\n",
    "I mean, until the author of this book encountered the Pompadour,\n",
    "and Lady Castlemaine, and some other executive heads of that kind;\n",
    "these were found so difficult to work into the scheme, that it\n",
    "was judged better to take the other tack in this book (which\n",
    "must be issued this fall), and then go into training and settle\n",
    "the question in another book.  It is, of course, a thing which\n",
    "ought to be settled, and I am not going to have anything particular\n",
    "to do next winter anyway.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jane Austen\"Pride and Prejudice\"\n",
    "\n",
    "input = \"\"\"Mary had neither genius nor taste; and though vanity had given her\n",
    "application, it had given her likewise a pedantic air and conceited\n",
    "manner, which would have injured a higher degree of excellence than she\n",
    "had reached. Elizabeth, easy and unaffected, had been listened to with\n",
    "much more pleasure, though not playing half so well; and Mary, at the\n",
    "end of a long concerto, was glad to purchase praise and gratitude by\n",
    "Scotch and Irish airs, at the request of her younger sisters, who with\n",
    "some of the Lucases and two or three officers joined eagerly in dancing\n",
    "at one end of the room.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jane Austen\"Pride and Prejudice\"\n",
    "\n",
    "input = \"\"\"Elizabeth listened in silence, but was not convinced; their behaviour at\n",
    "the assembly had not been calculated to please in general; and with more\n",
    "quickness of observation and less pliancy of temper than her sister,\n",
    "and with a judgement too unassailed by any attention to herself, she\n",
    "was very little disposed to approve them. They were in fact very fine\n",
    "ladies; not deficient in good humour when they were pleased, nor in the\n",
    "power of making themselves agreeable when they chose it, but proud and\n",
    "conceited. They were rather handsome, had been educated in one of the\n",
    "first private seminaries in town, had a fortune of twenty thousand\n",
    "pounds, were in the habit of spending more than they ought, and of\n",
    "associating with people of rank, and were therefore in every respect\n",
    "entitled to think well of themselves, and meanly of others. They were of\n",
    "a respectable family in the north of England; a circumstance more deeply\n",
    "impressed on their memories than that their brother's fortune and their\n",
    "own had been acquired by trade.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumas\n",
    "\n",
    "input = \"\"\" In the meanwhile, Monsieur continued his route with an air at once so\n",
    "melancholy and so majestic, that he certainly would have attracted the\n",
    "attention of spectators, if spectators there had been; but the good\n",
    "citizens of Blois could not pardon Monsieur for having chosen their gay\n",
    "city for an abode in which to indulge melancholy at his ease, and as\n",
    "often as they caught a glimpse of the illustrious ennuye, they stole\n",
    "away gaping, or drew back their heads into the interior of their\n",
    "dwellings, to escape the soporific influence of that long pale face, of\n",
    "those watery eyes, and that languid address; so that the worthy prince\n",
    "was almost certain to find the streets deserted whenever he chanced to\n",
    "pass through them.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "I5JYUNmQIcTj",
    "outputId": "49b4eb18-b33c-41cc-e696-f4866f170d35"
   },
   "outputs": [],
   "source": [
    "# does not work on AWS!\n",
    "# upload your content file once\n",
    "\n",
    "from google.colab import files\n",
    "uploaded=files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "5YaJs3eY9FL5",
    "outputId": "7f24633a-9846-43d2-d450-9e3f5cef1ff3"
   },
   "outputs": [],
   "source": [
    "# one-time\n",
    "# install dependencies if needed \n",
    "!pip install sacremoses\n",
    "!pip install gpt-2-simple \n",
    "!pip install nltk\n",
    "!pip install tensorflow-gpu==1.14\n",
    "!pip install tensorflow-hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ztLigiKA9Tle",
    "outputId": "da0955e4-01fe-4575-feef-dc179f071406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "# @kernel restart\n",
    "import sys, getopt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from sacremoses import MosesDetokenizer\n",
    "import gpt_2_simple as gpt2\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bh5M0YtAbTLt",
    "outputId": "372e1752-ef7b-42df-dfec-0454da8467eb"
   },
   "outputs": [],
   "source": [
    "# one-time\n",
    "# download one-time tokenizer file\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5sSsJ7--ERt"
   },
   "outputs": [],
   "source": [
    "# @kernel restart\n",
    "# GPT-2 released three models\n",
    "#model_name=\"117M\"\n",
    "model_name=\"124M\"\n",
    "#model_name=\"345M\"\n",
    "\n",
    "# hyperparameters\n",
    "minsimilarity = 0.68 # rejection threshold\n",
    "# lexeme bounds\n",
    "minwords = 2         # min lexeme length\n",
    "softmaxwords = 4     # min lexeme ending in -and-\n",
    "maxwords = 7        # max lexeme without a clear end\n",
    "# generator parameters\n",
    "temperature = 1.0    # generator madness\n",
    "seedLexemes = 10     # how many lexemes to put in a prior\n",
    "nsamples = 10000      # number of samples generated to choose from\n",
    "\n",
    "\n",
    "bestLexemes = []\n",
    "output = \"\"\n",
    "\n",
    "# this is your style for GPT-2 finetuning\n",
    "style = \"Nabokov-All.txt\"\n",
    "# content input and output\n",
    "inputfile = 'Trained-1000'\n",
    "outputfile = inputfile+\"_\"+model_name+\"_\"+str(nsamples)+\"_\"+style\n",
    "statsfile = outputfile+\".stats\"\n",
    "\n",
    "# get around resource exhaustion for embeddings\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "        #device_count = {'GPU': 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "RCuLOVes-Jvo",
    "outputId": "9b0f226b-84ee-4131-cdd2-d77155e800fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# @kernel restart\n",
    "# init tokenizer and sentence embedder\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "dt = MosesDetokenizer()\n",
    "\n",
    "# Google Sentence encoder v2 appears worse than V1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
    "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "embed = hub.Module(module_url)\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "B3XQ_r2RD3MN",
    "outputId": "a5fd5aa0-8f24-4738-9491-0fd5e828f17e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 554Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 24.9Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 557Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 65.3Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 531Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 30.9Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 27.6Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "# download GPT-2 model (only needed once)\n",
    "gpt2.download_gpt2(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 24993
    },
    "colab_type": "code",
    "id": "UO6-wr7dGs67",
    "outputId": "6ed2870e-1b59-4ced-86c1-6fb0dff58a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-1\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 2363594 tokens\n",
      "Training...\n",
      "Saving checkpoint/run1/model-1\n",
      "[2 | 12.12] loss=3.74 avg=3.74\n",
      "[3 | 16.44] loss=4.19 avg=3.97\n",
      "[4 | 20.77] loss=4.08 avg=4.01\n",
      "[5 | 25.08] loss=3.80 avg=3.95\n",
      "[6 | 29.41] loss=3.83 avg=3.93\n",
      "[7 | 33.75] loss=4.21 avg=3.98\n",
      "[8 | 38.07] loss=3.86 avg=3.96\n",
      "[9 | 42.41] loss=3.77 avg=3.94\n",
      "[10 | 46.76] loss=3.98 avg=3.94\n",
      "[11 | 51.09] loss=3.87 avg=3.93\n",
      "[12 | 55.43] loss=3.72 avg=3.91\n",
      "[13 | 59.78] loss=3.76 avg=3.90\n",
      "[14 | 64.13] loss=3.73 avg=3.89\n",
      "[15 | 68.48] loss=3.75 avg=3.88\n",
      "[16 | 72.85] loss=3.89 avg=3.88\n",
      "[17 | 77.21] loss=3.61 avg=3.86\n",
      "[18 | 81.58] loss=4.09 avg=3.87\n",
      "[19 | 85.92] loss=3.79 avg=3.87\n",
      "[20 | 90.30] loss=3.64 avg=3.86\n",
      "[21 | 94.67] loss=3.82 avg=3.85\n",
      "[22 | 99.05] loss=4.06 avg=3.86\n",
      "[23 | 103.43] loss=3.86 avg=3.86\n",
      "[24 | 107.81] loss=3.96 avg=3.87\n",
      "[25 | 112.18] loss=3.77 avg=3.86\n",
      "[26 | 116.55] loss=4.19 avg=3.88\n",
      "[27 | 120.94] loss=3.56 avg=3.87\n",
      "[28 | 125.31] loss=3.78 avg=3.86\n",
      "[29 | 129.68] loss=3.72 avg=3.86\n",
      "[30 | 134.04] loss=3.79 avg=3.85\n",
      "[31 | 138.42] loss=3.86 avg=3.85\n",
      "[32 | 142.79] loss=3.80 avg=3.85\n",
      "[33 | 147.15] loss=3.89 avg=3.85\n",
      "[34 | 151.52] loss=3.85 avg=3.85\n",
      "[35 | 155.90] loss=3.74 avg=3.85\n",
      "[36 | 160.28] loss=3.85 avg=3.85\n",
      "[37 | 164.66] loss=3.70 avg=3.84\n",
      "[38 | 169.04] loss=3.87 avg=3.84\n",
      "[39 | 173.42] loss=3.43 avg=3.83\n",
      "[40 | 177.79] loss=3.65 avg=3.83\n",
      "[41 | 182.16] loss=3.77 avg=3.82\n",
      "[42 | 186.53] loss=3.80 avg=3.82\n",
      "[43 | 190.93] loss=3.69 avg=3.82\n",
      "[44 | 195.30] loss=3.55 avg=3.81\n",
      "[45 | 199.69] loss=3.52 avg=3.80\n",
      "[46 | 204.07] loss=3.90 avg=3.81\n",
      "[47 | 208.46] loss=3.53 avg=3.80\n",
      "[48 | 212.84] loss=3.88 avg=3.80\n",
      "[49 | 217.23] loss=3.45 avg=3.79\n",
      "[50 | 221.64] loss=3.54 avg=3.79\n",
      "[51 | 226.05] loss=3.48 avg=3.78\n",
      "[52 | 230.45] loss=3.77 avg=3.78\n",
      "[53 | 234.82] loss=3.54 avg=3.77\n",
      "[54 | 239.21] loss=3.64 avg=3.77\n",
      "[55 | 243.59] loss=3.64 avg=3.77\n",
      "[56 | 247.97] loss=3.40 avg=3.76\n",
      "[57 | 252.33] loss=4.16 avg=3.77\n",
      "[58 | 256.70] loss=3.88 avg=3.77\n",
      "[59 | 261.08] loss=4.09 avg=3.78\n",
      "[60 | 265.46] loss=3.30 avg=3.77\n",
      "[61 | 269.83] loss=3.74 avg=3.76\n",
      "[62 | 274.20] loss=3.75 avg=3.76\n",
      "[63 | 278.59] loss=4.00 avg=3.77\n",
      "[64 | 282.95] loss=3.88 avg=3.77\n",
      "[65 | 287.34] loss=3.56 avg=3.77\n",
      "[66 | 291.72] loss=3.52 avg=3.76\n",
      "[67 | 296.10] loss=3.70 avg=3.76\n",
      "[68 | 300.47] loss=4.04 avg=3.77\n",
      "[69 | 304.84] loss=3.78 avg=3.77\n",
      "[70 | 309.21] loss=3.42 avg=3.76\n",
      "[71 | 313.60] loss=3.63 avg=3.76\n",
      "[72 | 317.99] loss=3.75 avg=3.76\n",
      "[73 | 322.37] loss=3.84 avg=3.76\n",
      "[74 | 326.75] loss=3.78 avg=3.76\n",
      "[75 | 331.13] loss=3.74 avg=3.76\n",
      "[76 | 335.54] loss=3.62 avg=3.76\n",
      "[77 | 339.93] loss=3.78 avg=3.76\n",
      "[78 | 344.32] loss=3.74 avg=3.76\n",
      "[79 | 348.70] loss=3.69 avg=3.76\n",
      "[80 | 353.08] loss=3.94 avg=3.76\n",
      "[81 | 357.47] loss=3.49 avg=3.75\n",
      "[82 | 361.86] loss=3.62 avg=3.75\n",
      "[83 | 366.24] loss=3.66 avg=3.75\n",
      "[84 | 370.64] loss=3.64 avg=3.75\n",
      "[85 | 375.03] loss=3.81 avg=3.75\n",
      "[86 | 379.40] loss=3.36 avg=3.74\n",
      "[87 | 383.77] loss=3.68 avg=3.74\n",
      "[88 | 388.15] loss=3.60 avg=3.74\n",
      "[89 | 392.54] loss=3.80 avg=3.74\n",
      "[90 | 396.91] loss=3.85 avg=3.74\n",
      "[91 | 401.27] loss=3.35 avg=3.73\n",
      "[92 | 405.67] loss=3.81 avg=3.74\n",
      "[93 | 410.05] loss=3.66 avg=3.73\n",
      "[94 | 414.43] loss=3.82 avg=3.74\n",
      "[95 | 418.82] loss=3.90 avg=3.74\n",
      "[96 | 423.21] loss=3.68 avg=3.74\n",
      "[97 | 427.61] loss=3.68 avg=3.74\n",
      "[98 | 432.00] loss=3.86 avg=3.74\n",
      "[99 | 436.38] loss=3.75 avg=3.74\n",
      "[100 | 440.77] loss=3.51 avg=3.74\n",
      "======== SAMPLE 1 ========\n",
      " to a small girl with curly blondish hair with a little bit of pink in her chin—but even her dark red eyes remained fixed on her. “Why are you here?” “The boy has the letter, doesn’t he?” “Yes,” she said, “because they would like to hear a little speech of him. And there is something that they want to see from you. They want their lips to look just the same —” “That’s a very nice way of saying it!” she murmured, shaking her head with excitement. “But you are not the prettiest person,” continued the girl, “I have a pretty face: I like the light-brown hair, the round ears and the pink-hued lips.”\n",
      "\n",
      "“A man has a mustache,” thought the girl. “He always wears a mustache,” she exclaimed, “and he’s always a man’s mustache.”\n",
      "\n",
      "“What’s the matter?” asked the girl, “What the old man was like in his youth?”\n",
      "\n",
      "“He’d want to marry me,” said the girl. “Now I’m telling you, that’s wrong. He’d rather wait for him to marry me.”\n",
      "\n",
      "The boy came back with a small stick, and went out, and the girl was asleep. The old man, meanwhile, who sat on the chair, and was now leaning over her shoulders, saw the girl and immediately went looking at her, and said nothing. The boy went on shaking his head, and the girl looked at him and said: “My dear, what a sad dream it was—it’s a dream!”\n",
      "\n",
      "“I’m not sure how it could be that I should have been so kind as to invite you to visit me. I do not know what I can do for you, but I have not found a warm place here that would be quite pleasant to me, for I am convinced that at the moment you are visiting me, I wish to have you with me.”\n",
      "\n",
      "But he did nothing. He went to sleep and slept again.\n",
      "\n",
      "He would go on for days on end to the window, as if his wife would give him an answer to the girl’s question. She would say: “That’s the beauty of that,” to which he could not make out whether that was true or not, or whether or not the explanation to be made was true; and then she would not let him go and would start to say something in vain. On the next day, for instance, he was in bed by the fire for one hour at a time, in order to wash his face and take a bath while walking. And this, again, he did not answer. In the next hour, in the same corridor, he was asleep by the window with those red eyes, and no answer: “No,” he answered with a smile of satisfaction.\n",
      "\n",
      "When he woke at nightfall on Thursday the second of April, he had a dream in which, at night, in order to keep him awake, he took with him a cigarette into which he had been inhaling a single kind of cigarette and was immediately awakened by the sounds of a roaring roar.\n",
      "\n",
      "“Is there anything I can do for you that does not involve you in my room,” said the boy, while in a different part of the room he could hear a familiar noise. “” But he was not able to come to what he thought was a very good deal: the fire had started and was ablaze. In his dream, all around, the walls were in a sort of darkness, with a soft glow hanging up above the floor, and it was impossible to see the dark under it. In a corner in the room in which he was lying, in between the smoke of candles and the flames of the fire, there was the door of the corridor. The only sound was a knock, and the door was thrown open. “A woman,” murmured the girl. “I believe she is a French woman. I have not met her in a long time. Tell me how they met.”\n",
      "\n",
      "“You are in love with the same woman,” said the little girl, with a queer smile.\n",
      "\n",
      "She went on with the conversation between herself and the boy, the conversation between her and the girl which now seemed to her quite like no other: the girl was in love with him a long time ago, and yet he was much more interested in the boy than in her.\n",
      "\n",
      "She sat on a damp bench with a pale-blue chair in a corner. He had a\n",
      "\n",
      "[101 | 463.42] loss=3.65 avg=3.73\n",
      "[102 | 467.79] loss=3.41 avg=3.73\n",
      "[103 | 472.15] loss=4.11 avg=3.74\n",
      "[104 | 476.53] loss=3.89 avg=3.74\n",
      "[105 | 480.90] loss=3.67 avg=3.74\n",
      "[106 | 485.27] loss=4.04 avg=3.74\n",
      "[107 | 489.65] loss=3.68 avg=3.74\n",
      "[108 | 494.03] loss=3.59 avg=3.74\n",
      "[109 | 498.41] loss=3.75 avg=3.74\n",
      "[110 | 502.79] loss=3.49 avg=3.73\n",
      "[111 | 507.17] loss=3.74 avg=3.73\n",
      "[112 | 511.56] loss=3.62 avg=3.73\n",
      "[113 | 515.95] loss=3.60 avg=3.73\n",
      "[114 | 520.34] loss=3.64 avg=3.73\n",
      "[115 | 524.72] loss=3.70 avg=3.73\n",
      "[116 | 529.11] loss=3.78 avg=3.73\n",
      "[117 | 533.49] loss=3.70 avg=3.73\n",
      "[118 | 537.92] loss=3.43 avg=3.72\n",
      "[119 | 542.34] loss=3.62 avg=3.72\n",
      "[120 | 546.75] loss=3.48 avg=3.72\n",
      "[121 | 551.15] loss=3.72 avg=3.72\n",
      "[122 | 555.53] loss=3.63 avg=3.72\n",
      "[123 | 559.92] loss=3.37 avg=3.71\n",
      "[124 | 564.31] loss=3.35 avg=3.71\n",
      "[125 | 568.71] loss=3.56 avg=3.71\n",
      "[126 | 573.11] loss=3.75 avg=3.71\n",
      "[127 | 577.50] loss=3.72 avg=3.71\n",
      "[128 | 581.87] loss=3.66 avg=3.71\n",
      "[129 | 586.25] loss=3.68 avg=3.71\n",
      "[130 | 590.62] loss=3.44 avg=3.70\n",
      "[131 | 595.00] loss=3.64 avg=3.70\n",
      "[132 | 599.39] loss=3.41 avg=3.70\n",
      "[133 | 603.78] loss=3.43 avg=3.69\n",
      "[134 | 608.17] loss=3.49 avg=3.69\n",
      "[135 | 612.55] loss=4.05 avg=3.70\n",
      "[136 | 616.93] loss=3.93 avg=3.70\n",
      "[137 | 621.31] loss=3.31 avg=3.69\n",
      "[138 | 625.67] loss=3.51 avg=3.69\n",
      "[139 | 630.03] loss=3.70 avg=3.69\n",
      "[140 | 634.40] loss=3.68 avg=3.69\n",
      "[141 | 638.78] loss=3.57 avg=3.69\n",
      "[142 | 643.17] loss=3.71 avg=3.69\n",
      "[143 | 647.53] loss=3.85 avg=3.69\n",
      "[144 | 651.92] loss=3.61 avg=3.69\n",
      "[145 | 656.29] loss=3.72 avg=3.69\n",
      "[146 | 660.66] loss=3.67 avg=3.69\n",
      "[147 | 665.03] loss=3.33 avg=3.69\n",
      "[148 | 669.42] loss=3.66 avg=3.69\n",
      "[149 | 673.82] loss=3.64 avg=3.69\n",
      "[150 | 678.19] loss=3.60 avg=3.68\n",
      "[151 | 682.56] loss=3.77 avg=3.69\n",
      "[152 | 686.95] loss=3.59 avg=3.68\n",
      "[153 | 691.33] loss=3.45 avg=3.68\n",
      "[154 | 695.71] loss=3.57 avg=3.68\n",
      "[155 | 700.07] loss=3.51 avg=3.68\n",
      "[156 | 704.45] loss=3.74 avg=3.68\n",
      "[157 | 708.84] loss=3.64 avg=3.68\n",
      "[158 | 713.20] loss=3.50 avg=3.68\n",
      "[159 | 717.58] loss=3.43 avg=3.67\n",
      "[160 | 721.96] loss=3.25 avg=3.67\n",
      "[161 | 726.34] loss=3.55 avg=3.67\n",
      "[162 | 730.72] loss=3.47 avg=3.66\n",
      "[163 | 735.10] loss=3.85 avg=3.67\n",
      "[164 | 739.48] loss=3.41 avg=3.66\n",
      "[165 | 743.86] loss=3.42 avg=3.66\n",
      "[166 | 748.27] loss=3.82 avg=3.66\n",
      "[167 | 752.65] loss=3.35 avg=3.66\n",
      "[168 | 757.04] loss=3.48 avg=3.66\n",
      "[169 | 761.43] loss=3.52 avg=3.65\n",
      "[170 | 765.81] loss=3.67 avg=3.65\n",
      "[171 | 770.21] loss=3.59 avg=3.65\n",
      "[172 | 774.60] loss=3.93 avg=3.66\n",
      "[173 | 778.99] loss=3.15 avg=3.65\n",
      "[174 | 783.37] loss=3.72 avg=3.65\n",
      "[175 | 787.75] loss=3.46 avg=3.65\n",
      "[176 | 792.14] loss=3.84 avg=3.65\n",
      "[177 | 796.53] loss=3.48 avg=3.65\n",
      "[178 | 800.91] loss=3.54 avg=3.65\n",
      "[179 | 805.30] loss=3.77 avg=3.65\n",
      "[180 | 809.69] loss=3.62 avg=3.65\n",
      "[181 | 814.08] loss=3.79 avg=3.65\n",
      "[182 | 818.45] loss=3.61 avg=3.65\n",
      "[183 | 822.82] loss=3.56 avg=3.65\n",
      "[184 | 827.20] loss=3.63 avg=3.65\n",
      "[185 | 831.60] loss=3.84 avg=3.65\n",
      "[186 | 835.97] loss=3.66 avg=3.65\n",
      "[187 | 840.35] loss=3.47 avg=3.65\n",
      "[188 | 844.74] loss=3.52 avg=3.65\n",
      "[189 | 849.15] loss=3.64 avg=3.65\n",
      "[190 | 853.53] loss=3.80 avg=3.65\n",
      "[191 | 857.92] loss=3.73 avg=3.65\n",
      "[192 | 862.31] loss=3.77 avg=3.65\n",
      "[193 | 866.67] loss=3.92 avg=3.65\n",
      "[194 | 871.05] loss=3.78 avg=3.66\n",
      "[195 | 875.42] loss=3.83 avg=3.66\n",
      "[196 | 879.80] loss=3.57 avg=3.66\n",
      "[197 | 884.18] loss=3.53 avg=3.66\n",
      "[198 | 888.58] loss=3.96 avg=3.66\n",
      "[199 | 892.96] loss=3.79 avg=3.66\n",
      "[200 | 897.35] loss=3.50 avg=3.66\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Chet\n",
      "\n",
      "…\n",
      "\n",
      "Brent\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "Peter\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Petr A\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Chet\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "Peter\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Nag\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Lydia\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Pod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Nag\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Trying to get out of the tent, Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Peter\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod,\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod,\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Rod\n",
      "\n",
      "…\n",
      "\n",
      "Nag\n",
      "\n",
      "— —\n",
      "\n",
      "[201 | 917.76] loss=3.64 avg=3.66\n",
      "[202 | 922.14] loss=3.83 avg=3.66\n",
      "[203 | 926.51] loss=3.38 avg=3.66\n",
      "[204 | 930.88] loss=3.37 avg=3.65\n",
      "[205 | 935.26] loss=3.57 avg=3.65\n",
      "[206 | 939.63] loss=3.70 avg=3.65\n",
      "[207 | 944.00] loss=3.14 avg=3.65\n",
      "[208 | 948.37] loss=3.43 avg=3.65\n",
      "[209 | 952.75] loss=3.45 avg=3.64\n",
      "[210 | 957.14] loss=3.84 avg=3.65\n",
      "[211 | 961.52] loss=3.64 avg=3.64\n",
      "[212 | 965.89] loss=4.01 avg=3.65\n",
      "[213 | 970.26] loss=3.31 avg=3.65\n",
      "[214 | 974.64] loss=3.53 avg=3.64\n",
      "[215 | 979.01] loss=3.14 avg=3.64\n",
      "[216 | 983.41] loss=3.21 avg=3.63\n",
      "[217 | 987.79] loss=3.69 avg=3.63\n",
      "[218 | 992.18] loss=3.47 avg=3.63\n",
      "[219 | 996.56] loss=3.52 avg=3.63\n",
      "[220 | 1000.95] loss=3.26 avg=3.63\n",
      "[221 | 1005.33] loss=3.23 avg=3.62\n",
      "[222 | 1009.72] loss=3.45 avg=3.62\n",
      "[223 | 1014.09] loss=3.56 avg=3.62\n",
      "[224 | 1018.48] loss=3.92 avg=3.62\n",
      "[225 | 1022.85] loss=3.74 avg=3.62\n",
      "[226 | 1027.22] loss=3.80 avg=3.63\n",
      "[227 | 1031.60] loss=3.37 avg=3.62\n",
      "[228 | 1035.98] loss=3.94 avg=3.63\n",
      "[229 | 1040.37] loss=3.55 avg=3.63\n",
      "[230 | 1044.76] loss=3.65 avg=3.63\n",
      "[231 | 1049.14] loss=3.33 avg=3.62\n",
      "[232 | 1053.54] loss=3.48 avg=3.62\n",
      "[233 | 1057.93] loss=3.72 avg=3.62\n",
      "[234 | 1062.31] loss=3.47 avg=3.62\n",
      "[235 | 1066.69] loss=3.48 avg=3.62\n",
      "[236 | 1071.07] loss=3.73 avg=3.62\n",
      "[237 | 1075.44] loss=3.43 avg=3.62\n",
      "[238 | 1079.83] loss=3.56 avg=3.62\n",
      "[239 | 1084.21] loss=3.89 avg=3.62\n",
      "[240 | 1088.60] loss=3.72 avg=3.62\n",
      "[241 | 1092.97] loss=3.58 avg=3.62\n",
      "[242 | 1097.36] loss=3.70 avg=3.62\n",
      "[243 | 1101.74] loss=3.47 avg=3.62\n",
      "[244 | 1106.12] loss=3.48 avg=3.62\n",
      "[245 | 1110.50] loss=3.55 avg=3.62\n",
      "[246 | 1114.88] loss=3.84 avg=3.62\n",
      "[247 | 1119.28] loss=3.64 avg=3.62\n",
      "[248 | 1123.66] loss=3.35 avg=3.62\n",
      "[249 | 1128.05] loss=3.57 avg=3.62\n",
      "[250 | 1132.43] loss=3.64 avg=3.62\n",
      "[251 | 1136.80] loss=3.58 avg=3.62\n",
      "[252 | 1141.19] loss=3.71 avg=3.62\n",
      "[253 | 1145.57] loss=3.48 avg=3.62\n",
      "[254 | 1149.95] loss=3.72 avg=3.62\n",
      "[255 | 1154.37] loss=3.08 avg=3.61\n",
      "[256 | 1158.77] loss=3.41 avg=3.61\n",
      "[257 | 1163.15] loss=3.40 avg=3.61\n",
      "[258 | 1167.54] loss=3.67 avg=3.61\n",
      "[259 | 1171.92] loss=3.57 avg=3.61\n",
      "[260 | 1176.31] loss=3.81 avg=3.61\n",
      "[261 | 1180.69] loss=3.34 avg=3.61\n",
      "[262 | 1185.07] loss=3.67 avg=3.61\n",
      "[263 | 1189.44] loss=3.30 avg=3.60\n",
      "[264 | 1193.82] loss=3.31 avg=3.60\n",
      "[265 | 1198.20] loss=3.67 avg=3.60\n",
      "[266 | 1202.58] loss=3.67 avg=3.60\n",
      "[267 | 1206.96] loss=3.70 avg=3.60\n",
      "[268 | 1211.34] loss=3.58 avg=3.60\n",
      "[269 | 1215.72] loss=3.74 avg=3.60\n",
      "[270 | 1220.11] loss=3.41 avg=3.60\n",
      "[271 | 1224.50] loss=3.64 avg=3.60\n",
      "[272 | 1228.89] loss=3.40 avg=3.60\n",
      "[273 | 1233.27] loss=3.73 avg=3.60\n",
      "[274 | 1237.66] loss=3.58 avg=3.60\n",
      "[275 | 1242.04] loss=3.44 avg=3.60\n",
      "[276 | 1246.40] loss=3.71 avg=3.60\n",
      "[277 | 1250.77] loss=3.54 avg=3.60\n",
      "[278 | 1255.15] loss=3.51 avg=3.60\n",
      "[279 | 1259.54] loss=3.38 avg=3.60\n",
      "[280 | 1263.91] loss=3.37 avg=3.60\n",
      "[281 | 1268.28] loss=3.61 avg=3.60\n",
      "[282 | 1272.66] loss=3.78 avg=3.60\n",
      "[283 | 1277.05] loss=3.70 avg=3.60\n",
      "[284 | 1281.44] loss=3.69 avg=3.60\n",
      "[285 | 1285.82] loss=3.30 avg=3.60\n",
      "[286 | 1290.22] loss=3.61 avg=3.60\n",
      "[287 | 1294.59] loss=3.43 avg=3.59\n",
      "[288 | 1298.96] loss=3.83 avg=3.60\n",
      "[289 | 1303.34] loss=3.88 avg=3.60\n",
      "[290 | 1307.72] loss=3.47 avg=3.60\n",
      "[291 | 1312.11] loss=3.60 avg=3.60\n",
      "[292 | 1316.47] loss=3.46 avg=3.60\n",
      "[293 | 1320.86] loss=3.75 avg=3.60\n",
      "[294 | 1325.24] loss=3.55 avg=3.60\n",
      "[295 | 1329.63] loss=3.63 avg=3.60\n",
      "[296 | 1334.02] loss=3.59 avg=3.60\n",
      "[297 | 1338.42] loss=3.41 avg=3.60\n",
      "[298 | 1342.79] loss=3.50 avg=3.60\n",
      "[299 | 1347.17] loss=3.60 avg=3.60\n",
      "[300 | 1351.55] loss=3.19 avg=3.59\n",
      "======== SAMPLE 1 ========\n",
      " sidelity, though he was also on the whole, a great admirer of Hovhino, who even went to see him in California where his father taught.\n",
      "\n",
      "“He has a kind of crazy temper which keeps him out of easy matters. He was once arrested because he was a Jew but was in no position to explain all this. He says a great deal of nonsense on his part about the Jewish question, but then he really does really talk about the Jews.\n",
      "\n",
      "“One cannot help thinking that he is quite as silly as he. On the contrary, as I learned from our own Professor, he may actually be as clever as one may imagine him.\n",
      "\n",
      "“He has the impression that there is little chance of the Soviets turning into a political organization, something that would really upset the balance of power. The question of the Soviets’ independence from the Soviets has also been raised, in terms of policy. In my experience of political thought my opinion of their independence has grown and is quite different from the opinion of my colleagues. To a tremendous extent I have known that a great leader can be taken as a comrade, but this was not always the case, since the people generally are unable to stand his kind. He has had to make a very sharp distinction between what the leadership is and what he is—which is to say that his position as a leader does not quite fit with those of his compatriots.\n",
      "\n",
      "“It is really a matter of pride not to allow him the kind of views that he might suggest. If he is a member of the Soviet Union, there really is nothing to fear. But I know that if he were not the leader of a people, he would not be as successful as he is and could not control the movement, which is how it ought to be conducted.\n",
      "\n",
      "“I shall not, however, agree with your views. I myself am not sure what his motives are. I think that he thinks that the people should be happy in general but that if they try to interfere with something that they have already done, the people would start throwing the question away.\n",
      "\n",
      "“He is a very honest man. And he was right to stress that if the Bolsheviks wanted to interfere with the movements they could take measures against their own officers. As I recall, this is a strange situation and not a moment for complacency.\n",
      "\n",
      "“That did not bother you too highly—although perhaps we need an example of that kind in the case of the Soviets themselves. I am sure they would be quite interested.\n",
      "\n",
      "“I am the writer’s daughter, and I have no problem with the question of whom to marry. I think it is quite possible—but it is not a question of who to marry—because I know that they would not like to enter a war with any partner who did not agree with their opinion. But this is only the beginning. The whole point is that the people should not lose sight of the important point: the existence and the power of a Soviet government.\n",
      "\n",
      "“It is very amusing to meet this man. His name is Zilanov, and his government, according to him, consists, as I was told, of a series of governmental steps which might be carried out by a revolutionary. But if I am mistaken, this government is totally ignorant of the situation. This is completely unacceptable. It would be an impossible thing, a betrayal of the whole of reality.\n",
      "\n",
      "“Let me add one more point, an additional point—that there are those who refuse to follow the leadership and those who demand absolute equality between the leaders. For instance, if you find yourself with the Communist leader, the situation is even more dire than before.\n",
      "\n",
      "“My attitude of sympathy is quite satisfactory. It is a pity that that would be the case, as even if I did not know what was happening, the situation might not quite turn out as its originally hoped to turn out. But at the same time, it is difficult for me to imagine how it would be if you, a man of the Soviet Union, became in that position. It would be an impossible situation. And this is why it is so interesting—not only to me or even to the general public—that you have now forgotten how the situation of our people has turned out.\n",
      "\n",
      "“And yet—oh, I do hope, I do hope. But it does look a lot like the situation I found in my own country. I hope that what I had done is right as well.\n",
      "\n",
      "“I would still like to live with the Soviet State, if I were not so lazy that I made such a fuss, but how I am trying on my own. Let me add that my wife has said that her only consolation as a daughter could be the fact that we are in one big family. Yes, I have not decided yet whether to marry her or not, but since you seem to be quite happy with\n",
      "\n",
      "[301 | 1372.05] loss=3.51 avg=3.59\n",
      "[302 | 1376.42] loss=3.58 avg=3.59\n",
      "[303 | 1380.78] loss=3.47 avg=3.59\n",
      "[304 | 1385.15] loss=3.37 avg=3.59\n",
      "[305 | 1389.52] loss=3.68 avg=3.59\n",
      "[306 | 1393.90] loss=3.16 avg=3.58\n",
      "[307 | 1398.28] loss=3.89 avg=3.59\n",
      "[308 | 1402.67] loss=3.66 avg=3.59\n",
      "[309 | 1407.05] loss=3.61 avg=3.59\n",
      "[310 | 1411.43] loss=3.33 avg=3.58\n",
      "[311 | 1415.83] loss=3.68 avg=3.59\n",
      "[312 | 1420.19] loss=3.33 avg=3.58\n",
      "[313 | 1424.57] loss=3.48 avg=3.58\n",
      "[314 | 1428.96] loss=3.39 avg=3.58\n",
      "[315 | 1433.34] loss=3.65 avg=3.58\n",
      "[316 | 1437.73] loss=3.44 avg=3.58\n",
      "[317 | 1442.11] loss=3.53 avg=3.58\n",
      "[318 | 1446.49] loss=3.66 avg=3.58\n",
      "[319 | 1450.87] loss=4.04 avg=3.58\n",
      "[320 | 1455.25] loss=3.64 avg=3.58\n",
      "[321 | 1459.62] loss=3.70 avg=3.59\n",
      "[322 | 1464.00] loss=3.69 avg=3.59\n",
      "[323 | 1468.36] loss=3.58 avg=3.59\n",
      "[324 | 1472.73] loss=3.65 avg=3.59\n",
      "[325 | 1477.10] loss=3.39 avg=3.59\n",
      "[326 | 1481.48] loss=3.56 avg=3.59\n",
      "[327 | 1485.87] loss=3.78 avg=3.59\n",
      "[328 | 1490.25] loss=3.60 avg=3.59\n",
      "[329 | 1494.62] loss=3.73 avg=3.59\n",
      "[330 | 1499.01] loss=3.45 avg=3.59\n",
      "[331 | 1503.39] loss=3.49 avg=3.59\n",
      "[332 | 1507.77] loss=3.43 avg=3.59\n",
      "[333 | 1512.16] loss=3.35 avg=3.58\n",
      "[334 | 1516.55] loss=3.59 avg=3.58\n",
      "[335 | 1520.91] loss=3.64 avg=3.58\n",
      "[336 | 1525.29] loss=3.77 avg=3.59\n",
      "[337 | 1529.65] loss=3.52 avg=3.58\n",
      "[338 | 1534.04] loss=3.38 avg=3.58\n",
      "[339 | 1538.42] loss=3.50 avg=3.58\n",
      "[340 | 1542.80] loss=3.67 avg=3.58\n",
      "[341 | 1547.17] loss=3.37 avg=3.58\n",
      "[342 | 1551.55] loss=3.55 avg=3.58\n",
      "[343 | 1555.92] loss=3.59 avg=3.58\n",
      "[344 | 1560.29] loss=3.44 avg=3.58\n",
      "[345 | 1564.68] loss=3.39 avg=3.58\n",
      "[346 | 1569.04] loss=3.37 avg=3.57\n",
      "[347 | 1573.43] loss=3.58 avg=3.57\n",
      "[348 | 1577.80] loss=3.46 avg=3.57\n",
      "[349 | 1582.19] loss=3.28 avg=3.57\n",
      "[350 | 1586.57] loss=3.28 avg=3.57\n",
      "[351 | 1590.96] loss=3.64 avg=3.57\n",
      "[352 | 1595.35] loss=3.40 avg=3.57\n",
      "[353 | 1599.73] loss=3.58 avg=3.57\n",
      "[354 | 1604.12] loss=3.31 avg=3.56\n",
      "[355 | 1608.50] loss=3.54 avg=3.56\n",
      "[356 | 1612.89] loss=3.32 avg=3.56\n",
      "[357 | 1617.27] loss=3.13 avg=3.56\n",
      "[358 | 1621.64] loss=3.16 avg=3.55\n",
      "[359 | 1626.03] loss=3.61 avg=3.55\n",
      "[360 | 1630.41] loss=3.24 avg=3.55\n",
      "[361 | 1634.79] loss=3.77 avg=3.55\n",
      "[362 | 1639.16] loss=3.75 avg=3.55\n",
      "[363 | 1643.54] loss=3.59 avg=3.55\n",
      "[364 | 1647.90] loss=3.44 avg=3.55\n",
      "[365 | 1652.28] loss=3.73 avg=3.56\n",
      "[366 | 1656.64] loss=3.61 avg=3.56\n",
      "[367 | 1661.02] loss=3.39 avg=3.55\n",
      "[368 | 1665.40] loss=3.36 avg=3.55\n",
      "[369 | 1669.77] loss=3.28 avg=3.55\n",
      "[370 | 1674.16] loss=3.43 avg=3.55\n",
      "[371 | 1678.53] loss=3.52 avg=3.55\n",
      "[372 | 1682.91] loss=3.66 avg=3.55\n",
      "[373 | 1687.31] loss=3.35 avg=3.55\n",
      "[374 | 1691.69] loss=3.43 avg=3.55\n",
      "[375 | 1696.06] loss=3.46 avg=3.54\n",
      "[376 | 1700.43] loss=3.91 avg=3.55\n",
      "[377 | 1704.81] loss=3.30 avg=3.55\n",
      "[378 | 1709.18] loss=3.73 avg=3.55\n",
      "[379 | 1713.56] loss=3.63 avg=3.55\n",
      "[380 | 1717.93] loss=3.37 avg=3.55\n",
      "[381 | 1722.31] loss=3.37 avg=3.55\n",
      "[382 | 1726.68] loss=3.62 avg=3.55\n",
      "[383 | 1731.05] loss=3.94 avg=3.55\n",
      "[384 | 1735.44] loss=3.33 avg=3.55\n",
      "[385 | 1739.82] loss=3.61 avg=3.55\n",
      "[386 | 1744.19] loss=3.38 avg=3.55\n",
      "[387 | 1748.56] loss=3.22 avg=3.54\n",
      "[388 | 1752.95] loss=3.61 avg=3.54\n",
      "[389 | 1757.32] loss=3.34 avg=3.54\n",
      "[390 | 1761.68] loss=3.20 avg=3.54\n",
      "[391 | 1766.07] loss=3.29 avg=3.54\n",
      "[392 | 1770.47] loss=3.35 avg=3.53\n",
      "[393 | 1774.85] loss=3.15 avg=3.53\n",
      "[394 | 1779.24] loss=3.36 avg=3.53\n",
      "[395 | 1783.61] loss=3.73 avg=3.53\n",
      "[396 | 1787.99] loss=3.61 avg=3.53\n",
      "[397 | 1792.36] loss=3.44 avg=3.53\n",
      "[398 | 1796.76] loss=3.42 avg=3.53\n",
      "[399 | 1801.17] loss=3.57 avg=3.53\n",
      "[400 | 1805.57] loss=3.34 avg=3.53\n",
      "======== SAMPLE 1 ========\n",
      " wine, one would look like some kind of giant and the other like a giant with the same body. If you think that an arm of a giant, having stood like that on the top of an arm of a giant (if you want to see a picture of my arm), is a real thing, then let me show you a picture.”\n",
      "\n",
      "“A picture!” said K.\n",
      "\n",
      "At about the same time he found that he remembered vividly the words he had used when he was little. The next morning he had the same feeling. That day, however, he remembered only the words he had used at all the same time in all his lives, and it was only when he went home, thinking that he too was the son of a great poet, that he realized that the whole mania for his existence seemed to him like that of some great painter who had been in the most remote spot for nearly two thousand years and had become the supreme living figure of art, and that, as a consequence of this, he was a man who had been the inspiration in his life.\n",
      "\n",
      "He took a look at his diary, and, while it was in there, and saw that he had the same expression as if that letter were true, he felt that at the same time of its being written in this state he felt that at an instant he himself had been transformed into his human form, that, while writing on it, he was writing in another world, to be found the very day upon which his image was supposed to come crashing down; that, instead of his being a man, he was a man of incredible genius. A man who had just been born is endowed, even by the act of birth, with an enormous capacity for art.\n",
      "\n",
      "He had had an enormous number of dreams, some as small as a man; the most that he had ever had; and yet a certain combination was at work in his waking. And so on and so forth.\n",
      "\n",
      "He remembered all that day in terms of a great poet whom he had known for years; and that for some reason the same day he remembered a small but not remarkable thing, a great one. At the same time he remembered, nevertheless, to the utmost astonishment that he himself, if this were possible, would, and indeed ought to, remember that day, that wonderful morning on which, with his eyes fixed on a large boulder, he had fallen asleep. This, in fact, was true.\n",
      "\n",
      "One night in the early afternoon, as he sat in the study of a garden of roses and peonies and gables — for a brief while, when he did not want to see the flowers and began to walk, and when he began to feel vaguely that he had no friends — he was so happy that after having walked the whole summer, had visited with his neighbor, that day had been a quiet, but very emotional, joy, and now, with his neighbor as well, he was feeling even happier than before. He had seen the roses and was delighted by the fact that they contained everything he felt — all the flowers, the roses, the garden, the trees — and he was already on his way to getting there, when it suddenly happened: the path they had chosen had been filled with these beautiful things, which he already knew now, just because he knew not what that path was. Suddenly he felt a thrill; the next moment a sudden happiness had passed, and now at once the whole of his self — the whole idea of man, the whole course of his life, that which he already knew to be absolutely true — was transformed into the object of his wish and his imagination, into that which he had hoped he would ever find himself once again and it was just as he had realized it all his life.\n",
      "\n",
      "He felt a very joyful and joyful excitement, the ecstasy of being at ease and happy. He felt as one of those wonderful dreams, in such a way, with such depth of abstraction, which is seldom noticed and which, being a dream, produces very little of any real thought, but something that is very simple and pure. For the moment he imagined himself in paradise; then he imagined the world in the light of God and the darkness of his own light, and then he could not have dreamed of anything more than this and that and that; he could not have dreamed of a whole city in its entire light and darkness; and he could not have dreamed of any human substance in the entire shade of that tree of life: that whole tree of life. This was a very special experience, this unique pleasure; it was even more special than when it was possible to see yourself and live in it. He kept on the path, all the while thinking that he had long enough of the possibility of being alone — that this strange freedom of thought, this strange freedom of feeling, which is precisely so that, like some precious jewel, it seems to go through every kind of vibration. No wonder, however, that he felt that\n",
      "\n",
      "[401 | 1826.05] loss=3.49 avg=3.53\n",
      "[402 | 1830.41] loss=3.71 avg=3.53\n",
      "[403 | 1834.80] loss=3.24 avg=3.53\n",
      "[404 | 1839.19] loss=3.61 avg=3.53\n",
      "[405 | 1843.61] loss=3.31 avg=3.52\n",
      "[406 | 1848.01] loss=3.35 avg=3.52\n",
      "[407 | 1852.39] loss=3.67 avg=3.52\n",
      "[408 | 1856.76] loss=3.38 avg=3.52\n",
      "[409 | 1861.14] loss=3.39 avg=3.52\n",
      "[410 | 1865.51] loss=3.24 avg=3.52\n",
      "[411 | 1869.87] loss=3.54 avg=3.52\n",
      "[412 | 1874.25] loss=3.48 avg=3.52\n",
      "[413 | 1878.62] loss=3.76 avg=3.52\n",
      "[414 | 1882.98] loss=3.25 avg=3.52\n",
      "[415 | 1887.35] loss=3.37 avg=3.52\n",
      "[416 | 1891.74] loss=3.45 avg=3.52\n",
      "[417 | 1896.10] loss=3.17 avg=3.51\n",
      "[418 | 1900.48] loss=3.38 avg=3.51\n",
      "[419 | 1904.86] loss=3.01 avg=3.51\n",
      "[420 | 1909.25] loss=3.24 avg=3.50\n",
      "[421 | 1913.62] loss=3.69 avg=3.51\n",
      "[422 | 1918.00] loss=3.83 avg=3.51\n",
      "[423 | 1922.38] loss=3.76 avg=3.51\n",
      "[424 | 1926.75] loss=3.61 avg=3.51\n",
      "[425 | 1931.13] loss=3.30 avg=3.51\n",
      "[426 | 1935.52] loss=3.58 avg=3.51\n",
      "[427 | 1939.89] loss=2.91 avg=3.50\n",
      "[428 | 1944.28] loss=3.56 avg=3.51\n",
      "[429 | 1948.66] loss=3.39 avg=3.50\n",
      "[430 | 1953.05] loss=3.50 avg=3.50\n",
      "[431 | 1957.43] loss=3.52 avg=3.50\n",
      "[432 | 1961.82] loss=3.65 avg=3.51\n",
      "[433 | 1966.21] loss=3.57 avg=3.51\n",
      "[434 | 1970.62] loss=3.23 avg=3.50\n",
      "[435 | 1974.99] loss=3.26 avg=3.50\n",
      "[436 | 1979.39] loss=3.54 avg=3.50\n",
      "[437 | 1983.77] loss=3.16 avg=3.50\n",
      "[438 | 1988.15] loss=3.68 avg=3.50\n",
      "[439 | 1992.54] loss=3.66 avg=3.50\n",
      "[440 | 1996.92] loss=3.57 avg=3.50\n",
      "[441 | 2001.31] loss=3.39 avg=3.50\n",
      "[442 | 2005.70] loss=3.50 avg=3.50\n",
      "[443 | 2010.08] loss=3.43 avg=3.50\n",
      "[444 | 2014.46] loss=3.65 avg=3.50\n",
      "[445 | 2018.87] loss=3.46 avg=3.50\n",
      "[446 | 2023.24] loss=3.46 avg=3.50\n",
      "[447 | 2027.62] loss=3.40 avg=3.50\n",
      "[448 | 2032.00] loss=3.27 avg=3.50\n",
      "[449 | 2036.39] loss=3.46 avg=3.50\n",
      "[450 | 2040.76] loss=3.32 avg=3.50\n",
      "[451 | 2045.14] loss=3.63 avg=3.50\n",
      "[452 | 2049.52] loss=3.51 avg=3.50\n",
      "[453 | 2053.91] loss=3.79 avg=3.50\n",
      "[454 | 2058.28] loss=3.55 avg=3.50\n",
      "[455 | 2062.68] loss=3.70 avg=3.50\n",
      "[456 | 2067.07] loss=3.48 avg=3.50\n",
      "[457 | 2071.48] loss=3.38 avg=3.50\n",
      "[458 | 2075.88] loss=3.57 avg=3.50\n",
      "[459 | 2080.27] loss=3.40 avg=3.50\n",
      "[460 | 2084.68] loss=3.36 avg=3.50\n",
      "[461 | 2089.07] loss=3.51 avg=3.50\n",
      "[462 | 2093.46] loss=3.35 avg=3.50\n",
      "[463 | 2097.85] loss=3.49 avg=3.50\n",
      "[464 | 2102.24] loss=3.54 avg=3.50\n",
      "[465 | 2106.66] loss=3.62 avg=3.50\n",
      "[466 | 2111.05] loss=3.22 avg=3.50\n",
      "[467 | 2115.45] loss=3.28 avg=3.49\n",
      "[468 | 2119.86] loss=3.48 avg=3.49\n",
      "[469 | 2124.25] loss=3.65 avg=3.50\n",
      "[470 | 2128.63] loss=3.46 avg=3.50\n",
      "[471 | 2133.03] loss=3.23 avg=3.49\n",
      "[472 | 2137.41] loss=3.65 avg=3.49\n",
      "[473 | 2141.80] loss=3.65 avg=3.50\n",
      "[474 | 2146.18] loss=3.44 avg=3.50\n",
      "[475 | 2150.57] loss=3.40 avg=3.49\n",
      "[476 | 2154.95] loss=3.29 avg=3.49\n",
      "[477 | 2159.35] loss=3.60 avg=3.49\n",
      "[478 | 2163.76] loss=3.34 avg=3.49\n",
      "[479 | 2168.13] loss=3.28 avg=3.49\n",
      "[480 | 2172.52] loss=3.84 avg=3.49\n",
      "[481 | 2176.92] loss=3.33 avg=3.49\n",
      "[482 | 2181.31] loss=3.08 avg=3.49\n",
      "[483 | 2185.69] loss=3.54 avg=3.49\n",
      "[484 | 2190.07] loss=3.35 avg=3.49\n",
      "[485 | 2194.47] loss=3.36 avg=3.49\n",
      "[486 | 2198.86] loss=3.47 avg=3.49\n",
      "[487 | 2203.27] loss=3.32 avg=3.48\n",
      "[488 | 2207.67] loss=3.46 avg=3.48\n",
      "[489 | 2212.07] loss=3.42 avg=3.48\n",
      "[490 | 2216.49] loss=3.34 avg=3.48\n",
      "[491 | 2220.88] loss=3.28 avg=3.48\n",
      "[492 | 2225.29] loss=3.53 avg=3.48\n",
      "[493 | 2229.68] loss=3.60 avg=3.48\n",
      "[494 | 2234.08] loss=3.32 avg=3.48\n",
      "[495 | 2238.47] loss=3.20 avg=3.48\n",
      "[496 | 2242.87] loss=3.63 avg=3.48\n",
      "[497 | 2247.26] loss=3.35 avg=3.48\n",
      "[498 | 2251.68] loss=3.75 avg=3.48\n",
      "[499 | 2256.07] loss=3.21 avg=3.48\n",
      "[500 | 2260.50] loss=3.55 avg=3.48\n",
      "======== SAMPLE 1 ========\n",
      " as he came out of the dark room, as I did, I did not look at him as he was.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter Three\n",
      "\n",
      "\n",
      "THE VACATION TO BE PERIODED\n",
      "\n",
      "As in a novel, allusions are to be removed in a single word—that word, the word which makes the thing stand its own. There is an element of the second, to be sure, in the first sentence of the first. A third sentence begins with a \"b\" and ends with the \"b.\" This is to be avoided for it might become \"a.\" In other words, only a third letter would have been \"a.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE FIRST VISIT BY HENRY L. PRIVIERE\n",
      "\n",
      "\n",
      "For the first time in its history, the first page of the first novel, The Voyage of the Gifted Knight at the end of the first volume, depicts the world of the Knight—its starry-colored streets, huge lakes, gigantic mountains, wild forests, a tall wall of red tape and blue water, and a long, narrow canal, with its blue-painted bridges and arches, as seen from Berlin's central station. The first two (the lake, the lake) are depicted in one-half detail, but they are not necessarily the same, and there is another discrepancy of position, where a blue bridge stands up as it would on a blue bridge of a blue butterfly butterfly. The main difference with this painting is the absence of the title, its plainness, the fact that the water is on the other side of the bridge.\n",
      "\n",
      "A couple of pages later, in the second volume (that very same year), we find a picture of the castle where the Knight sits resting his head. This is in the form of a large oak, the picture is very very rough and the whole is quite small, except for one passage out of the first chapter. The story takes us to Potsdam. This is the second volume of The Voyage of the Gifted Knight, its last entry, but it is not the last. The story is a great romance, the love between the Knight and his wife is so great that it will not be possible to finish the whole thing, and the tale is interrupted by the fact that in the end, the two men manage to get along well enough.\n",
      "\n",
      "The third volume, entitled The Voyage of the Gifted Knight, is, as mentioned in the first volume, about three times as long. The first volume is a collection of tales. Its conclusion is written in the fourth century, in which time has run out of all forms of narrative. A third volume has to be written by the ninth century, and the fourth volume is a collection of books—the first is The Law of the Sea. This fourth volume would be a complete work, of three-fourths of a century.\n",
      "\n",
      "THE TONGUE OF DARKNESS\n",
      "\n",
      "\n",
      "The book contains three tales, each of which can be enjoyed by all kinds of people. The third tale, About the Devil is very strong as a result of the constant reconditeings with which the people who have read the stories have to deal in various situations with the Devil as a person. It is very brief, and if the readers are not interested, they will not be allowed one chapter or two of this or that story.\n",
      "\n",
      "Another story in the fourth volume is About the Red Knight, a character of unknown origin. There is no mention of him in the story, or even in this or that passage of the story—it is the narrator who keeps talking. The fact of his being present in our minds or at some other place is not mentioned. There is also little or no description of the circumstances in which his being at that time may have been a part of our existence.\n",
      "\n",
      "The fifth book, Being the Way I am, consists of three or four stories, but it is not the last. It is very long, and when I finish the first one I shall write in it three or four stories, all of them in the order of the five stories in The Law of the Sea.\n",
      "\n",
      "The whole thing is a rather complicated affair, and is very depressing to read. I must admit that I am rather aching myself up, and that it may be hard to say just how awful it is, and what to do about it, if I do not feel sure it is all right. At the same time I would like to know, by whom, and how often, and how many times a day, how many pages of material are on my desk, how many pages of pictures of certain objects have been preserved, and how much of what they were in these books; but I have not found out a single one of them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.1 THE VOYAGE OF THE Gifted Knight\n",
      "\n",
      "\n",
      "I must have had a lot of trouble in writing this one, but now I have done in vain, and, having got so far\n",
      "\n",
      "[501 | 2281.06] loss=3.20 avg=3.47\n",
      "[502 | 2285.43] loss=3.30 avg=3.47\n",
      "[503 | 2289.82] loss=3.44 avg=3.47\n",
      "[504 | 2294.19] loss=3.20 avg=3.47\n",
      "[505 | 2298.56] loss=3.07 avg=3.47\n",
      "[506 | 2302.94] loss=3.60 avg=3.47\n",
      "[507 | 2307.32] loss=3.61 avg=3.47\n",
      "[508 | 2311.71] loss=3.28 avg=3.47\n",
      "[509 | 2316.08] loss=3.07 avg=3.46\n",
      "[510 | 2320.45] loss=3.47 avg=3.46\n",
      "[511 | 2324.82] loss=3.59 avg=3.46\n",
      "[512 | 2329.21] loss=3.22 avg=3.46\n",
      "[513 | 2333.61] loss=3.42 avg=3.46\n",
      "[514 | 2337.97] loss=3.25 avg=3.46\n",
      "[515 | 2342.35] loss=3.37 avg=3.46\n",
      "[516 | 2346.75] loss=3.31 avg=3.46\n",
      "[517 | 2351.13] loss=3.25 avg=3.45\n",
      "[518 | 2355.51] loss=3.26 avg=3.45\n",
      "[519 | 2359.88] loss=3.27 avg=3.45\n",
      "[520 | 2364.25] loss=3.20 avg=3.45\n",
      "[521 | 2368.64] loss=3.61 avg=3.45\n",
      "[522 | 2373.03] loss=3.53 avg=3.45\n",
      "[523 | 2377.42] loss=3.09 avg=3.45\n",
      "[524 | 2381.80] loss=3.51 avg=3.45\n",
      "[525 | 2386.20] loss=3.78 avg=3.45\n",
      "[526 | 2390.58] loss=3.37 avg=3.45\n",
      "[527 | 2394.97] loss=3.15 avg=3.45\n",
      "[528 | 2399.36] loss=3.41 avg=3.45\n",
      "[529 | 2403.75] loss=3.57 avg=3.45\n",
      "[530 | 2408.14] loss=3.61 avg=3.45\n",
      "[531 | 2412.52] loss=3.49 avg=3.45\n",
      "[532 | 2416.90] loss=3.15 avg=3.45\n",
      "[533 | 2421.30] loss=3.09 avg=3.44\n",
      "[534 | 2425.69] loss=3.26 avg=3.44\n",
      "[535 | 2430.08] loss=3.35 avg=3.44\n",
      "[536 | 2434.45] loss=3.50 avg=3.44\n",
      "[537 | 2438.83] loss=3.56 avg=3.44\n",
      "[538 | 2443.22] loss=3.28 avg=3.44\n",
      "[539 | 2447.61] loss=3.35 avg=3.44\n",
      "[540 | 2451.98] loss=3.48 avg=3.44\n",
      "[541 | 2456.37] loss=3.70 avg=3.44\n",
      "[542 | 2460.75] loss=3.07 avg=3.44\n",
      "[543 | 2465.14] loss=3.67 avg=3.44\n",
      "[544 | 2469.52] loss=3.38 avg=3.44\n",
      "[545 | 2473.91] loss=3.42 avg=3.44\n",
      "[546 | 2478.30] loss=3.06 avg=3.44\n",
      "[547 | 2482.69] loss=3.28 avg=3.44\n",
      "[548 | 2487.07] loss=3.44 avg=3.44\n",
      "[549 | 2491.45] loss=3.83 avg=3.44\n",
      "[550 | 2495.82] loss=3.34 avg=3.44\n",
      "[551 | 2500.19] loss=3.37 avg=3.44\n",
      "[552 | 2504.56] loss=3.18 avg=3.44\n",
      "[553 | 2508.94] loss=3.34 avg=3.43\n",
      "[554 | 2513.33] loss=3.32 avg=3.43\n",
      "[555 | 2517.72] loss=3.62 avg=3.44\n",
      "[556 | 2522.10] loss=3.69 avg=3.44\n",
      "[557 | 2526.48] loss=3.23 avg=3.44\n",
      "[558 | 2530.85] loss=3.66 avg=3.44\n",
      "[559 | 2535.22] loss=3.37 avg=3.44\n",
      "[560 | 2539.58] loss=3.68 avg=3.44\n",
      "[561 | 2543.96] loss=3.48 avg=3.44\n",
      "[562 | 2548.36] loss=3.57 avg=3.44\n",
      "[563 | 2552.75] loss=3.79 avg=3.44\n",
      "[564 | 2557.13] loss=3.27 avg=3.44\n",
      "[565 | 2561.51] loss=3.37 avg=3.44\n",
      "[566 | 2565.89] loss=3.31 avg=3.44\n",
      "[567 | 2570.25] loss=3.47 avg=3.44\n",
      "[568 | 2574.61] loss=3.00 avg=3.44\n",
      "[569 | 2578.98] loss=3.51 avg=3.44\n",
      "[570 | 2583.36] loss=3.15 avg=3.43\n",
      "[571 | 2587.73] loss=3.36 avg=3.43\n",
      "[572 | 2592.10] loss=3.39 avg=3.43\n",
      "[573 | 2596.49] loss=3.36 avg=3.43\n",
      "[574 | 2600.87] loss=3.37 avg=3.43\n",
      "[575 | 2605.25] loss=3.38 avg=3.43\n",
      "[576 | 2609.64] loss=2.87 avg=3.43\n",
      "[577 | 2614.03] loss=3.22 avg=3.42\n",
      "[578 | 2618.42] loss=3.13 avg=3.42\n",
      "[579 | 2622.80] loss=3.43 avg=3.42\n",
      "[580 | 2627.19] loss=3.20 avg=3.42\n",
      "[581 | 2631.57] loss=3.40 avg=3.42\n",
      "[582 | 2635.93] loss=3.54 avg=3.42\n",
      "[583 | 2640.30] loss=2.96 avg=3.42\n",
      "[584 | 2644.67] loss=3.29 avg=3.41\n",
      "[585 | 2649.05] loss=3.27 avg=3.41\n",
      "[586 | 2653.41] loss=3.30 avg=3.41\n",
      "[587 | 2657.78] loss=3.34 avg=3.41\n",
      "[588 | 2662.17] loss=3.22 avg=3.41\n",
      "[589 | 2666.54] loss=3.09 avg=3.41\n",
      "[590 | 2670.93] loss=3.63 avg=3.41\n",
      "[591 | 2675.31] loss=3.14 avg=3.41\n",
      "[592 | 2679.68] loss=3.18 avg=3.40\n",
      "[593 | 2684.06] loss=3.59 avg=3.40\n",
      "[594 | 2688.45] loss=3.61 avg=3.41\n",
      "[595 | 2692.84] loss=3.19 avg=3.40\n",
      "[596 | 2697.23] loss=3.22 avg=3.40\n",
      "[597 | 2701.61] loss=2.90 avg=3.40\n",
      "[598 | 2706.00] loss=3.33 avg=3.40\n",
      "[599 | 2710.39] loss=3.28 avg=3.40\n",
      "[600 | 2714.75] loss=3.36 avg=3.40\n",
      "======== SAMPLE 1 ========\n",
      " willpower and the love that is yours.\n",
      "\n",
      "As in the past, every year, in the spring, I would go to the theatre, where some other theatre-going couple in a hotel was about to go — and I did not feel at home. The day I saw him, on his third flight, after a seven-hour journey (with an additional three hours for the long journey in a small town to the west) I felt a strange, inexplicable urge to embrace her and, on the day she died, to embrace her.\n",
      "\n",
      "I was a little reluctant to take that flight and go back to the one-man-night-bar where my usual companions were sleeping, since I felt very weak in some places. I remembered distinctly the little house on the corner of the highway where the train left London, the red couch nearby, and on the way I heard him telling newsmen on the platform that the train was nearing, but I did not remember the train and the train-station entrance gate. Somewhere along the highway was a street where it had once been, when my parents and one of my late father's closest friends visited it in the summer of 1869. On the opposite road it was very much the opposite house from which the train always passed on Friday, during the fortnight that I was leaving for England.\n",
      "\n",
      "When my parents and I retired to the country, the train left with another station at a little town near my own but with a very distant second-class compartment. But I could not imagine why it should pass so suddenly. Had there been no train, my father might have left me and no one would have noticed: I had no idea.\n",
      "\n",
      "So I did not write the whole text of “Bravne,” but rather made it look as if I should do so, not because I did not need it, I did not intend it; for it is the first time I have suddenly had a chance to give the passage a try.\n",
      "\n",
      "I remember the train being delayed for several days at an unknown station, with three other stations that had also been mentioned. Its first stop was at the airport, where it took me three hours to reach Bournemouth, and then the second, at the airport. The two other departures were at the airport, and then at Berlin, in Berlin, and then finally at Paris, near L'Eton. At Berlin I went on to Paris and Berlin. And, as I was leaving, I could see the train moving up the avenue, and then I heard the train moving behind me again, and then I saw the train approaching again; an instant later came the train again.\n",
      "\n",
      "My father went to London but, having been sent for, his letter did not appear at all. A few days later I found him and his wife in Berlin. In a letter to the same letter I did not find out that the letter had indeed been found. I had hoped it might be found. In another letter I found it. I did not quite understand what had been written and what had remained of the letter. I did not write it with a kind soul, as I remember seeing from the railway office and then from another office, the one I had visited once or twice since 1869, where in the course of the last year all three trains had stopped. Therefore I decided to try my hand at the letter, so as to get at the whole point.\n",
      "\n",
      "I found at the office the exact place of a letter written by my father. The letter addressed to him and signed by its author, which had already been discovered, was placed on a large tray on the desk where I had sat, under an interesting and uncomfortable part, as if it were just a book which one had forgotten to open; but on the same shelf there was a large envelope with the address on it and a note addressed to me, explaining a very important note which I had received.\n",
      "\n",
      "I tried to look after the letter, because there was only one problem. What was the matter with the letter? The matter being that I had read it only once. What was the matter with the letter? I had never read the letter. What was the matter with the letter? This had always irritated me, and I thought, it was the reason why I had not read the letter on my first try.\n",
      "\n",
      "I started to read it at first on a bench near the railway station. There it had a long crossword puzzle inside and there was a long-drawn-out letter addressed to me with the first letter of the alphabet, which had been in my bag.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A similar thing has happened to me on other occasions; for, as I have always felt that the letter had never been typed in a comfortable position — at least not in the first place, since I always found it impossible to write it in this position once or twice, and was so clumsy and awkward for the time being because I had not had the time to make up my mind, and I\n",
      "\n",
      "[601 | 2735.31] loss=3.10 avg=3.39\n",
      "[602 | 2739.67] loss=3.21 avg=3.39\n",
      "[603 | 2744.04] loss=3.26 avg=3.39\n",
      "[604 | 2748.41] loss=3.61 avg=3.39\n",
      "[605 | 2752.79] loss=3.29 avg=3.39\n",
      "[606 | 2757.16] loss=3.47 avg=3.39\n",
      "[607 | 2761.54] loss=3.20 avg=3.39\n",
      "[608 | 2765.91] loss=3.37 avg=3.39\n",
      "[609 | 2770.29] loss=3.39 avg=3.39\n",
      "[610 | 2774.66] loss=3.45 avg=3.39\n",
      "[611 | 2779.04] loss=3.58 avg=3.39\n",
      "[612 | 2783.43] loss=3.47 avg=3.39\n",
      "[613 | 2787.81] loss=3.42 avg=3.39\n",
      "[614 | 2792.20] loss=3.56 avg=3.39\n",
      "[615 | 2796.58] loss=3.17 avg=3.39\n",
      "[616 | 2800.97] loss=3.46 avg=3.39\n",
      "[617 | 2805.37] loss=3.52 avg=3.39\n",
      "[618 | 2809.74] loss=3.12 avg=3.39\n",
      "[619 | 2814.13] loss=2.85 avg=3.39\n",
      "[620 | 2818.53] loss=3.18 avg=3.38\n",
      "[621 | 2822.91] loss=3.28 avg=3.38\n",
      "[622 | 2827.31] loss=3.30 avg=3.38\n",
      "[623 | 2831.69] loss=3.10 avg=3.38\n",
      "[624 | 2836.07] loss=3.32 avg=3.38\n",
      "[625 | 2840.44] loss=3.17 avg=3.38\n",
      "[626 | 2844.84] loss=3.38 avg=3.38\n",
      "[627 | 2849.24] loss=3.66 avg=3.38\n",
      "[628 | 2853.62] loss=3.69 avg=3.38\n",
      "[629 | 2858.02] loss=3.16 avg=3.38\n",
      "[630 | 2862.40] loss=3.31 avg=3.38\n",
      "[631 | 2866.76] loss=3.50 avg=3.38\n",
      "[632 | 2871.14] loss=3.20 avg=3.38\n",
      "[633 | 2875.52] loss=3.30 avg=3.38\n",
      "[634 | 2879.90] loss=3.47 avg=3.38\n",
      "[635 | 2884.28] loss=3.27 avg=3.38\n",
      "[636 | 2888.65] loss=3.36 avg=3.38\n",
      "[637 | 2893.04] loss=3.46 avg=3.38\n",
      "[638 | 2897.41] loss=3.32 avg=3.38\n",
      "[639 | 2901.80] loss=3.38 avg=3.38\n",
      "[640 | 2906.21] loss=3.12 avg=3.38\n",
      "[641 | 2910.58] loss=2.99 avg=3.37\n",
      "[642 | 2914.96] loss=3.16 avg=3.37\n",
      "[643 | 2919.33] loss=3.42 avg=3.37\n",
      "[644 | 2923.72] loss=3.43 avg=3.37\n",
      "[645 | 2928.11] loss=3.27 avg=3.37\n",
      "[646 | 2932.48] loss=3.55 avg=3.37\n",
      "[647 | 2936.87] loss=3.38 avg=3.37\n",
      "[648 | 2941.24] loss=3.47 avg=3.37\n",
      "[649 | 2945.63] loss=3.66 avg=3.38\n",
      "[650 | 2950.01] loss=3.24 avg=3.37\n",
      "[651 | 2954.39] loss=3.55 avg=3.38\n",
      "[652 | 2958.77] loss=3.20 avg=3.37\n",
      "[653 | 2963.16] loss=3.49 avg=3.37\n",
      "[654 | 2967.54] loss=3.23 avg=3.37\n",
      "[655 | 2971.92] loss=3.13 avg=3.37\n",
      "[656 | 2976.30] loss=3.52 avg=3.37\n",
      "[657 | 2980.68] loss=3.73 avg=3.38\n",
      "[658 | 2985.07] loss=3.40 avg=3.38\n",
      "[659 | 2989.46] loss=3.51 avg=3.38\n",
      "[660 | 2993.84] loss=3.23 avg=3.38\n",
      "[661 | 2998.23] loss=3.47 avg=3.38\n",
      "[662 | 3002.63] loss=3.28 avg=3.38\n",
      "[663 | 3007.01] loss=3.21 avg=3.37\n",
      "[664 | 3011.39] loss=3.30 avg=3.37\n",
      "[665 | 3015.79] loss=3.43 avg=3.37\n",
      "[666 | 3020.17] loss=3.21 avg=3.37\n",
      "[667 | 3024.58] loss=3.15 avg=3.37\n",
      "[668 | 3028.98] loss=3.17 avg=3.37\n",
      "[669 | 3033.37] loss=3.30 avg=3.37\n",
      "[670 | 3037.76] loss=3.37 avg=3.37\n",
      "[671 | 3042.16] loss=3.19 avg=3.37\n",
      "[672 | 3046.56] loss=3.16 avg=3.36\n",
      "[673 | 3050.95] loss=3.16 avg=3.36\n",
      "[674 | 3055.33] loss=3.11 avg=3.36\n",
      "[675 | 3059.72] loss=3.42 avg=3.36\n",
      "[676 | 3064.11] loss=3.16 avg=3.36\n",
      "[677 | 3068.50] loss=3.33 avg=3.36\n",
      "[678 | 3072.89] loss=3.42 avg=3.36\n",
      "[679 | 3077.27] loss=3.72 avg=3.36\n",
      "[680 | 3081.66] loss=3.36 avg=3.36\n",
      "[681 | 3086.06] loss=3.11 avg=3.36\n",
      "[682 | 3090.44] loss=3.45 avg=3.36\n",
      "[683 | 3094.84] loss=2.65 avg=3.35\n",
      "[684 | 3099.22] loss=3.30 avg=3.35\n",
      "[685 | 3103.61] loss=3.63 avg=3.36\n",
      "[686 | 3107.99] loss=3.44 avg=3.36\n",
      "[687 | 3112.36] loss=3.47 avg=3.36\n",
      "[688 | 3116.75] loss=3.09 avg=3.35\n",
      "[689 | 3121.13] loss=3.18 avg=3.35\n",
      "[690 | 3125.51] loss=3.40 avg=3.35\n",
      "[691 | 3129.88] loss=3.46 avg=3.35\n",
      "[692 | 3134.27] loss=3.59 avg=3.36\n",
      "[693 | 3138.65] loss=3.21 avg=3.36\n",
      "[694 | 3143.04] loss=3.19 avg=3.35\n",
      "[695 | 3147.43] loss=3.18 avg=3.35\n",
      "[696 | 3151.82] loss=3.39 avg=3.35\n",
      "[697 | 3156.21] loss=2.96 avg=3.35\n",
      "[698 | 3160.60] loss=3.28 avg=3.35\n",
      "[699 | 3164.98] loss=2.84 avg=3.34\n",
      "[700 | 3169.37] loss=3.08 avg=3.34\n",
      "======== SAMPLE 1 ========\n",
      " turn, there was that bright gleam of white that was so familiar—and now only one thing remained: the same pink as when she first left school or the pale reflection of her nose that always came after that. She still felt something like that after five, six and seven. She had to find something. She had to find the reason of her life. She felt it would be easier to kill herself than it would be possible to live.\n",
      "\n",
      "At daybreak, as she was going to work, his shadow—now on the wall, now in the gloom—took off all the white, and now the red and blue of the night came flying away and the shadows vanished.\n",
      "\n",
      "She began to imagine an endless sea of dreams. She got lost and had to find some convenient place. She had to come from nowhere to her own bed.\n",
      "\n",
      "Her dreams—the blackness of the sea, her dream that had once surrounded her, that her dream of death—took on a different, more terrifying quality when she again found herself suddenly immersed in darkness (a dream of the future that she knew and cherished, in which she had now been made aware of the fact that she was a ghost and that her dream belonged to another, but completely different category). She moved off to a secluded place and now, having arrived to her old house, she thought about such things as the shape of the house, the type of clothes (which she wore), the colors of the floor, the shapes of the drawers—everything. Suddenly she stopped in her turn and thought of the past she must see now; of her childhood, of her life in the past itself.\n",
      "\n",
      "On her way home, she came to an empty house with an empty window and suddenly felt ashamed. The smell of the night was unbearable and her thoughts wandered in and out of her. As she walked on slowly uphill against the wall for the last time, her vision dimmed for a long time. Suddenly she came to a halt beside the bed and the wall. One could see the dim outline of the window, the outline of its broken frame: a shabby and shabby place, with a white sheet of paper, and on the sheet a single, narrow bed. She was afraid she would drown somewhere—and was afraid.\n",
      "\n",
      "Next day, in the country her fear grew worse. The window would shatter and the sun would be cast over the house. She stopped at the gate. She was frightened. She had to come, to a place where it was dark, it would be impossible to live there in this darkness, in all its horrors, and she found herself suddenly at a loss. She was terrified again, she had to find someone to look after herself. She felt that everything would stop and everybody would start talking about the death of her father. And, she resolved to find someone to talk to.… The first person she saw who spoke to her was the pale-faced shadow of a shopkeeper with a small, mauve hand. Her heart swelled, and she thought with such a sudden, like a little red flower that it burst, and something stopped in its place—something in place—an empty chair—that shadow.…\n",
      "\n",
      "How did things go? A house in a town, or some dreadful story in a church book? She turned into the dark room and there one was suddenly struck by a sudden and violent cold, for the light was broken—and, in a flash, he was gone. She turned around. Her husband was already on his feet—and that stunned her.\n",
      "\n",
      "She sat on a chair at night, blinking, shivering, and, as one who knows someone knows that blouse, her thoughts raced and she could not help imagining something horrible. And then everything, everything changed and all the time she was about to fall silent, as if everything had happened so quickly without her having seen it yet, and everything had changed for the time being as if she had not yet noticed it.\n",
      "\n",
      "A long distance, a long way to a different house—he seemed to have gone. As she walked across the road, she was afraid that her heart would break and she would burst into a fit of crying and she was glad to see that her head was healthy and that the window was looking right.\n",
      "\n",
      "She entered an empty shop and there there, through the blackness of the night, a pair of little black shorts were wrapped up in their black silk stockings. On the day of her arrival there was no school, so she came to go on a business trip and sat with her business friend—and when she began to think something was terribly wrong, she began to think that she ought not to talk to anyone about it.\n",
      "\n",
      "She opened her suitcase, put on a white nightgown, laid down on the floor, and in the room was a kind of puddle, like a small pond, with blue water all over it. There was a yellow spot somewhere near to it\n",
      "\n",
      "[701 | 3189.93] loss=3.62 avg=3.34\n",
      "[702 | 3194.32] loss=3.20 avg=3.34\n",
      "[703 | 3198.70] loss=3.05 avg=3.34\n",
      "[704 | 3203.09] loss=3.19 avg=3.34\n",
      "[705 | 3207.46] loss=3.56 avg=3.34\n",
      "[706 | 3211.87] loss=3.11 avg=3.34\n",
      "[707 | 3216.24] loss=3.16 avg=3.34\n",
      "[708 | 3220.63] loss=3.48 avg=3.34\n",
      "[709 | 3225.02] loss=2.90 avg=3.33\n",
      "[710 | 3229.42] loss=3.28 avg=3.33\n",
      "[711 | 3233.80] loss=3.23 avg=3.33\n",
      "[712 | 3238.18] loss=3.34 avg=3.33\n",
      "[713 | 3242.56] loss=3.42 avg=3.33\n",
      "[714 | 3246.95] loss=3.25 avg=3.33\n",
      "[715 | 3251.35] loss=3.42 avg=3.33\n",
      "[716 | 3255.74] loss=3.40 avg=3.33\n",
      "[717 | 3260.12] loss=3.59 avg=3.34\n",
      "[718 | 3264.53] loss=3.32 avg=3.34\n",
      "[719 | 3268.91] loss=3.13 avg=3.33\n",
      "[720 | 3273.30] loss=3.68 avg=3.34\n",
      "[721 | 3277.69] loss=3.58 avg=3.34\n",
      "[722 | 3282.09] loss=3.38 avg=3.34\n",
      "[723 | 3286.49] loss=3.47 avg=3.34\n",
      "[724 | 3290.86] loss=2.85 avg=3.34\n",
      "[725 | 3295.25] loss=3.70 avg=3.34\n",
      "[726 | 3299.63] loss=3.36 avg=3.34\n",
      "[727 | 3304.02] loss=3.65 avg=3.34\n",
      "[728 | 3308.40] loss=3.31 avg=3.34\n",
      "[729 | 3312.78] loss=3.49 avg=3.34\n",
      "[730 | 3317.17] loss=2.96 avg=3.34\n",
      "[731 | 3321.55] loss=3.66 avg=3.34\n",
      "[732 | 3325.95] loss=3.20 avg=3.34\n",
      "[733 | 3330.34] loss=3.36 avg=3.34\n",
      "[734 | 3334.72] loss=3.53 avg=3.34\n",
      "[735 | 3339.08] loss=3.43 avg=3.34\n",
      "[736 | 3343.48] loss=3.13 avg=3.34\n",
      "[737 | 3347.87] loss=3.28 avg=3.34\n",
      "[738 | 3352.26] loss=3.30 avg=3.34\n",
      "[739 | 3356.65] loss=3.33 avg=3.34\n",
      "[740 | 3361.02] loss=3.40 avg=3.34\n",
      "[741 | 3365.41] loss=3.37 avg=3.34\n",
      "[742 | 3369.80] loss=3.44 avg=3.34\n",
      "[743 | 3374.18] loss=3.32 avg=3.34\n",
      "[744 | 3378.55] loss=3.12 avg=3.34\n",
      "[745 | 3382.94] loss=3.28 avg=3.34\n",
      "[746 | 3387.32] loss=3.16 avg=3.34\n",
      "[747 | 3391.71] loss=3.31 avg=3.34\n",
      "[748 | 3396.10] loss=2.95 avg=3.33\n",
      "[749 | 3400.48] loss=3.22 avg=3.33\n",
      "[750 | 3404.86] loss=3.26 avg=3.33\n",
      "[751 | 3409.25] loss=3.22 avg=3.33\n",
      "[752 | 3413.64] loss=3.27 avg=3.33\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "# set number of steps for finetuning or load the pretrained model\n",
    "# if a model is already mounted, no training required\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "# load an existing, fine-tuned model\n",
    "# gpt2.load_gpt2(sess)\n",
    "#\n",
    "# finetuning frequently fails in collab based on resource exhaustion\n",
    "# Good training duration results in a loss of around 0.1-0.2 (over 10K iterations)\n",
    "# GPT-2 can generate coherent output even on low finetuning, but with a loss of style\n",
    "\n",
    "# gpt2.finetune(sess, style, model_name=model_name, overwrite=True, steps=1000)\n",
    "# continue from an existing model\n",
    "# gpt2.finetune(sess, style, model_name=model_name, restore_from = 'latest', overwrite=True, steps=1)\n",
    "\n",
    "# fine-tune from scratch\n",
    "\n",
    "gpt2.finetune(sess, style, model_name=model_name, overwrite=True, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePqv72z4Sn8_"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# take a list of unstructured expansions and tidy them up\n",
    "# expansion formation rules: seed phrase deleted, truncation at first end \n",
    "# of sentence, the last comma, or allow to run off unclipped otherwise\n",
    "# \n",
    "def cleaned(expansions, seed):\n",
    "\n",
    "   seedbag = word_tokenize(seed)\n",
    "   cutseed = dt.detokenize(seedbag[1:])\n",
    "   symlen = len(cutseed)\n",
    "   cleaned = []\n",
    "   expansion = \"\"\n",
    "   for exp in expansions:\n",
    "      pos = exp.find(cutseed)\n",
    "      if pos > 0:\n",
    "         expansion = exp[pos+symlen:].strip()\n",
    "      else:\n",
    "         expansion = exp\n",
    "      pos = expansion.find(\".\")\n",
    "      if pos > 0:\n",
    "         expansion = expansion[:pos]+\".\"\n",
    "      else:\n",
    "         pos = expansion[::-1].find(\",\")\n",
    "         if pos > 0:\n",
    "            expansion = expansion[:-pos]\n",
    "      cleaned.append(expansion)\n",
    "      \n",
    "   return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OL9417-w-dLJ"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# define the next piece to replace in the donor text\n",
    "def generateLexeme(sent):\n",
    "\n",
    "      if sent == \"\":\n",
    "         return\n",
    "\n",
    "      words = word_tokenize(sent)\n",
    "\n",
    "      lexeme = \"\"\n",
    "      count = 0\n",
    "\n",
    "      if len(words) <= maxwords:\n",
    "         processLexeme(sent)\n",
    "\n",
    "      else:\n",
    "         for word in words:\n",
    "            count += 1\n",
    "            if (word in [\";\", \",\",\"-\",\"--\"] and count > minwords) or (word in [\"and\", \"for\", \"at\"] and count > softmaxwords) or count > maxwords:\n",
    "               if (len(words)-count) >  minwords:\n",
    "                  processLexeme(dt.detokenize(words[:count]))\n",
    "                  sentence = dt.detokenize(words[count:])\n",
    "                  generateLexeme(sentence)\n",
    "               else:\n",
    "                  processLexeme(dt.detokenize(words))\n",
    "\n",
    "               break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDmLlzZo-vpj"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# main RPS routine: extract a lexeme, generate candidates, replace with a best one\n",
    "def processLexeme(sent):\n",
    "\n",
    "   # always start from the first phrase\n",
    "   if not bestLexemes:\n",
    "      bestLexemes.append(sent)\n",
    "      print(\"Leading output with seed phrase: {}\".format(sent))\n",
    "      return\n",
    "    \n",
    "   seed = dt.detokenize(bestLexemes[-seedLexemes:])\n",
    "   print(\"seed: \\\"{}\\\"\".format(seed))\n",
    "   print(\"\\t  processing lexeme: \\\"{}\\\"\".format(sent))\n",
    "   \n",
    "   expansions = gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, return_as_list=True, include_prefix=False)\n",
    "   #expansions = gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, truncate=\".\", return_as_list=True, include_prefix=False)\n",
    "   #expansions += gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, truncate=\",\", return_as_list=True, include_prefix=False)\n",
    "   expansions = cleaned(expansions, seed)\n",
    "   #print(\"  expansion set: {}\".format(expansions))\n",
    "  \n",
    "   messages = [sent]+expansions\n",
    "   bestLexeme = sent\n",
    "  \n",
    "   gc.collect()\n",
    "\n",
    "   with tf.Session(config=config) as session:\n",
    "      session.run(tf.global_variables_initializer())\n",
    "      session.run(tf.tables_initializer())\n",
    "      message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})\n",
    "\n",
    "      corr = np.inner(message_embeddings_, message_embeddings_)\n",
    "      embeddings = corr[0,1:]\n",
    "      #print(\"embeddings: {}\".format(embeddings))\n",
    "      \n",
    "      bestIndex = np.argmax(embeddings)\n",
    "      \n",
    "      if embeddings[bestIndex] >  minsimilarity:\n",
    "        \n",
    "         bestLexeme = expansions[bestIndex]\n",
    "            \n",
    "         # stats       \n",
    "         # calculate running average for accepted scores\n",
    "\n",
    "         acceptedLexemes[\"accepted\"] = acceptedLexemes[\"accepted\"]+1\n",
    "         acceptedLexemes[\"candidate_percent\"].append(np.sum(embeddings > minsimilarity)/nsamples)\n",
    "            \n",
    "         # join expansion with future lexeme smoothly\n",
    "        \n",
    "         originalEnding = sent[-1]\n",
    "         end = originalEnding if originalEnding in [\".\",\",\", \";\", \"-\", \"--\", \"?\", \"!\", \"...\", \"and\",\"for\", \"at\"] else \"\"\n",
    "        \n",
    "         lexemeEnding = bestLexeme[-1]\n",
    "         \n",
    "         # replace punctuation at end of lexeme if original punctuation existed\n",
    "         # if donor sentence not ending yet, ignore the expansion markers\n",
    "         if lexemeEnding in [\".\", \",\", \";\", \"?\", \"!\", \"and\", \"or\", \"for\", \"at\"]:\n",
    "            bestLexeme = bestLexeme[:-1]+end\n",
    "         # trying a plug from the content. Removing this line allows a lot of freedom to style engine.\n",
    "         else:\n",
    "            bestLexeme = bestLexeme+end\n",
    "            \n",
    "      else:\n",
    "         acceptedLexemes[\"rejected\"] = acceptedLexemes[\"rejected\"]+1  \n",
    "        \n",
    "   print(\"  expanding with: \\\"{}\\\" bestscore: {} bestphrase: {}\".format(bestLexeme, embeddings[bestIndex], expansions[bestIndex]))\n",
    "   bestLexemes.append(bestLexeme)\n",
    "\n",
    "   output = dt.detokenize(bestLexemes)\n",
    "   Path(outputfile).write_text(output)\n",
    "    \n",
    "   #stats \n",
    "   acceptedLexemes[\"total_score\"] = acceptedLexemes[\"total_score\"]+embeddings[bestIndex]\n",
    "   print(acceptedLexemes)\n",
    "   Path(statsfile).write_text(str(acceptedLexemes))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Goy4zoFhvH7e"
   },
   "outputs": [],
   "source": [
    "# !!!!! playground cell 1: test expansion quality of the model\n",
    "\n",
    "seed = \"This opportunity leads us to nowhere.\"\n",
    "\n",
    "expansions = gpt2.generate(sess, nsamples=10, batch_size=10, length=20, prefix=seed, return_as_list=True, include_prefix=False)\n",
    "\n",
    "# get around a bug \n",
    "print(expansions)\n",
    "expansions = cleaned(expansions,seed)\n",
    "print(expansions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rze1G485vSFA"
   },
   "outputs": [],
   "source": [
    "# !!!!! playground cell 2: test lexeme processor\n",
    "\n",
    "# first lexeme always identity transform\n",
    "# generateLexeme(\" there never was a better place than\")\n",
    "\n",
    "bestLexemes = []\n",
    "generateLexeme(\"There never was a better place.\")\n",
    "print(bestLexemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1363
    },
    "colab_type": "code",
    "id": "hNj7LbB7JsMU",
    "outputId": "0d2b102d-8d83-4b91-a6bc-40edc04b94e8"
   },
   "outputs": [],
   "source": [
    "# launch RPS from here.\n",
    "# \n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    outputfile =  outputfile+str(i)\n",
    "    statsfile = statsfile + str(i)\n",
    "    \n",
    "    bestLexemes = []\n",
    "    acceptedLexemes = {\"accepted\":0, \"rejected\":0, \"total_score\": 0, \"candidate_percent\":[]}\n",
    "\n",
    "    sentences = tokenizer.tokenize(input)\n",
    "\n",
    "    for sent in sentences:\n",
    "       print(\"Input sentence: {}\".format(sent))\n",
    "       generateLexeme(sent)\n",
    "\n",
    "    output = dt.detokenize(bestLexemes)\n",
    "    #Path(outputfile).write_text(output)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(output)\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestLexemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "RPS.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
