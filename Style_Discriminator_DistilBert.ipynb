{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "1b1bbe93-cf93-4313-9b95-74ed01ef9586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\r\u001b[K     |█                               | 10kB 27.4MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 54.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 63.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=c598ebb1c1121fd35ed65ebc480e9919273670a8af93c106f6ff2aedabe9967c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sacremoses, sentencepiece, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "41ac38ff-64ec-4fc1-f927-dcd4c6d1b818"
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Embeddings()\n",
        "\n",
        "X_text = []\n",
        "for sentence in tqdm(X):\n",
        "    X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "578a95d2-eaa1-407a-bea3-6f41b2e46df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUmYQdKgdUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "53d2c8ba-a1ad-4a92-fb33-760594e51aa9"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>-0.131176</td>\n",
              "      <td>0.266133</td>\n",
              "      <td>0.148921</td>\n",
              "      <td>-0.081992</td>\n",
              "      <td>0.240797</td>\n",
              "      <td>0.114559</td>\n",
              "      <td>0.177920</td>\n",
              "      <td>0.347606</td>\n",
              "      <td>-0.077707</td>\n",
              "      <td>-0.249226</td>\n",
              "      <td>-0.026828</td>\n",
              "      <td>-0.199450</td>\n",
              "      <td>-0.153355</td>\n",
              "      <td>0.141490</td>\n",
              "      <td>-0.139063</td>\n",
              "      <td>0.490254</td>\n",
              "      <td>0.275401</td>\n",
              "      <td>-0.005560</td>\n",
              "      <td>-0.148209</td>\n",
              "      <td>0.194834</td>\n",
              "      <td>0.182864</td>\n",
              "      <td>0.126983</td>\n",
              "      <td>-0.014447</td>\n",
              "      <td>0.492367</td>\n",
              "      <td>0.375321</td>\n",
              "      <td>0.009203</td>\n",
              "      <td>-0.024804</td>\n",
              "      <td>-0.014858</td>\n",
              "      <td>-0.232795</td>\n",
              "      <td>-0.135277</td>\n",
              "      <td>0.193110</td>\n",
              "      <td>0.110863</td>\n",
              "      <td>-0.075256</td>\n",
              "      <td>-0.265667</td>\n",
              "      <td>-0.019461</td>\n",
              "      <td>-0.203811</td>\n",
              "      <td>0.240621</td>\n",
              "      <td>-0.080151</td>\n",
              "      <td>0.055712</td>\n",
              "      <td>0.038758</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.055767</td>\n",
              "      <td>-0.280417</td>\n",
              "      <td>-0.054035</td>\n",
              "      <td>0.328030</td>\n",
              "      <td>-0.125772</td>\n",
              "      <td>-0.201065</td>\n",
              "      <td>0.365234</td>\n",
              "      <td>0.327626</td>\n",
              "      <td>-0.144236</td>\n",
              "      <td>0.004945</td>\n",
              "      <td>-0.197208</td>\n",
              "      <td>-0.018046</td>\n",
              "      <td>0.261606</td>\n",
              "      <td>0.026114</td>\n",
              "      <td>0.139331</td>\n",
              "      <td>-0.357711</td>\n",
              "      <td>-0.121657</td>\n",
              "      <td>0.028659</td>\n",
              "      <td>-0.138672</td>\n",
              "      <td>-0.115437</td>\n",
              "      <td>0.096502</td>\n",
              "      <td>0.072845</td>\n",
              "      <td>0.039624</td>\n",
              "      <td>0.048676</td>\n",
              "      <td>0.275667</td>\n",
              "      <td>-0.195468</td>\n",
              "      <td>-0.262197</td>\n",
              "      <td>-0.287638</td>\n",
              "      <td>0.105576</td>\n",
              "      <td>-0.193612</td>\n",
              "      <td>-0.144565</td>\n",
              "      <td>0.062709</td>\n",
              "      <td>0.076550</td>\n",
              "      <td>-0.242212</td>\n",
              "      <td>-0.015980</td>\n",
              "      <td>-0.177825</td>\n",
              "      <td>0.040596</td>\n",
              "      <td>-0.012199</td>\n",
              "      <td>0.064456</td>\n",
              "      <td>-0.037346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5383</th>\n",
              "      <td>-0.316470</td>\n",
              "      <td>0.154551</td>\n",
              "      <td>-0.044945</td>\n",
              "      <td>-0.193507</td>\n",
              "      <td>0.159468</td>\n",
              "      <td>0.101464</td>\n",
              "      <td>0.108350</td>\n",
              "      <td>0.450681</td>\n",
              "      <td>-0.141811</td>\n",
              "      <td>-0.141473</td>\n",
              "      <td>-0.005227</td>\n",
              "      <td>-0.256182</td>\n",
              "      <td>0.054740</td>\n",
              "      <td>0.135534</td>\n",
              "      <td>-0.370629</td>\n",
              "      <td>0.534947</td>\n",
              "      <td>0.196179</td>\n",
              "      <td>-0.011051</td>\n",
              "      <td>-0.148997</td>\n",
              "      <td>0.012289</td>\n",
              "      <td>0.093987</td>\n",
              "      <td>0.040672</td>\n",
              "      <td>-0.072825</td>\n",
              "      <td>0.373685</td>\n",
              "      <td>0.305296</td>\n",
              "      <td>-0.044486</td>\n",
              "      <td>0.228893</td>\n",
              "      <td>-0.092303</td>\n",
              "      <td>-0.059622</td>\n",
              "      <td>-0.159867</td>\n",
              "      <td>0.473357</td>\n",
              "      <td>0.427233</td>\n",
              "      <td>0.079723</td>\n",
              "      <td>0.078847</td>\n",
              "      <td>-0.323614</td>\n",
              "      <td>-0.110843</td>\n",
              "      <td>0.144783</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>0.005005</td>\n",
              "      <td>0.103818</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016835</td>\n",
              "      <td>-0.126305</td>\n",
              "      <td>0.052172</td>\n",
              "      <td>0.114835</td>\n",
              "      <td>-0.064550</td>\n",
              "      <td>-0.179597</td>\n",
              "      <td>0.026677</td>\n",
              "      <td>0.171687</td>\n",
              "      <td>0.091505</td>\n",
              "      <td>-0.021027</td>\n",
              "      <td>0.083243</td>\n",
              "      <td>-0.007528</td>\n",
              "      <td>0.388532</td>\n",
              "      <td>0.065488</td>\n",
              "      <td>0.352679</td>\n",
              "      <td>-0.443432</td>\n",
              "      <td>-0.234592</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.477019</td>\n",
              "      <td>0.176724</td>\n",
              "      <td>0.014132</td>\n",
              "      <td>0.030622</td>\n",
              "      <td>0.196392</td>\n",
              "      <td>0.064944</td>\n",
              "      <td>0.207005</td>\n",
              "      <td>-0.269276</td>\n",
              "      <td>-0.175285</td>\n",
              "      <td>-0.203082</td>\n",
              "      <td>-0.078734</td>\n",
              "      <td>-0.150577</td>\n",
              "      <td>0.058437</td>\n",
              "      <td>0.180866</td>\n",
              "      <td>-0.137352</td>\n",
              "      <td>-0.297719</td>\n",
              "      <td>0.085819</td>\n",
              "      <td>-0.410921</td>\n",
              "      <td>-0.111298</td>\n",
              "      <td>-0.090707</td>\n",
              "      <td>0.121389</td>\n",
              "      <td>0.118943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2448</th>\n",
              "      <td>0.092707</td>\n",
              "      <td>0.272643</td>\n",
              "      <td>0.176209</td>\n",
              "      <td>-0.011071</td>\n",
              "      <td>0.153302</td>\n",
              "      <td>0.005880</td>\n",
              "      <td>0.030753</td>\n",
              "      <td>0.435551</td>\n",
              "      <td>-0.104705</td>\n",
              "      <td>-0.141612</td>\n",
              "      <td>0.032267</td>\n",
              "      <td>-0.071691</td>\n",
              "      <td>-0.130723</td>\n",
              "      <td>0.386975</td>\n",
              "      <td>-0.050786</td>\n",
              "      <td>0.445956</td>\n",
              "      <td>0.196578</td>\n",
              "      <td>0.104146</td>\n",
              "      <td>0.028415</td>\n",
              "      <td>0.147619</td>\n",
              "      <td>0.151438</td>\n",
              "      <td>0.047032</td>\n",
              "      <td>-0.070511</td>\n",
              "      <td>0.388379</td>\n",
              "      <td>0.296974</td>\n",
              "      <td>0.110106</td>\n",
              "      <td>0.031055</td>\n",
              "      <td>-0.022608</td>\n",
              "      <td>-0.283120</td>\n",
              "      <td>0.126767</td>\n",
              "      <td>0.274742</td>\n",
              "      <td>0.136422</td>\n",
              "      <td>-0.042977</td>\n",
              "      <td>-0.120756</td>\n",
              "      <td>-0.094774</td>\n",
              "      <td>-0.196844</td>\n",
              "      <td>-0.077938</td>\n",
              "      <td>-0.223963</td>\n",
              "      <td>0.032770</td>\n",
              "      <td>0.127072</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.245162</td>\n",
              "      <td>-0.170411</td>\n",
              "      <td>-0.014785</td>\n",
              "      <td>0.122141</td>\n",
              "      <td>-0.168193</td>\n",
              "      <td>-0.117060</td>\n",
              "      <td>0.194875</td>\n",
              "      <td>0.269597</td>\n",
              "      <td>-0.005396</td>\n",
              "      <td>0.039881</td>\n",
              "      <td>-0.070641</td>\n",
              "      <td>0.019881</td>\n",
              "      <td>0.028769</td>\n",
              "      <td>0.117927</td>\n",
              "      <td>0.077930</td>\n",
              "      <td>-0.221662</td>\n",
              "      <td>-0.145010</td>\n",
              "      <td>-0.121209</td>\n",
              "      <td>-0.347139</td>\n",
              "      <td>-0.089856</td>\n",
              "      <td>0.136124</td>\n",
              "      <td>0.160794</td>\n",
              "      <td>0.081143</td>\n",
              "      <td>0.094066</td>\n",
              "      <td>0.478036</td>\n",
              "      <td>-0.168841</td>\n",
              "      <td>-0.209813</td>\n",
              "      <td>-0.216403</td>\n",
              "      <td>-0.003892</td>\n",
              "      <td>-0.019624</td>\n",
              "      <td>0.030146</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>-0.122808</td>\n",
              "      <td>-0.145937</td>\n",
              "      <td>0.055718</td>\n",
              "      <td>-0.297261</td>\n",
              "      <td>-0.016026</td>\n",
              "      <td>-0.006477</td>\n",
              "      <td>0.145422</td>\n",
              "      <td>0.028647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8412</th>\n",
              "      <td>-0.144989</td>\n",
              "      <td>0.249846</td>\n",
              "      <td>-0.032966</td>\n",
              "      <td>-0.074443</td>\n",
              "      <td>0.191077</td>\n",
              "      <td>-0.022496</td>\n",
              "      <td>0.185094</td>\n",
              "      <td>0.596307</td>\n",
              "      <td>-0.191237</td>\n",
              "      <td>-0.145253</td>\n",
              "      <td>-0.021918</td>\n",
              "      <td>-0.042338</td>\n",
              "      <td>0.046627</td>\n",
              "      <td>0.065985</td>\n",
              "      <td>-0.216331</td>\n",
              "      <td>0.465437</td>\n",
              "      <td>0.366972</td>\n",
              "      <td>-0.085402</td>\n",
              "      <td>-0.142934</td>\n",
              "      <td>0.186064</td>\n",
              "      <td>0.039708</td>\n",
              "      <td>0.114470</td>\n",
              "      <td>-0.166818</td>\n",
              "      <td>0.412560</td>\n",
              "      <td>0.276228</td>\n",
              "      <td>0.027942</td>\n",
              "      <td>0.212482</td>\n",
              "      <td>0.051142</td>\n",
              "      <td>-0.154804</td>\n",
              "      <td>-0.165632</td>\n",
              "      <td>0.330933</td>\n",
              "      <td>0.213092</td>\n",
              "      <td>-0.158626</td>\n",
              "      <td>-0.028376</td>\n",
              "      <td>-0.152750</td>\n",
              "      <td>-0.174787</td>\n",
              "      <td>0.176252</td>\n",
              "      <td>-0.161872</td>\n",
              "      <td>0.067942</td>\n",
              "      <td>0.094570</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047624</td>\n",
              "      <td>-0.291609</td>\n",
              "      <td>-0.034189</td>\n",
              "      <td>0.421426</td>\n",
              "      <td>-0.104581</td>\n",
              "      <td>-0.077029</td>\n",
              "      <td>0.133641</td>\n",
              "      <td>0.313814</td>\n",
              "      <td>-0.171700</td>\n",
              "      <td>-0.083820</td>\n",
              "      <td>-0.114426</td>\n",
              "      <td>-0.016215</td>\n",
              "      <td>0.163013</td>\n",
              "      <td>-0.066032</td>\n",
              "      <td>0.214723</td>\n",
              "      <td>-0.355283</td>\n",
              "      <td>-0.087042</td>\n",
              "      <td>-0.026524</td>\n",
              "      <td>-0.368888</td>\n",
              "      <td>-0.143112</td>\n",
              "      <td>0.103253</td>\n",
              "      <td>0.095139</td>\n",
              "      <td>0.212501</td>\n",
              "      <td>0.010271</td>\n",
              "      <td>0.388163</td>\n",
              "      <td>-0.142768</td>\n",
              "      <td>-0.120668</td>\n",
              "      <td>-0.228226</td>\n",
              "      <td>-0.047740</td>\n",
              "      <td>-0.039385</td>\n",
              "      <td>-0.105743</td>\n",
              "      <td>0.134047</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>-0.287196</td>\n",
              "      <td>0.117638</td>\n",
              "      <td>-0.180990</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>0.011383</td>\n",
              "      <td>-0.013590</td>\n",
              "      <td>0.123433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4448</th>\n",
              "      <td>-0.202350</td>\n",
              "      <td>0.210460</td>\n",
              "      <td>0.008651</td>\n",
              "      <td>0.094393</td>\n",
              "      <td>0.167715</td>\n",
              "      <td>0.033537</td>\n",
              "      <td>-0.074196</td>\n",
              "      <td>0.558049</td>\n",
              "      <td>-0.128097</td>\n",
              "      <td>0.013981</td>\n",
              "      <td>-0.051787</td>\n",
              "      <td>-0.195080</td>\n",
              "      <td>-0.042312</td>\n",
              "      <td>0.189100</td>\n",
              "      <td>-0.341049</td>\n",
              "      <td>0.467573</td>\n",
              "      <td>0.414612</td>\n",
              "      <td>0.037331</td>\n",
              "      <td>-0.096112</td>\n",
              "      <td>0.165886</td>\n",
              "      <td>0.040293</td>\n",
              "      <td>-0.024822</td>\n",
              "      <td>-0.048073</td>\n",
              "      <td>0.480281</td>\n",
              "      <td>0.390848</td>\n",
              "      <td>-0.046351</td>\n",
              "      <td>0.114299</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>-0.113570</td>\n",
              "      <td>-0.146070</td>\n",
              "      <td>0.502898</td>\n",
              "      <td>0.261103</td>\n",
              "      <td>0.096735</td>\n",
              "      <td>-0.035281</td>\n",
              "      <td>-0.313417</td>\n",
              "      <td>-0.172383</td>\n",
              "      <td>0.053957</td>\n",
              "      <td>-0.145919</td>\n",
              "      <td>-0.134239</td>\n",
              "      <td>0.214037</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024570</td>\n",
              "      <td>-0.162921</td>\n",
              "      <td>-0.006297</td>\n",
              "      <td>0.160630</td>\n",
              "      <td>-0.240705</td>\n",
              "      <td>-0.119764</td>\n",
              "      <td>0.144046</td>\n",
              "      <td>0.312836</td>\n",
              "      <td>-0.075514</td>\n",
              "      <td>-0.080052</td>\n",
              "      <td>-0.095569</td>\n",
              "      <td>-0.034356</td>\n",
              "      <td>0.139042</td>\n",
              "      <td>0.055819</td>\n",
              "      <td>0.211669</td>\n",
              "      <td>-0.305341</td>\n",
              "      <td>-0.076088</td>\n",
              "      <td>-0.208474</td>\n",
              "      <td>-0.406612</td>\n",
              "      <td>0.034593</td>\n",
              "      <td>0.039124</td>\n",
              "      <td>0.013608</td>\n",
              "      <td>0.084151</td>\n",
              "      <td>-0.043041</td>\n",
              "      <td>0.343054</td>\n",
              "      <td>-0.082053</td>\n",
              "      <td>-0.094414</td>\n",
              "      <td>-0.275407</td>\n",
              "      <td>-0.088390</td>\n",
              "      <td>0.066978</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.036421</td>\n",
              "      <td>0.097612</td>\n",
              "      <td>-0.230413</td>\n",
              "      <td>-0.059588</td>\n",
              "      <td>-0.275788</td>\n",
              "      <td>-0.064902</td>\n",
              "      <td>0.060040</td>\n",
              "      <td>0.071253</td>\n",
              "      <td>-0.016311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5512</th>\n",
              "      <td>-0.328258</td>\n",
              "      <td>0.180167</td>\n",
              "      <td>-0.091994</td>\n",
              "      <td>-0.323650</td>\n",
              "      <td>0.203776</td>\n",
              "      <td>0.137485</td>\n",
              "      <td>0.120990</td>\n",
              "      <td>0.571294</td>\n",
              "      <td>-0.144595</td>\n",
              "      <td>-0.086984</td>\n",
              "      <td>-0.070475</td>\n",
              "      <td>-0.257330</td>\n",
              "      <td>0.100284</td>\n",
              "      <td>-0.035773</td>\n",
              "      <td>-0.311281</td>\n",
              "      <td>0.650705</td>\n",
              "      <td>0.171884</td>\n",
              "      <td>-0.041560</td>\n",
              "      <td>-0.102503</td>\n",
              "      <td>0.035522</td>\n",
              "      <td>0.058407</td>\n",
              "      <td>0.043087</td>\n",
              "      <td>-0.086341</td>\n",
              "      <td>0.416647</td>\n",
              "      <td>0.403935</td>\n",
              "      <td>-0.112734</td>\n",
              "      <td>0.263002</td>\n",
              "      <td>-0.125636</td>\n",
              "      <td>-0.139427</td>\n",
              "      <td>-0.252405</td>\n",
              "      <td>0.415004</td>\n",
              "      <td>0.439248</td>\n",
              "      <td>0.025836</td>\n",
              "      <td>-0.128850</td>\n",
              "      <td>-0.068407</td>\n",
              "      <td>-0.010265</td>\n",
              "      <td>0.193717</td>\n",
              "      <td>-0.038397</td>\n",
              "      <td>0.061398</td>\n",
              "      <td>0.178040</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.032689</td>\n",
              "      <td>-0.197113</td>\n",
              "      <td>-0.002318</td>\n",
              "      <td>0.310494</td>\n",
              "      <td>-0.212990</td>\n",
              "      <td>-0.332839</td>\n",
              "      <td>0.157192</td>\n",
              "      <td>0.316049</td>\n",
              "      <td>-0.129806</td>\n",
              "      <td>-0.080578</td>\n",
              "      <td>-0.123229</td>\n",
              "      <td>-0.002427</td>\n",
              "      <td>0.558338</td>\n",
              "      <td>0.084797</td>\n",
              "      <td>0.275698</td>\n",
              "      <td>-0.277978</td>\n",
              "      <td>-0.169872</td>\n",
              "      <td>-0.040155</td>\n",
              "      <td>-0.494804</td>\n",
              "      <td>0.129812</td>\n",
              "      <td>0.003058</td>\n",
              "      <td>0.004096</td>\n",
              "      <td>0.063989</td>\n",
              "      <td>-0.032680</td>\n",
              "      <td>0.308163</td>\n",
              "      <td>-0.220515</td>\n",
              "      <td>-0.342375</td>\n",
              "      <td>-0.327071</td>\n",
              "      <td>0.145743</td>\n",
              "      <td>-0.085600</td>\n",
              "      <td>-0.016211</td>\n",
              "      <td>0.245718</td>\n",
              "      <td>-0.226035</td>\n",
              "      <td>-0.241044</td>\n",
              "      <td>-0.023335</td>\n",
              "      <td>-0.288023</td>\n",
              "      <td>-0.150109</td>\n",
              "      <td>-0.040640</td>\n",
              "      <td>0.105872</td>\n",
              "      <td>0.237510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>-0.214997</td>\n",
              "      <td>0.105096</td>\n",
              "      <td>0.110988</td>\n",
              "      <td>-0.107998</td>\n",
              "      <td>0.157610</td>\n",
              "      <td>0.075074</td>\n",
              "      <td>0.079468</td>\n",
              "      <td>0.551146</td>\n",
              "      <td>-0.241772</td>\n",
              "      <td>-0.216851</td>\n",
              "      <td>-0.044589</td>\n",
              "      <td>-0.110567</td>\n",
              "      <td>-0.088803</td>\n",
              "      <td>0.228565</td>\n",
              "      <td>-0.084628</td>\n",
              "      <td>0.318394</td>\n",
              "      <td>0.312286</td>\n",
              "      <td>0.042812</td>\n",
              "      <td>-0.112899</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.279102</td>\n",
              "      <td>0.114410</td>\n",
              "      <td>0.017866</td>\n",
              "      <td>0.333262</td>\n",
              "      <td>0.423692</td>\n",
              "      <td>-0.122588</td>\n",
              "      <td>0.294783</td>\n",
              "      <td>-0.025602</td>\n",
              "      <td>-0.118026</td>\n",
              "      <td>-0.124031</td>\n",
              "      <td>0.401833</td>\n",
              "      <td>0.154028</td>\n",
              "      <td>-0.154554</td>\n",
              "      <td>0.012429</td>\n",
              "      <td>-0.296389</td>\n",
              "      <td>0.011273</td>\n",
              "      <td>0.193029</td>\n",
              "      <td>-0.241300</td>\n",
              "      <td>-0.203860</td>\n",
              "      <td>0.023090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063027</td>\n",
              "      <td>-0.104707</td>\n",
              "      <td>-0.012073</td>\n",
              "      <td>0.189740</td>\n",
              "      <td>-0.103919</td>\n",
              "      <td>-0.042700</td>\n",
              "      <td>0.241941</td>\n",
              "      <td>0.161067</td>\n",
              "      <td>-0.031749</td>\n",
              "      <td>0.050348</td>\n",
              "      <td>-0.045989</td>\n",
              "      <td>0.098786</td>\n",
              "      <td>0.133632</td>\n",
              "      <td>0.151346</td>\n",
              "      <td>0.247223</td>\n",
              "      <td>-0.153786</td>\n",
              "      <td>0.175146</td>\n",
              "      <td>-0.096405</td>\n",
              "      <td>-0.368278</td>\n",
              "      <td>0.073516</td>\n",
              "      <td>0.055629</td>\n",
              "      <td>0.283305</td>\n",
              "      <td>0.007426</td>\n",
              "      <td>-0.063350</td>\n",
              "      <td>0.218870</td>\n",
              "      <td>-0.116894</td>\n",
              "      <td>-0.059873</td>\n",
              "      <td>-0.297153</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>-0.070245</td>\n",
              "      <td>0.117386</td>\n",
              "      <td>-0.001182</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>-0.337374</td>\n",
              "      <td>-0.052093</td>\n",
              "      <td>-0.382730</td>\n",
              "      <td>0.046814</td>\n",
              "      <td>-0.037034</td>\n",
              "      <td>0.024203</td>\n",
              "      <td>0.100774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>-0.124094</td>\n",
              "      <td>0.266427</td>\n",
              "      <td>0.089506</td>\n",
              "      <td>-0.162416</td>\n",
              "      <td>0.191304</td>\n",
              "      <td>0.175139</td>\n",
              "      <td>0.105429</td>\n",
              "      <td>0.468201</td>\n",
              "      <td>-0.226402</td>\n",
              "      <td>-0.352504</td>\n",
              "      <td>-0.070126</td>\n",
              "      <td>-0.095713</td>\n",
              "      <td>-0.046720</td>\n",
              "      <td>0.138359</td>\n",
              "      <td>-0.241666</td>\n",
              "      <td>0.490370</td>\n",
              "      <td>0.248636</td>\n",
              "      <td>0.043771</td>\n",
              "      <td>-0.241564</td>\n",
              "      <td>-0.018077</td>\n",
              "      <td>0.250159</td>\n",
              "      <td>0.009544</td>\n",
              "      <td>-0.036105</td>\n",
              "      <td>0.337513</td>\n",
              "      <td>0.416581</td>\n",
              "      <td>0.036748</td>\n",
              "      <td>0.158945</td>\n",
              "      <td>0.016377</td>\n",
              "      <td>-0.150259</td>\n",
              "      <td>-0.222817</td>\n",
              "      <td>0.410973</td>\n",
              "      <td>0.337476</td>\n",
              "      <td>-0.083998</td>\n",
              "      <td>-0.083999</td>\n",
              "      <td>-0.372399</td>\n",
              "      <td>-0.027496</td>\n",
              "      <td>0.140945</td>\n",
              "      <td>-0.232921</td>\n",
              "      <td>-0.055440</td>\n",
              "      <td>0.163234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.203995</td>\n",
              "      <td>-0.133664</td>\n",
              "      <td>-0.032132</td>\n",
              "      <td>0.251794</td>\n",
              "      <td>-0.011210</td>\n",
              "      <td>0.021508</td>\n",
              "      <td>0.218606</td>\n",
              "      <td>0.207865</td>\n",
              "      <td>-0.142285</td>\n",
              "      <td>0.072569</td>\n",
              "      <td>0.019126</td>\n",
              "      <td>0.106304</td>\n",
              "      <td>0.166922</td>\n",
              "      <td>0.207961</td>\n",
              "      <td>0.334044</td>\n",
              "      <td>-0.086406</td>\n",
              "      <td>0.140209</td>\n",
              "      <td>-0.171206</td>\n",
              "      <td>-0.412556</td>\n",
              "      <td>-0.028679</td>\n",
              "      <td>0.303469</td>\n",
              "      <td>0.142256</td>\n",
              "      <td>0.018725</td>\n",
              "      <td>-0.011468</td>\n",
              "      <td>0.197156</td>\n",
              "      <td>-0.129221</td>\n",
              "      <td>-0.171410</td>\n",
              "      <td>-0.245258</td>\n",
              "      <td>-0.079744</td>\n",
              "      <td>-0.087423</td>\n",
              "      <td>0.091674</td>\n",
              "      <td>0.072020</td>\n",
              "      <td>0.060984</td>\n",
              "      <td>-0.301178</td>\n",
              "      <td>-0.091165</td>\n",
              "      <td>-0.245942</td>\n",
              "      <td>0.003124</td>\n",
              "      <td>0.144312</td>\n",
              "      <td>0.076276</td>\n",
              "      <td>0.238285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0.094299</td>\n",
              "      <td>0.213722</td>\n",
              "      <td>0.231118</td>\n",
              "      <td>-0.048091</td>\n",
              "      <td>0.222329</td>\n",
              "      <td>-0.020921</td>\n",
              "      <td>-0.080170</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>-0.111779</td>\n",
              "      <td>-0.053050</td>\n",
              "      <td>-0.037679</td>\n",
              "      <td>-0.147375</td>\n",
              "      <td>-0.147850</td>\n",
              "      <td>0.436482</td>\n",
              "      <td>0.043574</td>\n",
              "      <td>0.480371</td>\n",
              "      <td>0.186687</td>\n",
              "      <td>-0.100682</td>\n",
              "      <td>-0.235162</td>\n",
              "      <td>0.306830</td>\n",
              "      <td>0.273201</td>\n",
              "      <td>0.158255</td>\n",
              "      <td>-0.079768</td>\n",
              "      <td>0.378315</td>\n",
              "      <td>0.255954</td>\n",
              "      <td>-0.029738</td>\n",
              "      <td>0.112169</td>\n",
              "      <td>-0.209095</td>\n",
              "      <td>-0.091520</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>0.378167</td>\n",
              "      <td>0.009786</td>\n",
              "      <td>-0.162033</td>\n",
              "      <td>-0.108168</td>\n",
              "      <td>-0.183461</td>\n",
              "      <td>-0.056198</td>\n",
              "      <td>0.155757</td>\n",
              "      <td>-0.058208</td>\n",
              "      <td>0.049086</td>\n",
              "      <td>-0.089039</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146013</td>\n",
              "      <td>-0.368767</td>\n",
              "      <td>-0.074549</td>\n",
              "      <td>0.189503</td>\n",
              "      <td>-0.076028</td>\n",
              "      <td>-0.009351</td>\n",
              "      <td>0.200265</td>\n",
              "      <td>0.357008</td>\n",
              "      <td>-0.156376</td>\n",
              "      <td>-0.145963</td>\n",
              "      <td>-0.165583</td>\n",
              "      <td>0.101163</td>\n",
              "      <td>0.019063</td>\n",
              "      <td>0.156450</td>\n",
              "      <td>0.058693</td>\n",
              "      <td>-0.344494</td>\n",
              "      <td>0.167419</td>\n",
              "      <td>-0.097786</td>\n",
              "      <td>-0.136815</td>\n",
              "      <td>-0.117936</td>\n",
              "      <td>0.233147</td>\n",
              "      <td>0.185568</td>\n",
              "      <td>0.093692</td>\n",
              "      <td>-0.200672</td>\n",
              "      <td>0.407291</td>\n",
              "      <td>-0.055597</td>\n",
              "      <td>-0.124648</td>\n",
              "      <td>-0.232686</td>\n",
              "      <td>-0.153616</td>\n",
              "      <td>-0.237394</td>\n",
              "      <td>-0.001175</td>\n",
              "      <td>-0.119957</td>\n",
              "      <td>0.022837</td>\n",
              "      <td>-0.303107</td>\n",
              "      <td>0.007492</td>\n",
              "      <td>-0.111416</td>\n",
              "      <td>-0.016027</td>\n",
              "      <td>0.118592</td>\n",
              "      <td>0.038088</td>\n",
              "      <td>0.126923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7315</th>\n",
              "      <td>-0.124400</td>\n",
              "      <td>0.292632</td>\n",
              "      <td>0.013982</td>\n",
              "      <td>-0.109755</td>\n",
              "      <td>0.145452</td>\n",
              "      <td>0.037359</td>\n",
              "      <td>0.131389</td>\n",
              "      <td>0.312803</td>\n",
              "      <td>-0.242758</td>\n",
              "      <td>-0.180056</td>\n",
              "      <td>-0.058725</td>\n",
              "      <td>-0.153107</td>\n",
              "      <td>-0.090281</td>\n",
              "      <td>0.153189</td>\n",
              "      <td>-0.238658</td>\n",
              "      <td>0.535441</td>\n",
              "      <td>0.249895</td>\n",
              "      <td>-0.112670</td>\n",
              "      <td>-0.260294</td>\n",
              "      <td>0.124967</td>\n",
              "      <td>0.190331</td>\n",
              "      <td>0.031764</td>\n",
              "      <td>0.069574</td>\n",
              "      <td>0.350832</td>\n",
              "      <td>0.360666</td>\n",
              "      <td>0.117889</td>\n",
              "      <td>0.256778</td>\n",
              "      <td>0.046026</td>\n",
              "      <td>-0.248134</td>\n",
              "      <td>-0.093435</td>\n",
              "      <td>0.332288</td>\n",
              "      <td>0.372646</td>\n",
              "      <td>-0.001021</td>\n",
              "      <td>-0.256028</td>\n",
              "      <td>-0.284347</td>\n",
              "      <td>-0.034993</td>\n",
              "      <td>-0.054338</td>\n",
              "      <td>-0.116619</td>\n",
              "      <td>-0.016424</td>\n",
              "      <td>0.154671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.274255</td>\n",
              "      <td>-0.284403</td>\n",
              "      <td>-0.059243</td>\n",
              "      <td>0.157820</td>\n",
              "      <td>-0.166103</td>\n",
              "      <td>-0.214672</td>\n",
              "      <td>0.141407</td>\n",
              "      <td>0.192863</td>\n",
              "      <td>-0.038790</td>\n",
              "      <td>0.058167</td>\n",
              "      <td>0.006519</td>\n",
              "      <td>0.017632</td>\n",
              "      <td>0.262642</td>\n",
              "      <td>-0.060179</td>\n",
              "      <td>0.318064</td>\n",
              "      <td>-0.326633</td>\n",
              "      <td>0.057799</td>\n",
              "      <td>-0.208900</td>\n",
              "      <td>-0.446819</td>\n",
              "      <td>-0.003815</td>\n",
              "      <td>0.396436</td>\n",
              "      <td>0.030492</td>\n",
              "      <td>0.061110</td>\n",
              "      <td>0.048072</td>\n",
              "      <td>0.331306</td>\n",
              "      <td>-0.140603</td>\n",
              "      <td>-0.174909</td>\n",
              "      <td>-0.201898</td>\n",
              "      <td>0.152716</td>\n",
              "      <td>0.055190</td>\n",
              "      <td>0.157329</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>-0.379392</td>\n",
              "      <td>-0.037193</td>\n",
              "      <td>-0.129479</td>\n",
              "      <td>-0.034084</td>\n",
              "      <td>0.019232</td>\n",
              "      <td>0.048329</td>\n",
              "      <td>0.059880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7200 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2  ...       765       766       767\n",
              "Unnamed: 0                                ...                              \n",
              "2614       -0.131176  0.266133  0.148921  ... -0.012199  0.064456 -0.037346\n",
              "5383       -0.316470  0.154551 -0.044945  ... -0.090707  0.121389  0.118943\n",
              "2448        0.092707  0.272643  0.176209  ... -0.006477  0.145422  0.028647\n",
              "8412       -0.144989  0.249846 -0.032966  ...  0.011383 -0.013590  0.123433\n",
              "4448       -0.202350  0.210460  0.008651  ...  0.060040  0.071253 -0.016311\n",
              "...              ...       ...       ...  ...       ...       ...       ...\n",
              "5512       -0.328258  0.180167 -0.091994  ... -0.040640  0.105872  0.237510\n",
              "8694       -0.214997  0.105096  0.110988  ... -0.037034  0.024203  0.100774\n",
              "8368       -0.124094  0.266427  0.089506  ...  0.144312  0.076276  0.238285\n",
              "886         0.094299  0.213722  0.231118  ...  0.118592  0.038088  0.126923\n",
              "7315       -0.124400  0.292632  0.013982  ...  0.019232  0.048329  0.059880\n",
              "\n",
              "[7200 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "87fb78ca-c846-47d3-ee3b-5b5a11147d1a"
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c9e336e0-cb7f-4d3c-e038-3a66e9885a96"
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-layer-normalization\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (1.17.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.0.8)\n",
            "Building wheels for collected packages: keras-layer-normalization\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=5909d910deeb66b355b14e57da91dc8e4073a3098e71c7db9244dff45e678ab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "Successfully built keras-layer-normalization\n",
            "Installing collected packages: keras-layer-normalization\n",
            "Successfully installed keras-layer-normalization-0.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "250b4b46-1c48-48eb-d278-6003e222df4b"
      },
      "source": [
        "units = 512\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "c = concatenate([x, c])\n",
        "\n",
        "x = Dense(units)(c)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "c = concatenate([x, c])\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=32)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=32,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=32,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "7200/7200 [==============================] - 4s 577us/step - loss: 0.3314 - acc: 0.9568 - val_loss: 0.2225 - val_acc: 0.9878\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 2s 285us/step - loss: 0.2467 - acc: 0.9811 - val_loss: 0.1956 - val_acc: 0.9900\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 2s 279us/step - loss: 0.2097 - acc: 0.9837 - val_loss: 0.1835 - val_acc: 0.9889\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 2s 287us/step - loss: 0.1750 - acc: 0.9868 - val_loss: 0.2150 - val_acc: 0.9667\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.1494 - acc: 0.9904 - val_loss: 0.1527 - val_acc: 0.9889\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 4s 577us/step - loss: 0.1337 - acc: 0.9903 - val_loss: 0.1468 - val_acc: 0.9922\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 2s 283us/step - loss: 0.1261 - acc: 0.9911 - val_loss: 0.1129 - val_acc: 0.9911\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 2s 286us/step - loss: 0.1208 - acc: 0.9899 - val_loss: 0.1063 - val_acc: 0.9922\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 2s 280us/step - loss: 0.1085 - acc: 0.9950 - val_loss: 0.1029 - val_acc: 0.9922\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 2s 281us/step - loss: 0.1009 - acc: 0.9943 - val_loss: 0.1032 - val_acc: 0.9922\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 2s 287us/step - loss: 0.0965 - acc: 0.9939 - val_loss: 0.0987 - val_acc: 0.9889\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 2s 290us/step - loss: 0.0945 - acc: 0.9922 - val_loss: 0.0936 - val_acc: 0.9911\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.0890 - acc: 0.9944 - val_loss: 0.0825 - val_acc: 0.9956\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 2s 292us/step - loss: 0.0835 - acc: 0.9951 - val_loss: 0.0826 - val_acc: 0.9933\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 2s 298us/step - loss: 0.0758 - acc: 0.9967 - val_loss: 0.0724 - val_acc: 0.9944\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 2s 283us/step - loss: 0.0729 - acc: 0.9956 - val_loss: 0.0723 - val_acc: 0.9933\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 2s 284us/step - loss: 0.0690 - acc: 0.9967 - val_loss: 0.0719 - val_acc: 0.9944\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.0682 - acc: 0.9961 - val_loss: 0.0666 - val_acc: 0.9944\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 2s 308us/step - loss: 0.0627 - acc: 0.9969 - val_loss: 0.0618 - val_acc: 0.9944\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 2s 285us/step - loss: 0.0571 - acc: 0.9972 - val_loss: 0.0683 - val_acc: 0.9944\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 2s 296us/step - loss: 0.0564 - acc: 0.9971 - val_loss: 0.0602 - val_acc: 0.9956\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 2s 304us/step - loss: 0.0551 - acc: 0.9965 - val_loss: 0.0560 - val_acc: 0.9967\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 2s 291us/step - loss: 0.0566 - acc: 0.9953 - val_loss: 0.0770 - val_acc: 0.9856\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 2s 285us/step - loss: 0.0516 - acc: 0.9965 - val_loss: 0.0544 - val_acc: 0.9967\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 2s 276us/step - loss: 0.0457 - acc: 0.9985 - val_loss: 0.0486 - val_acc: 0.9956\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 2s 287us/step - loss: 0.0451 - acc: 0.9978 - val_loss: 0.0507 - val_acc: 0.9933\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 2s 296us/step - loss: 0.0452 - acc: 0.9968 - val_loss: 0.0480 - val_acc: 0.9933\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 2s 294us/step - loss: 0.0436 - acc: 0.9975 - val_loss: 0.0517 - val_acc: 0.9933\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 2s 292us/step - loss: 0.0395 - acc: 0.9978 - val_loss: 0.0502 - val_acc: 0.9933\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 2s 284us/step - loss: 0.0363 - acc: 0.9990 - val_loss: 0.0406 - val_acc: 0.9967\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 2s 292us/step - loss: 0.0379 - acc: 0.9967 - val_loss: 0.0472 - val_acc: 0.9911\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 2s 289us/step - loss: 0.0358 - acc: 0.9979 - val_loss: 0.0418 - val_acc: 0.9956\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.0327 - acc: 0.9983 - val_loss: 0.0372 - val_acc: 0.9956\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.0309 - acc: 0.9986 - val_loss: 0.0393 - val_acc: 0.9944\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 2s 300us/step - loss: 0.0319 - acc: 0.9982 - val_loss: 0.0361 - val_acc: 0.9967\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 2s 302us/step - loss: 0.0301 - acc: 0.9976 - val_loss: 0.0457 - val_acc: 0.9922\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 2s 286us/step - loss: 0.0270 - acc: 0.9989 - val_loss: 0.0324 - val_acc: 0.9967\n",
            "Epoch 33/99\n",
            "7200/7200 [==============================] - 2s 290us/step - loss: 0.0263 - acc: 0.9990 - val_loss: 0.0304 - val_acc: 0.9967\n",
            "Epoch 34/99\n",
            "7200/7200 [==============================] - 2s 274us/step - loss: 0.0268 - acc: 0.9975 - val_loss: 0.0325 - val_acc: 0.9944\n",
            "Epoch 35/99\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.0279 - acc: 0.9971 - val_loss: 0.0440 - val_acc: 0.9933\n",
            "Epoch 36/99\n",
            "7200/7200 [==============================] - 2s 293us/step - loss: 0.0252 - acc: 0.9982 - val_loss: 0.0285 - val_acc: 0.9967\n",
            "Epoch 37/99\n",
            "7200/7200 [==============================] - 2s 292us/step - loss: 0.0225 - acc: 0.9988 - val_loss: 0.0356 - val_acc: 0.9944\n",
            "Epoch 38/99\n",
            "7200/7200 [==============================] - 2s 299us/step - loss: 0.0226 - acc: 0.9983 - val_loss: 0.0326 - val_acc: 0.9933\n",
            "Epoch 39/99\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.0214 - acc: 0.9992 - val_loss: 0.0258 - val_acc: 0.9967\n",
            "Epoch 40/99\n",
            "7200/7200 [==============================] - 2s 302us/step - loss: 0.0200 - acc: 0.9990 - val_loss: 0.0259 - val_acc: 0.9967\n",
            "Epoch 41/99\n",
            "7200/7200 [==============================] - 2s 294us/step - loss: 0.0185 - acc: 0.9992 - val_loss: 0.0295 - val_acc: 0.9956\n",
            "Epoch 42/99\n",
            "7200/7200 [==============================] - 2s 297us/step - loss: 0.0184 - acc: 0.9990 - val_loss: 0.0272 - val_acc: 0.9956\n",
            "Epoch 43/99\n",
            "7200/7200 [==============================] - 2s 283us/step - loss: 0.0175 - acc: 0.9990 - val_loss: 0.0241 - val_acc: 0.9956\n",
            "Epoch 44/99\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.0169 - acc: 0.9993 - val_loss: 0.0295 - val_acc: 0.9933\n",
            "Epoch 45/99\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.0184 - acc: 0.9986 - val_loss: 0.0277 - val_acc: 0.9956\n",
            "Epoch 46/99\n",
            "7200/7200 [==============================] - 2s 285us/step - loss: 0.0155 - acc: 0.9993 - val_loss: 0.0318 - val_acc: 0.9911\n",
            "Epoch 47/99\n",
            "7200/7200 [==============================] - 2s 296us/step - loss: 0.0166 - acc: 0.9992 - val_loss: 0.0252 - val_acc: 0.9967\n",
            "Epoch 48/99\n",
            "7200/7200 [==============================] - 2s 289us/step - loss: 0.0134 - acc: 0.9994 - val_loss: 0.0231 - val_acc: 0.9956\n",
            "Epoch 49/99\n",
            "7200/7200 [==============================] - 2s 294us/step - loss: 0.0133 - acc: 0.9993 - val_loss: 0.0264 - val_acc: 0.9944\n",
            "Epoch 50/99\n",
            "7200/7200 [==============================] - 2s 279us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 0.0213 - val_acc: 0.9956\n",
            "Epoch 51/99\n",
            "7200/7200 [==============================] - 2s 285us/step - loss: 0.0140 - acc: 0.9990 - val_loss: 0.0227 - val_acc: 0.9956\n",
            "Epoch 52/99\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.0133 - acc: 0.9994 - val_loss: 0.0228 - val_acc: 0.9944\n",
            "Epoch 53/99\n",
            "7200/7200 [==============================] - 2s 290us/step - loss: 0.0125 - acc: 0.9992 - val_loss: 0.0219 - val_acc: 0.9944\n",
            "Epoch 54/99\n",
            "7200/7200 [==============================] - 2s 291us/step - loss: 0.0141 - acc: 0.9983 - val_loss: 0.0195 - val_acc: 0.9978\n",
            "Epoch 55/99\n",
            "7200/7200 [==============================] - 2s 282us/step - loss: 0.0117 - acc: 0.9996 - val_loss: 0.0280 - val_acc: 0.9922\n",
            "Epoch 56/99\n",
            "7200/7200 [==============================] - 2s 274us/step - loss: 0.0097 - acc: 0.9996 - val_loss: 0.0189 - val_acc: 0.9978\n",
            "Epoch 57/99\n",
            "7200/7200 [==============================] - 2s 284us/step - loss: 0.0128 - acc: 0.9982 - val_loss: 0.0217 - val_acc: 0.9956\n",
            "Epoch 58/99\n",
            "7200/7200 [==============================] - 2s 287us/step - loss: 0.0108 - acc: 0.9989 - val_loss: 0.0308 - val_acc: 0.9889\n",
            "Epoch 59/99\n",
            "7200/7200 [==============================] - 2s 284us/step - loss: 0.0094 - acc: 0.9996 - val_loss: 0.0223 - val_acc: 0.9944\n",
            "Epoch 60/99\n",
            "7200/7200 [==============================] - 2s 286us/step - loss: 0.0093 - acc: 0.9996 - val_loss: 0.0168 - val_acc: 0.9956\n",
            "Epoch 61/99\n",
            "7200/7200 [==============================] - 2s 284us/step - loss: 0.0094 - acc: 0.9994 - val_loss: 0.0215 - val_acc: 0.9933\n",
            "Epoch 62/99\n",
            "7200/7200 [==============================] - 2s 294us/step - loss: 0.0091 - acc: 0.9993 - val_loss: 0.0200 - val_acc: 0.9944\n",
            "Epoch 63/99\n",
            "7200/7200 [==============================] - 2s 297us/step - loss: 0.0092 - acc: 0.9993 - val_loss: 0.0194 - val_acc: 0.9967\n",
            "Epoch 64/99\n",
            "7200/7200 [==============================] - 2s 289us/step - loss: 0.0075 - acc: 0.9999 - val_loss: 0.0155 - val_acc: 0.9978\n",
            "Epoch 65/99\n",
            "7200/7200 [==============================] - 2s 295us/step - loss: 0.0089 - acc: 0.9988 - val_loss: 0.0158 - val_acc: 0.9978\n",
            "Epoch 66/99\n",
            "7200/7200 [==============================] - 2s 297us/step - loss: 0.0093 - acc: 0.9992 - val_loss: 0.0156 - val_acc: 0.9967\n",
            "Epoch 67/99\n",
            "7200/7200 [==============================] - 2s 296us/step - loss: 0.0079 - acc: 0.9993 - val_loss: 0.0199 - val_acc: 0.9944\n",
            "Epoch 68/99\n",
            "7200/7200 [==============================] - 2s 299us/step - loss: 0.0073 - acc: 0.9996 - val_loss: 0.0171 - val_acc: 0.9967\n",
            "Epoch 69/99\n",
            "7200/7200 [==============================] - 2s 303us/step - loss: 0.0069 - acc: 0.9996 - val_loss: 0.0163 - val_acc: 0.9967\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 3s 361us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 0.9933\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 0.9933\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 0.9944\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 0.9944\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0242 - val_acc: 0.9944\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 0.9944\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0224 - val_acc: 0.9944\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0221 - val_acc: 0.9944\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 0.9944\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0212 - val_acc: 0.9944\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 0s 20us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0204 - val_acc: 0.9944\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 0s 20us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0195 - val_acc: 0.9944\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0190 - val_acc: 0.9944\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0184 - val_acc: 0.9944\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0179 - val_acc: 0.9944\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 0s 24us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0173 - val_acc: 0.9967\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9967\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 0s 20us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9956\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 0.9956\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 0.9956\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9967\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 0s 24us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9967\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0158 - val_acc: 0.9967\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 0s 23us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9967\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0156 - val_acc: 0.9967\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0156 - val_acc: 0.9967\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9967\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9967\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0152 - val_acc: 0.9967\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0152 - val_acc: 0.9967\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 0s 20us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9967\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9967\n",
            "Epoch 33/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9967\n",
            "Epoch 34/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9967\n",
            "Epoch 35/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9967\n",
            "Epoch 36/99\n",
            "7200/7200 [==============================] - 0s 20us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 0.9967\n",
            "Epoch 37/99\n",
            "7200/7200 [==============================] - 0s 22us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9967\n",
            "Epoch 38/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9967\n",
            "Epoch 39/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9967\n",
            "Epoch 40/99\n",
            "7200/7200 [==============================] - 0s 21us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9967\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 82us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01402373373715414, 0.9955555555555555]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}