{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOnanQvM-5iz",
        "colab_type": "text"
      },
      "source": [
        "# <center> Style Transfer Evaluation </center>\n",
        "\n",
        "Since our intention is to provide machine-based style transfer, we need to task ourselves with subjectof output evaluation.  It follows from our discussion on styling that such score should include themeasures of a triplet {*style source, narrative fluency, and content equivalence*}. Provided that our ultimate goal is a perfect imitation of source style conditioned on story from content source, missing either of the aforementioned factors will not yield a satisfactory result.For one example, if the output text does not employ the vocabulary and sentence structure of styledonor, it will result in the stylistic miss.  For another example, if the output employs the style butdeparts from the content, it will fail to form a parallel representation.  For a third example, if theoutput text successfully fuses the content with style of input sources but violates general languageand writing norms, it will result in a poor reading experience. Therefore, to evaluate the quality ofstyle transfer, we need to take all those considerations into account.\n",
        "\n",
        "In evaluating results of the literature style transfer, we must consider that the two dimensions of the metric (naturalness and content preservation) are of the satisficing type, while the style is the metric component we optimize for. We leverage the work done by Mir et. al [1] where the authors propose - \n",
        "\n",
        "* Style Transfer Intensity \n",
        "* Naturalness\n",
        "* Content preservation \n",
        "\n",
        "as key aspects of interest for style transfer for text. The authors propose a set of metrics for automated evaluation and demonstrate that they are are more strongly correlated and in agreement with human judgement than prior work in the area for the respective aspects. We leverage these automated metrics calculated as Earth Mover's Distance, Word Mover's Distance on style-masked texts, and adversarial classification for the respective aspects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HO9ban-pt5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8be54d1-fd86-401a-fdeb-b8acb91dba48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EStghsVkfG_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8v7CR99spqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e08a9143-aa85-4862-d9d4-d5fd3d68043c"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from tokenizer import tokenize\n",
        "from tokenizer import RE_PATTERN\n",
        "from pyemd import emd\n",
        "from collections import Counter\n",
        "from keras.models import load_model as load_keras_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tokenizer import RE_PATTERN\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8S-VaDpCFZR",
        "colab_type": "text"
      },
      "source": [
        "## Style Transfer Intensity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OlHZyResh9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" EVALUATION OF STYLE TRANSFER INTENSITY\n",
        "\n",
        "This code is used for evaluation of style transfer intensity (STI) between input and output texts of our style transfer model.\n",
        "Note that STI, makes use of both the input and the output texts for evaluation and tells how *much* the style changed from the input to the output.\n",
        "\n",
        "Two output texts could exhibit the same overall target style, but with one being more pronounced than the other,\n",
        "e.g. \"i like this\" vs. \"i love this !\" While past evaluations do not quantify that, STI can,\n",
        "given style distributions from a style classifier trained on labeled style datasets.\n",
        "\n",
        "As per [1] STI, based on Earth Mover's Distance (EMD) between style distributions of input and output texts,\n",
        "exhibited higher correlation with human evaluations of the same texts than did the method of past evaluations\n",
        "(using target style scores of output texts alone).\n",
        "\n",
        "Usage:\n",
        "    - Calculate STI for  input/output text style distributions              -> calculate_emd(...)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def calculate_emd(input_distribution, output_distribution):\n",
        "    '''\n",
        "    Calculate Earth Mover's Distance (aka Wasserstein distance) between\n",
        "    two distributions of equal length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an input text\n",
        "    output_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an output text, e.g. of a style transfer model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Earth Mover's Distance (float) between the two given style distributions\n",
        "\n",
        "    '''\n",
        "\n",
        "    N = len(input_distribution)\n",
        "    distance_matrix = np.ones((N, N))\n",
        "    return emd(input_distribution, output_distribution, distance_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXDoL1--CK37",
        "colab_type": "text"
      },
      "source": [
        "###### Ingest Sample Test Data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF1blTzw34WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "donor_y = pd.read_csv('/content/drive/My Drive/donor_y.csv')\n",
        "donor_yhat = pd.read_csv('/content/drive/My Drive/donor_yhat.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq0X1ZQuly5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7c747022-66cf-41f4-cf41-5e66fd466c98"
      },
      "source": [
        "print(donor_y.head())\n",
        "print(donor_yhat.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Austen  Dumas  Nabokov\n",
            "0     0.0    0.0      1.0\n",
            "1     0.0    0.0      1.0\n",
            "2     0.0    0.0      1.0\n",
            "3     0.0    0.0      1.0\n",
            "4     0.0    0.0      1.0\n",
            "     Austen     Dumas   Nabokov\n",
            "0  0.000679  0.001936  0.997385\n",
            "1  0.000076  0.628543  0.371381\n",
            "2  0.010569  0.853630  0.135801\n",
            "3  0.006065  0.646431  0.347504\n",
            "4  0.000003  0.001943  0.998054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3HGEwxU35uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emd_calc = []\n",
        "for i in range(donor_yhat.shape[0]):\n",
        "    emd_calc.append(calculate_emd(np.array(donor_y, order='C')[i].astype('float64'), np.array(donor_yhat, order='C')[i].astype('float64')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10WrMtfFB9L2",
        "colab_type": "text"
      },
      "source": [
        "###### Distribution of EMD Scores Calculated on Sample Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRAAJO4e5n8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a8b46d98-4f71-4cfa-f7a1-f8123e4c8a2f"
      },
      "source": [
        "pd.DataFrame(emd_calc, columns=['EMD']).plot.hist();"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVjUlEQVR4nO3df5RfdX3n8efLBA20KEgiSxPsBBcU\n6i+yU5YesUXTbRG7hGVdDKdWYFmzFRbaKtVge4pne9iD2xYru60alQKuGtBSyS52W6AiZz0CDYLK\nL2uMASZGM6JgqQEE3/vH9+Z2jDPJNzPz/X4z830+zpkz937uvd/7vpmB13zu5/5IVSFJEsCzBl2A\nJGnfYShIklqGgiSpZShIklqGgiSptXDQBczE4sWLa2RkZNBlSNKccuedd36nqpZMtmxOh8LIyAgb\nN24cdBmSNKckeXCqZZ4+kiS1DAVJUstQkCS15vSYgiRNxw9/+EPGxsZ44oknBl1KTy1atIhly5ax\n3377db2NoSBp6IyNjXHggQcyMjJCkkGX0xNVxSOPPMLY2BjLly/vejtPH0kaOk888QSHHHLIvA0E\ngCQccsghe90bMhQkDaX5HAg7TecYDQVJUssxBUlDb2TtDbP6eVsuff0e11mwYAEve9nL2vnVq1ez\ndu1aTjzxRDZv3syDDz7Y/qV/6qmnctNNN/H444+zZcsWjj76aF7ykpfwxBNPcOCBB3Luuedy1lln\nzUrtQxsKs/1LsDe6+YWRNL/tv//+3H333ZMuO+igg/j85z/PCSecwKOPPsq2bdt+bPmLXvQi7rrr\nLgA2b97MaaedRlVx9tlnz7guTx9J0j5m9erVrF+/HoDrrruO0047bcp1jzjiCC677DIuv/zyWdm3\noSBJA7Bjxw5e+cpXtl/XXHNNu2zlypXceuutPPPMM6xfv543vvGNu/2sFStW8MADD8xKXUN7+kiS\nBml3p48WLFjACSecwPr169mxYwd7ehp0Vc1aXfYUJGkftHr1ai644AJOP/30Pa571113cfTRR8/K\nfnsWCkmuSLI9yT27tJ+f5IEk9yb57xPaL0qyKclXk/xqr+qSpLng1a9+NRdddBFnnHHGbtfbsmUL\nF154Ieeff/6s7LeXp4+uBP4ncPXOhiSvAVYBr6iqJ5O8oGk/BlgN/BzwM8BNSY6qqmd6WJ8kAYO5\nInDnmMJOJ510Epdeemk7n4QLL7xw0m2//vWvc+yxx7aXpF5wwQX7/iWpVXVrkpFdmt8KXFpVTzbr\nbG/aVwHrm/ZvJNkEHAd8oVf1SdIgPfPM5H/z3nLLLZO2P/7440Dn5WI7duzoVVl9H1M4Cnh1ktuT\nfC7JzzftS4GHJ6w31rT9hCRrkmxMsnF8fLzH5UrScOl3KCwEng8cD/wucG328uEcVbWuqkaranTJ\nkklfMSpJmqZ+h8IYcF113AH8CFgMbAUOn7DesqZNknpiNi/j3FdN5xj7HQqfBl4DkOQo4NnAd4AN\nwOokz0myHDgSuKPPtUkaEosWLeKRRx6Z18Gw830KixYt2qvtejbQnOQTwInA4iRjwMXAFcAVzWWq\nTwFnVuencm+Sa4H7gKeB87zySFKvLFu2jLGxMeb7uOTON6/tjV5efTTVxbVvmmL9S4BLelWPJO20\n33777dXbyIaJdzRLklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShI\nklqGgiSpZShIklqGgiSpZShIklqGgiSp1bNQSHJFku3NW9Z2Xfb2JJVkcTOfJJcn2ZTky0lW9Kou\nSdLUetlTuBI4adfGJIcDvwI8NKH5dXTey3wksAZ4fw/rkiRNoWehUFW3At+dZNF7gXcAE9+YvQq4\nujpuAw5KclivapMkTa6vYwpJVgFbq+pLuyxaCjw8YX6saZvsM9Yk2Zhk43x/6bYk9VvfQiHJAcC7\ngD+YyedU1bqqGq2q0SVLlsxOcZIkABb2cV8vApYDX0oCsAz4YpLjgK3A4RPWXda0SZL6qG89har6\nSlW9oKpGqmqEzimiFVX1LWAD8ObmKqTjgceqalu/apMkdfTyktRPAF8AXpxkLMk5u1n9M8BmYBPw\nIeDcXtUlSZpaz04fVdUZe1g+MmG6gPN6VYskqTve0SxJahkKkqSWoSBJahkKkqSWoSBJahkKkqSW\noSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqRWL9+8dkWS7UnumdD2\nR0keSPLlJH+V5KAJyy5KsinJV5P8aq/qkiRNrZc9hSuBk3ZpuxF4aVW9HPgH4CKAJMcAq4Gfa7b5\n8yQLelibJGkSPQuFqroV+O4ubX9bVU83s7cBy5rpVcD6qnqyqr5B513Nx/WqNknS5AY5pvAfgb9u\nppcCD09YNta0/YQka5JsTLJxfHy8xyVK0nAZSCgk+T3gaeBje7ttVa2rqtGqGl2yZMnsFydJQ2xh\nv3eY5Czg14CVVVVN81bg8AmrLWvaJEl91NeeQpKTgHcAp1TVDyYs2gCsTvKcJMuBI4E7+lmbJKmH\nPYUknwBOBBYnGQMupnO10XOAG5MA3FZVv1lV9ya5FriPzmml86rqmV7VJkmaXM9CoarOmKT5I7tZ\n/xLgkl7VI0naM+9oliS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUqur\nUEjysl4XIkkavG57Cn+e5I4k5yZ5Xk8rkiQNTFehUFWvBn6dzjsP7kzy8ST/pqeVSZL6rusxhar6\nGvD7wDuBXwIuT/JAktN6VZwkqb+6HVN4eZL3AvcDrwX+bVUd3Uy/t4f1SZL6qNv3KfwP4MPAu6pq\nx87Gqvpmkt/vSWWSpL7r9vTR64GP7wyEJM9KcgBAVX10sg2SXJFke5J7JrQ9P8mNSb7WfD+4aU+S\ny5NsSvLlJCtmdliSpOnoNhRuAvafMH9A07Y7VwIn7dK2Fri5qo4Ebm7mAV5H573MRwJrgPd3WZck\naRZ1GwqLqurxnTPN9AG726CqbgW+u0vzKuCqZvoq4NQJ7VdXx23AQUkO67I2SdIs6TYU/mniKZ0k\n/wrYsZv1p3JoVW1rpr8FHNpMLwUenrDeWNP2E5KsSbIxycbx8fFplCBJmkq3A82/DXwyyTeBAP8C\neONMdlxVlaSmsd06YB3A6OjoXm8vSZpaV6FQVX+f5CXAi5umr1bVD6exv28nOayqtjWnh7Y37Vvp\n3Bi307KmTZLUR3vzQLyfB14OrADOSPLmaexvA3BmM30mcP2E9jc3VyEdDzw24TSTJKlPuuopJPko\n8CLgbuCZprmAq3ezzSeAE4HFScaAi4FLgWuTnAM8CJzerP4Z4GRgE/AD4Oy9PRBJ0sx1O6YwChxT\nVV2fw6+qM6ZYtHKSdQs4r9vPliT1Rrenj+6hM7gsSZrHuu0pLAbuS3IH8OTOxqo6pSdVSZIGottQ\neHcvi5Ak7Ru6vST1c0l+Fjiyqm5qnnu0oLelSZL6rdtHZ78F+BTwwaZpKfDpXhUlSRqMbgeazwNe\nBXwf2hfuvKBXRUmSBqPbUHiyqp7aOZNkIZ37FCRJ80i3ofC5JO8C9m/ezfxJ4H/3rixJ0iB0Gwpr\ngXHgK8B/pnMHsm9ck6R5pturj34EfKj5kiTNU90+++gbTDKGUFVHzHpFkqSB2ZtnH+20CPgPwPNn\nvxxJ0iB1NaZQVY9M+NpaVX8KvL7HtUmS+qzb00crJsw+i07PodtehiRpjuj2f+x/MmH6aWAL//wu\nBEnSPNHt1Uevmc2dJvkd4D/RGbz+Cp2X6hwGrAcOAe4EfmPiDXOSpN7r9vTR23a3vKou63aHSZYC\nF9B5ac+OJNcCq+m8ee29VbU+yQeAc4D3d/u5kqSZ6/bmtVHgrXQehLcU+E0672o+sPnaWwvp3B29\nEDgA2Aa8ls5D9wCuAk6dxudKkmag2zGFZcCKqvpHgCTvBm6oqjft7Q6ramuSPwYeAnYAf0vndNGj\nVfV0s9oYnfCRJPVRtz2FQ4GJ5/efatr2WpKDgVXAcuBngJ8CTtqL7dck2Zhk4/j4+HRKkCRNodue\nwtXAHUn+qpk/lc4pnun4ZeAbVTUOkOQ6Oo/lPijJwqa3sAzYOtnGVbUOWAcwOjrqk1olaRZ1e/Pa\nJXSuEPpe83V2Vf23ae7zIeD4JAckCbASuA/4LPCGZp0zgeun+fmSpGnq9vQRdAaEv19V7wPGkiyf\nzg6r6nY6A8pfpHM56rPo/OX/TuBtSTbRuSz1I9P5fEnS9HV7SerFdK5AejHwF8B+wP+ic9pnr1XV\nxcDFuzRvBo6bzudJkmZHtz2FfwecAvwTQFV9k+ldiipJ2od1GwpPVVXRPD47yU/1riRJ0qB0GwrX\nJvkgnSuE3gLchC/ckaR5p9tnH/1x827m79MZV/iDqrqxp5VJkvpuj6GQZAFwU/NQPINAkuaxPYZC\nVT2T5EdJnldVj/WjKEmaC0bW3jCwfW+5tDfvOev2jubHga8kuZHmCiSAqrqgJ1VJkgai21C4rvmS\nJM1juw2FJC+sqoeqarrPOZIkzSF7uiT10zsnkvxlj2uRJA3YnkIhE6aP6GUhkqTB21Mo1BTTkqR5\naE8Dza9I8n06PYb9m2ma+aqq5/a0OklSX+02FKpqQb8KkSQN3t68T0GSNM8ZCpKk1kBCIclBST6V\n5IEk9yf5hSTPT3Jjkq813w8eRG2SNMwG1VN4H/B/q+olwCuA+4G1wM1VdSRwczMvSeqjvodCkucB\nv0jzDuaqeqqqHgVWATvvnL4KOLXftUnSsBtET2E5MA78RZK7kny4eZPboVW1rVnnW8Chk22cZE2S\njUk2jo+P96lkSRoOgwiFhcAK4P1VdSydp67+2Kmiia/+3FVVrauq0aoaXbJkSc+LlaRhMohQGAPG\nqur2Zv5TdELi20kOA2i+bx9AbZI01PoeClX1LeDhJC9umlYC9wEbgDObtjOB6/tdmyQNu27fpzDb\nzgc+luTZwGbgbDoBdW2Sc4AHgdMHVJskDa2BhEJV3Q2MTrJoZb9rkST9M+9oliS1DAVJUstQkCS1\nDAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJ\nUmtgoZBkQZK7kvyfZn55ktuTbEpyTfNWNklSHw2yp/BbwP0T5t8DvLeq/iXwPeCcgVQlSUNsIKGQ\nZBnweuDDzXyA1wKfala5Cjh1ELVJ0jAbVE/hT4F3AD9q5g8BHq2qp5v5MWDpZBsmWZNkY5KN4+Pj\nva9UkoZI30Mhya8B26vqzulsX1Xrqmq0qkaXLFkyy9VJ0nBbOIB9vgo4JcnJwCLgucD7gIOSLGx6\nC8uArQOoTZKGWt97ClV1UVUtq6oRYDXwd1X168BngTc0q50JXN/v2iRp2O1L9ym8E3hbkk10xhg+\nMuB6JGnoDOL0UauqbgFuaaY3A8cNsh5JGnb7Uk9BkjRghoIkqWUoSJJahoIkqWUoSJJahoIkqWUo\nSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJaA32fwrAaWXvDQPa75dLXD2S/\nkuaOvvcUkhye5LNJ7ktyb5Lfatqfn+TGJF9rvh/c79okadgN4vTR08Dbq+oY4HjgvCTHAGuBm6vq\nSODmZl6S1Ed9D4Wq2lZVX2ym/xG4H1gKrAKuala7Cji137VJ0rAb6JhCkhHgWOB24NCq2tYs+hZw\n6BTbrAHWALzwhS/sfZGS9nmDGqebjwZ29VGSnwb+Evjtqvr+xGVVVUBNtl1Vrauq0aoaXbJkSR8q\nlaThMZBQSLIfnUD4WFVd1zR/O8lhzfLDgO2DqE2Shtkgrj4K8BHg/qq6bMKiDcCZzfSZwPX9rk2S\nht0gxhReBfwG8JUkdzdt7wIuBa5Ncg7wIHD6AGqT5jzvg9FM9D0Uqur/AZli8cp+1iJJ+nHe0Sxp\nVngF0Pzgs48kSS1DQZLU8vSR+sLBT2lusKcgSWrZUxgiwzgQOIzHLM2EPQVJUstQkCS1DAVJUstQ\nkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS19rlQSHJSkq8m2ZRk7aDrkaRhsk+FQpIFwJ8B\nrwOOAc5Icsxgq5Kk4bFPhQJwHLCpqjZX1VPAemDVgGuSpKGxrz0ldSnw8IT5MeBfT1whyRpgTTP7\neJKvTnNfi4HvTHPbucpjHg4e8xDIe2Z0zD871YJ9LRT2qKrWAetm+jlJNlbV6CyUNGd4zMPBYx4O\nvTrmfe300Vbg8Anzy5o2SVIf7Guh8PfAkUmWJ3k2sBrYMOCaJGlo7FOnj6rq6ST/BfgbYAFwRVXd\n26PdzfgU1BzkMQ8Hj3k49OSYU1W9+FxJ0hy0r50+kiQNkKEgSWrN+1DY02MzkjwnyTXN8tuTjPS/\nytnVxTG/Lcl9Sb6c5OYkU16zPFd0+3iUJP8+SSWZ85cvdnPMSU5vftb3Jvl4v2ucbV38br8wyWeT\n3NX8fp88iDpnS5IrkmxPcs8Uy5Pk8ubf48tJVsx4p1U1b7/oDFZ/HTgCeDbwJeCYXdY5F/hAM70a\nuGbQdffhmF8DHNBMv3UYjrlZ70DgVuA2YHTQdffh53wkcBdwcDP/gkHX3YdjXge8tZk+Btgy6Lpn\neMy/CKwA7pli+cnAXwMBjgdun+k+53tPoZvHZqwCrmqmPwWsTJI+1jjb9njMVfXZqvpBM3sbnftB\n5rJuH4/yh8B7gCf6WVyPdHPMbwH+rKq+B1BV2/tc42zr5pgLeG4z/Tzgm32sb9ZV1a3Ad3ezyirg\n6uq4DTgoyWEz2ed8D4XJHpuxdKp1qupp4DHgkL5U1xvdHPNE59D5S2Mu2+MxN93qw6vqhn4W1kPd\n/JyPAo5K8vkktyU5qW/V9UY3x/xu4E1JxoDPAOf3p7SB2dv/3vdon7pPQf2V5E3AKPBLg66ll5I8\nC7gMOGvApfTbQjqnkE6k0xu8NcnLqurRgVbVW2cAV1bVnyT5BeCjSV5aVT8adGFzxXzvKXTz2Ix2\nnSQL6XQ5H+lLdb3R1aNCkvwy8HvAKVX1ZJ9q65U9HfOBwEuBW5JsoXPudcMcH2zu5uc8Bmyoqh9W\n1TeAf6ATEnNVN8d8DnAtQFV9AVhE52F589WsPxpovodCN4/N2ACc2Uy/Afi7akZw5qg9HnOSY4EP\n0gmEuX6eGfZwzFX1WFUtrqqRqhqhM45ySlVtHEy5s6Kb3+1P0+klkGQxndNJm/tZ5Czr5pgfAlYC\nJDmaTiiM97XK/toAvLm5Cul44LGq2jaTD5zXp49qisdmJPmvwMaq2gB8hE4XcxOdAZ3Vg6t45ro8\n5j8Cfhr4ZDOm/lBVnTKwomeoy2OeV7o85r8BfiXJfcAzwO9W1ZztBXd5zG8HPpTkd+gMOp81l//I\nS/IJOsG+uBknuRjYD6CqPkBn3ORkYBPwA+DsGe9zDv97SZJm2Xw/fSRJ2guGgiSpZShIklqGgiSp\nZShIklqGgiSpZShIklr/H54lOFbGCMDmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afVGVM3zCWMV",
        "colab_type": "text"
      },
      "source": [
        "# Naturalness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJS7-3YC_ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "9ad311e7-a07a-4d1a-f81d-be083c2b1a83"
      },
      "source": [
        "\"\"\"EVALUATION OF NATURALNESS\n",
        "This is used to evaluate the naturalness of output texts of our style transfer model.\n",
        "For a baseline understanding of what is considered \"natural,\" any method used for automated evaluation of naturalness\n",
        "also requires an understanding of the human-sourced input texts.\n",
        "Inspired by the adversarial evaluation approach in \"Generating Sentences from a Continuous Space\"\n",
        "(Bowman et al., 2016), we use pretrained unigram logistic regression classifiers and LSTM logistic regression classifiers available from [1]\n",
        "on samples of input texts and output texts for each style transfer model.\n",
        "Via adversarial evaluation, the classifiers must distinguish human-generated inputs from machine-generated outputs.\n",
        "The more natural an output is, the likelier it is to fool an adversarial classifier.\n",
        "\n",
        "    - Calculate naturalness scores for texts with clf, a NaturalnessClassifier      -> clf.score(...)\n",
        "You can find examples of more detailed usage commands below.\n",
        "\"\"\"\n",
        "\n",
        "NATURALNESS_CLASSIFIER_BASE_PATH = '/content/drive/My Drive/NaturalnessClassifier/'\n",
        "MAX_SEQ_LEN = 30 # for neural classifier\n",
        "\n",
        "def load_model(path):\n",
        "    return joblib.load(path)\n",
        "\n",
        "def invert_dict(dictionary):\n",
        "    return dict(zip(dictionary.values(), dictionary.keys()))\n",
        "\n",
        "TEXT_VECTORIZER = load_model('/content/drive/My Drive/vectorizer.pkl')\n",
        "\n",
        "# adjust vocabulary to account for unknowns\n",
        "VOCABULARY = TEXT_VECTORIZER.vocabulary_\n",
        "INVERSE_VOCABULARY = invert_dict(VOCABULARY)\n",
        "VOCABULARY[INVERSE_VOCABULARY[0]] = len(VOCABULARY)\n",
        "VOCABULARY['CUSTOM_UNKNOWN'] = len(VOCABULARY)+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## DATA PREP\n",
        "def convert_to_indices(text):\n",
        "    # tokenize input text\n",
        "    tokens = re.compile(RE_PATTERN).split(text)\n",
        "    non_empty_tokens = list(filter(lambda token: token, tokens))\n",
        "\n",
        "    indices = []\n",
        "\n",
        "    # collect indices of tokens in vocabulary\n",
        "    for token in non_empty_tokens:\n",
        "        if token in VOCABULARY:\n",
        "            index = VOCABULARY[token]\n",
        "        else:\n",
        "            index = VOCABULARY['CUSTOM_UNKNOWN']\n",
        "\n",
        "        indices.append(index)\n",
        "\n",
        "    return indices\n",
        "\n",
        "def format_inputs(texts):\n",
        "    # prepare texts for use in neural classifier\n",
        "    texts_as_indices = []\n",
        "    for text in texts:\n",
        "        texts_as_indices.append(convert_to_indices(text))\n",
        "    return pad_sequences(texts_as_indices, maxlen=MAX_SEQ_LEN, padding='post', truncating='post', value=0.)\n",
        "\n",
        "def merge_datasets(dataset1, dataset2):\n",
        "    x = []\n",
        "    x.extend(dataset1)\n",
        "    x.extend(dataset2)\n",
        "    return x\n",
        "\n",
        "def load_dataset(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        data.append(f.read())\n",
        "    data = [s.strip() for s in data]\n",
        "    return data\n",
        "\n",
        "\n",
        "    \n",
        "## NATURALNESS CLASSIFIERS\n",
        "class NaturalnessClassifier:\n",
        "    '''\n",
        "    An external classifier was trained for a style transfer model -\n",
        "    more specifically using its inputs and outputs excluding test samples.\n",
        "\n",
        "    Use UnigramBasedClassifier (TBD) or NeuralBasedClassifier to load a\n",
        "    trained classifier and score texts of a given style transfer model.\n",
        "    The scores represent the probabilities of the texts being 'natural'.\n",
        "\n",
        "    '''\n",
        "\n",
        "    pass\n",
        "\n",
        "class UnigramBasedClassifier(NaturalnessClassifier):\n",
        "    ''' \n",
        "    Might implement in future if neccessary\n",
        "\n",
        "    '''\n",
        "\n",
        "class NeuralBasedClassifier(NaturalnessClassifier):\n",
        "    def __init__(self, style_transfer_model_name):\n",
        "        self.path = f'{NATURALNESS_CLASSIFIER_BASE_PATH}/neural_{style_transfer_model_name}.h5'\n",
        "        self.classifier = load_keras_model(self.path)\n",
        "\n",
        "    def score(self, texts):\n",
        "        inps = format_inputs(texts)\n",
        "        distribution = self.classifier.predict(inps)\n",
        "        scores = distribution.squeeze()\n",
        "        return scores\n",
        "\n",
        "    def summary(self):\n",
        "        return self.classifier.summary()\n",
        "\n",
        "model = 'ARAE' \n",
        "\n",
        "\n",
        "# # load data\n",
        "generated_1 = ['Her long muscular hands were free, her pink mane kept catching the rays, and the muscles, and the muscles of her hands continued to embrace, as if she suddenly slipped into a womans body.']\n",
        "generated_2 = ['he sat at the window of the train, brooding, shaking her head, shaking herself, and could not quite make ones feet rest on the floor. The window frame trembled with the speed of the motion, the empty darkness, and dots of light slashed across the glass as luminous streaks, once in a while. Her leg, sculptured by the tight sheen of the stocking, its long line running straight, now developed lateral instep at the base of the heel, accompanied by a small elastic at the tip; it, had a feminine elegance that seemed out of place in the dusty train car and oddly fitting the contours of her profile. She wore the most expensive dress in the world, the most expensive shoes, the most expensive mouth, wrapped shapelessly about her slender, nervous body']\n",
        "output_texts = merge_datasets(generated_1, generated_2)\n",
        "\n",
        "# # score\n",
        "neural_classifier = NeuralBasedClassifier(model)\n",
        "print(neural_classifier.score(output_texts))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version pre-0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "[0.05994288 0.9818979 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oH9wDGoDdg7",
        "colab_type": "text"
      },
      "source": [
        "# Content Preservation\n",
        "\n",
        "The below implementation of content preservation has the functionality for implementing a style mask (removing a set of style specific tokens) and then calculating WMD on the leftover 'sans-style' texts to see if content still has meaning. However, we are currently implemented WMD calculation without a style mask as we are still exploring different strategies to come up with a style mask. This is something we are currently working on and will add soon in the future. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsxKfHGewtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6001c8c2-4299-48b6-a798-402aa300191a"
      },
      "source": [
        "\"\"\"EVALUATION OF CONTENT PRESERVATION\n",
        "\n",
        "This code is used for evaluation of content preservation between input and output texts of a style transfer model.\n",
        "\n",
        "Word Mover's Distance (WMD) on texts with style masking (i.e. placeholders used in place of style words)\n",
        "exhibited the highest correlation with human evaluations of the same texts as per [1].\n",
        "\n",
        "Usage:\n",
        "    - Mask style words in a set of texts prior to evaluation                -> mark_style_words(texts, mask_style=True)\n",
        "    - Train a Word2Vec model for dataset, for use in WMD calculation   -> train_word2vec_model(...)\n",
        "    - Calculate WMD scores for  input/output texts                  -> calculate_wmd_scores(...)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "CUSTOM_STYLE = 'customstyle'\n",
        "STYLE_MODIFICATION_SETTINGS = ['style_masked', 'style_removed']\n",
        "\n",
        "\n",
        "# DATA PREP\n",
        "def mark_style_words(texts, style_tokens=None, mask_style=False):\n",
        "    '''\n",
        "    Mask or remove style words (based on a set of style tokens) from input texts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : list\n",
        "        String inputs\n",
        "    style_tokens : set\n",
        "        Style tokens\n",
        "    mask_style : boolean\n",
        "        Set to False to remove style tokens, True to replace with placeholder\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    edited_texts : list\n",
        "        Texts with style tokens masked or removed\n",
        "\n",
        "    '''\n",
        "\n",
        "    edited_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = tokenize(text)\n",
        "        edited_tokens = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if token.lower() in style_tokens:\n",
        "                if mask_style:\n",
        "                    edited_tokens.append(CUSTOM_STYLE)\n",
        "            else:\n",
        "                edited_tokens.append(token)\n",
        "\n",
        "        edited_texts.append(' '.join(edited_tokens))\n",
        "\n",
        "    return edited_texts\n",
        "\n",
        "\n",
        "def generate_style_modified_texts(texts):\n",
        "\n",
        "    # ensure consistent tokenization under different style modification settings\n",
        "    unmasked_texts = mark_style_words(texts, {})\n",
        "    texts_with_style_removed = mark_style_words(texts)\n",
        "    texts_with_style_masked = mark_style_words(texts, mask_style=True)\n",
        "    return unmasked_texts, texts_with_style_removed, texts_with_style_masked\n",
        "\n",
        "\n",
        "# MODELS / SCORING OF WMD\n",
        "def train_word2vec_model(texts, path):\n",
        "    tokenized_texts = []\n",
        "    for text in texts:\n",
        "        tokenized_texts.append(tokenize(text))\n",
        "    model = Word2Vec(tokenized_texts)\n",
        "    model.save(path)\n",
        "\n",
        "\n",
        "def load_word2vec_model(path):\n",
        "    model = Word2Vec.load(path)\n",
        "    model.init_sims(replace=True)  # normalize vectors\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_wmd_scores(references, candidates, wmd_model):\n",
        "    '''\n",
        "    Calculate Word Mover's Distance for each (reference, candidate)\n",
        "    pair in a list of reference texts and candidate texts.\n",
        "\n",
        "    The lower the distance, the more similar the texts are.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    references : list\n",
        "        Input texts\n",
        "    candidates : list\n",
        "        Output texts (e.g. from a style transfer model)\n",
        "    wmd_model : gensim.models.word2vec.Word2Vec\n",
        "        Trained Word2Vec model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    wmd_scores : list\n",
        "        WMD scores for all pairs\n",
        "\n",
        "    '''\n",
        "\n",
        "    wmd_scores = []\n",
        "\n",
        "    for i in range(len(references)):\n",
        "        wmd = wmd_model.wv.wmdistance(tokenize(references[i]), tokenize(candidates[i]))\n",
        "        wmd_scores.append(wmd)\n",
        "\n",
        "    return wmd_scores\n",
        "\n",
        "\n",
        "## Example usage \n",
        "\n",
        "source_1 = ['It swept space clean, and left nothing but the joy of an unobstructed effort. Only a faint echo within the sounds spoke of that from which the music had escaped, but spoke in laughing astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was the song of an immense deliverance.']\n",
        "generated_1 = ['It was a sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the tension of purpose. It swept space clean, and at the same time coming empty. The only sound was the coming of breathless, muffled room, but spoke in laughing astonishment at the discovery that there was no ugliness or pain, in fact, there was nothing at all. It was the song of an immense deliverance.']\n",
        "all_texts = merge_datasets(generated_1, generated_2)\n",
        "\n",
        "# # train models\n",
        "w2v_model_path = '/content/drive/My Drive/w2v'\n",
        "train_word2vec_model(all_texts, w2v_model_path)\n",
        "w2v_model = load_word2vec_model(w2v_model_path)\n",
        "print(calculate_wmd_scores(source_1, generated_1, w2v_model))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.15688595803156002]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}