{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Metrics.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"6HO9ban-pt5W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"9f6aa9a0-06b6-448e-f430-c9e8b57258e7","executionInfo":{"status":"ok","timestamp":1572950949698,"user_tz":480,"elapsed":44011,"user":{"displayName":"Guneet Singh Kohli","photoUrl":"","userId":"05329946377020450732"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EStghsVkfG_1","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8v7CR99spqj","colab_type":"code","colab":{}},"source":["from pyemd import emd\n","import numpy as np\n","import pandas as pd\n","from tokenizer import RE_PATTERN\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OlHZyResh9K","colab_type":"code","colab":{}},"source":["\"\"\" EVALUATION OF STYLE TRANSFER INTENSITY\n","\n","This code is used for evaluation of style transfer intensity (STI) between input and output texts of a style transfer model.\n","Note that STI, makes use of both the input and the output texts for a more meaningful evaluation.\n","STI tells how *much* the style changed from the input to the output.\n","\n","Two output texts could exhibit the same overall target style, but with one being more pronounced than the other,\n","e.g. \"i like this\" vs. \"i love this !\" While past evaluations do not quantify that, STI can,\n","given style distributions from a style classifier trained on labeled style datasets.\n","\n","As per [1] STI, based on Earth Mover's Distance (EMD) between style distributions of input and output texts,\n","exhibited higher correlation with human evaluations of the same texts than did the method of past evaluations\n","(using target style scores of output texts alone).\n","\n","Usage:\n","    - Calculate STI for  input/output text style distributions              -> calculate_direction_corrected_emd(...)\n","\n","\"\"\"\n","\n","\n","def calculate_emd(input_distribution, output_distribution):\n","    '''\n","    Calculate Earth Mover's Distance (aka Wasserstein distance) between\n","    two distributions of equal length.\n","\n","    Parameters\n","    ----------\n","    input_distribution : numpy.ndarray\n","        Probabilities assigned to style classes for an input text\n","    output_distribution : numpy.ndarray\n","        Probabilities assigned to style classes for an output text, e.g. of a style transfer model\n","\n","    Returns\n","    -------\n","    Earth Mover's Distance (float) between the two given style distributions\n","\n","    '''\n","\n","    N = len(input_distribution)\n","    distance_matrix = np.ones((N, N))\n","    return emd(input_distribution, output_distribution, distance_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XF1blTzw34WU","colab_type":"code","colab":{}},"source":["donor_y = pd.read_csv('/content/drive/My Drive/donor_y.csv')\n","donor_yhat = pd.read_csv('/content/drive/My Drive/donor_yhat.csv')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3HGEwxU35uN","colab_type":"code","colab":{}},"source":["emd_calc = []\n","for i in range(donor_yhat.shape[0]):\n","    emd_calc.append(calculate_emd(np.array(donor_y, order='C')[i].astype('float64'), np.array(donor_yhat, order='C')[i].astype('float64')))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRAAJO4e5n8T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"277d6017-d2c5-4cea-c804-dd39925d37bd","executionInfo":{"status":"ok","timestamp":1572950951417,"user_tz":480,"elapsed":45650,"user":{"displayName":"Guneet Singh Kohli","photoUrl":"","userId":"05329946377020450732"}}},"source":["pd.DataFrame(emd_calc, columns=['EMD']).plot.hist();"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZShIklqGgiSpZShIklr/H54lOFbGCMDmAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"sBJS7-3YC_ed","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":678},"outputId":"c8b978fb-8c35-4cd7-ba64-17657aace124","executionInfo":{"status":"ok","timestamp":1572950958173,"user_tz":480,"elapsed":52332,"user":{"displayName":"Guneet Singh Kohli","photoUrl":"","userId":"05329946377020450732"}}},"source":["\"\"\"EVALUATION OF NATURALNESS\n","This is used to evaluate the naturalness of output sentiment texts of examined style transfer models.\n","For a baseline understanding of what is considered \"natural,\" any method used for automated evaluation of naturalness\n","also requires an understanding of the human-sourced input texts.\n","Inspired by the adversarial evaluation approach in \"Generating Sentences from a Continuous Space\"\n","(Bowman et al., 2016), we use pretrained unigram logistic regression classifiers and LSTM logistic regression classifiers available from [1]\n","on samples of input texts and output texts for each style transfer model.\n","Via adversarial evaluation, the classifiers must distinguish human-generated inputs from machine-generated outputs.\n","The more natural an output is, the likelier it is to fool an adversarial classifier.\n","\n","    - Calculate naturalness scores for texts with clf, a NaturalnessClassifier      -> clf.score(...)\n","You can find examples of more detailed usage commands below.\n","\"\"\"\n","\n","from collections import Counter\n","from keras.models import load_model as load_keras_model\n","from keras.preprocessing.sequence import pad_sequences\n","from tokenizer import RE_PATTERN\n","from sklearn.externals import joblib\n","import re\n","\n","NATURALNESS_CLASSIFIER_BASE_PATH = '/content/drive/My Drive/NaturalnessClassifier/'\n","MAX_SEQ_LEN = 30 # for neural classifier\n","\n","def load_model(path):\n","    return joblib.load(path)\n","\n","def invert_dict(dictionary):\n","    return dict(zip(dictionary.values(), dictionary.keys()))\n","\n","TEXT_VECTORIZER = load_model('/content/drive/My Drive/vectorizer.pkl')\n","\n","# adjust vocabulary to account for unknowns\n","VOCABULARY = TEXT_VECTORIZER.vocabulary_\n","INVERSE_VOCABULARY = invert_dict(VOCABULARY)\n","VOCABULARY[INVERSE_VOCABULARY[0]] = len(VOCABULARY)\n","VOCABULARY['CUSTOM_UNKNOWN'] = len(VOCABULARY)+1\n","\n","\n","\n","\n","## DATA PREP\n","def convert_to_indices(text):\n","    # tokenize input text\n","    tokens = re.compile(RE_PATTERN).split(text)\n","    non_empty_tokens = list(filter(lambda token: token, tokens))\n","\n","    indices = []\n","\n","    # collect indices of tokens in vocabulary\n","    for token in non_empty_tokens:\n","        if token in VOCABULARY:\n","            index = VOCABULARY[token]\n","        else:\n","            index = VOCABULARY['CUSTOM_UNKNOWN']\n","\n","        indices.append(index)\n","\n","    return indices\n","\n","def format_inputs(texts):\n","    # prepare texts for use in neural classifier\n","    texts_as_indices = []\n","    for text in texts:\n","        texts_as_indices.append(convert_to_indices(text))\n","    return pad_sequences(texts_as_indices, maxlen=MAX_SEQ_LEN, padding='post', truncating='post', value=0.)\n","\n","def merge_datasets(dataset1, dataset2):\n","    x = []\n","    x.extend(dataset1)\n","    x.extend(dataset2)\n","    return x\n","\n","def load_dataset(path):\n","    data = []\n","    with open(path) as f:\n","        data.append(f.read())\n","    data = [s.strip() for s in data]\n","    return data\n","\n","\n","    \n","## NATURALNESS CLASSIFIERS\n","class NaturalnessClassifier:\n","    '''\n","    An external classifier was trained for each examined style transfer model -\n","    more specifically using its inputs and outputs, of course excluding test samples.\n","\n","    Use UnigramBasedClassifier or NeuralBasedClassifier to load a\n","    trained classifier and score texts of a given style transfer model.\n","    The scores represent the probabilities of the texts being 'natural'.\n","\n","    '''\n","\n","    pass\n","\n","class UnigramBasedClassifier(NaturalnessClassifier):\n","    ''' \n","    Might implement in future if neccessary\n","\n","    '''\n","\n","class NeuralBasedClassifier(NaturalnessClassifier):\n","    def __init__(self, style_transfer_model_name):\n","        self.path = f'{NATURALNESS_CLASSIFIER_BASE_PATH}/neural_{style_transfer_model_name}.h5'\n","        self.classifier = load_keras_model(self.path)\n","\n","    def score(self, texts):\n","        inps = format_inputs(texts)\n","        distribution = self.classifier.predict(inps)\n","        scores = distribution.squeeze()\n","        return scores\n","\n","    def summary(self):\n","        return self.classifier.summary()\n","\n","model = 'ARAE' \n","\n","\n","# # load data\n","generated_1 = ['Her long muscular hands were free, her pink mane kept catching the rays, and the muscles, and the muscles of her hands continued to embrace, as if she suddenly slipped into a womans body.']\n","generated_2 = ['he sat at the window of the train, brooding, shaking her head, shaking herself, and could not quite make ones feet rest on the floor. The window frame trembled with the speed of the motion, the empty darkness, and dots of light slashed across the glass as luminous streaks, once in a while. Her leg, sculptured by the tight sheen of the stocking, its long line running straight, now developed lateral instep at the base of the heel, accompanied by a small elastic at the tip; it, had a feminine elegance that seemed out of place in the dusty train car and oddly fitting the contours of her profile. She wore the most expensive dress in the world, the most expensive shoes, the most expensive mouth, wrapped shapelessly about her slender, nervous body']\n","output_texts = merge_datasets(generated_1, generated_2)\n","\n","# # score\n","neural_classifier = NeuralBasedClassifier(model)\n","print(neural_classifier.score(output_texts))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=DeprecationWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version pre-0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n","  UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","[0.05994292 0.981898  ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UgsxKfHGewtr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"670f6e77-14fb-47dc-b878-e290f3672d66","executionInfo":{"status":"ok","timestamp":1572954059124,"user_tz":480,"elapsed":487,"user":{"displayName":"Guneet Singh Kohli","photoUrl":"","userId":"05329946377020450732"}}},"source":["\"\"\"EVALUATION OF CONTENT PRESERVATION\n","\n","This code is used for evaluation of content preservation between input and output texts of a style transfer model.\n","\n","Word Mover's Distance (WMD) on texts with style masking (i.e. placeholders used in place of style words)\n","exhibited the highest correlation with human evaluations of the same texts as per [1].\n","\n","Usage:\n","    - Mask style words in a set of texts prior to evaluation                -> mark_style_words(texts, mask_style=True)\n","    - Train a Word2Vec model for dataset, for use in WMD calculation   -> train_word2vec_model(...)\n","    - Calculate WMD scores for  input/output texts                  -> calculate_wmd_scores(...)\n","\n","\n","\"\"\"\n","\n","from gensim.models.word2vec import Word2Vec\n","from tokenizer import tokenize\n","\n","CUSTOM_STYLE = 'customstyle'\n","STYLE_MODIFICATION_SETTINGS = ['style_masked', 'style_removed']\n","\n","\n","# DATA PREP\n","def mark_style_words(texts, style_tokens=None, mask_style=False):\n","    '''\n","    Mask or remove style words (based on a set of style tokens) from input texts.\n","\n","    Parameters\n","    ----------\n","    texts : list\n","        String inputs\n","    style_tokens : set\n","        Style tokens\n","    mask_style : boolean\n","        Set to False to remove style tokens, True to replace with placeholder\n","\n","    Returns\n","    -------\n","    edited_texts : list\n","        Texts with style tokens masked or removed\n","\n","    '''\n","\n","    edited_texts = []\n","\n","    for text in texts:\n","        tokens = tokenize(text)\n","        edited_tokens = []\n","\n","        for token in tokens:\n","            if token.lower() in style_tokens:\n","                if mask_style:\n","                    edited_tokens.append(CUSTOM_STYLE)\n","            else:\n","                edited_tokens.append(token)\n","\n","        edited_texts.append(' '.join(edited_tokens))\n","\n","    return edited_texts\n","\n","\n","def generate_style_modified_texts(texts):\n","\n","    # ensure consistent tokenization under different style modification settings\n","    unmasked_texts = mark_style_words(texts, {})\n","    texts_with_style_removed = mark_style_words(texts)\n","    texts_with_style_masked = mark_style_words(texts, mask_style=True)\n","    return unmasked_texts, texts_with_style_removed, texts_with_style_masked\n","\n","\n","# MODELS / SCORING OF WMD\n","def train_word2vec_model(texts, path):\n","    tokenized_texts = []\n","    for text in texts:\n","        tokenized_texts.append(tokenize(text))\n","    model = Word2Vec(tokenized_texts)\n","    model.save(path)\n","\n","\n","def load_word2vec_model(path):\n","    model = Word2Vec.load(path)\n","    model.init_sims(replace=True)  # normalize vectors\n","    return model\n","\n","\n","def calculate_wmd_scores(references, candidates, wmd_model):\n","    '''\n","    Calculate Word Mover's Distance for each (reference, candidate)\n","    pair in a list of reference texts and candidate texts.\n","\n","    The lower the distance, the more similar the texts are.\n","\n","    Parameters\n","    ----------\n","    references : list\n","        Input texts\n","    candidates : list\n","        Output texts (e.g. from a style transfer model)\n","    wmd_model : gensim.models.word2vec.Word2Vec\n","        Trained Word2Vec model\n","\n","    Returns\n","    -------\n","    wmd_scores : list\n","        WMD scores for all pairs\n","\n","    '''\n","\n","    wmd_scores = []\n","\n","    for i in range(len(references)):\n","        wmd = wmd_model.wv.wmdistance(tokenize(references[i]), tokenize(candidates[i]))\n","        wmd_scores.append(wmd)\n","\n","    return wmd_scores\n","\n","\n","# EXAMPLE USAGE (uncomment the following to play around with code)\n","# load data to train models used for WMD calculations\n","# all_texts = load_dataset('../data/sentiment.all')\n","# all_texts_style_masked = mark_style_words(all_texts, mask_style=True)\n","\n","source_1 = ['It swept space clean, and left nothing but the joy of an unobstructed effort. Only a faint echo within the sounds spoke of that from which the music had escaped, but spoke in laughing astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was the song of an immense deliverance.']\n","generated_1 = ['It was a sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the tension of purpose. It swept space clean, and at the same time coming empty. The only sound was the coming of breathless, muffled room, but spoke in laughing astonishment at the discovery that there was no ugliness or pain, in fact, there was nothing at all. It was the song of an immense deliverance.']\n","all_texts = merge_datasets(generated_1, generated_2)\n","\n","# # train models\n","w2v_model_path = '/content/drive/My Drive/w2v'\n","#w2v_model_style_masked_path = '../models/word2vec_masked'\n","train_word2vec_model(all_texts, w2v_model_path)\n","# # train_word2vec_model(all_texts_style_masked, w2v_model_style_masked_path)\n","w2v_model = load_word2vec_model(w2v_model_path)\n","# w2v_model_style_masked = load_word2vec_model(w2v_model_style_masked_path)\n","print(calculate_wmd_scores(source_1, generated_1, w2v_model))\n","\n","# # load texts under different style modification settings\n","# input_neg_texts = load_dataset('../data/sentiment.test.0')\n","# input_pos_texts = load_dataset('../data/sentiment.test.1')\n","# input_texts = merge_datasets(input_neg_texts, input_pos_texts)\n","#unmasked_inputs, inputs_with_style_removed, inputs_with_style_masked = generate_style_modified_texts(input_texts)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[0.1561831731232212]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]}]}