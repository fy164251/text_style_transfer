{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert_masked.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "73e5429e-b61f-4e3e-fb3a-e9137022db96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\r\u001b[K     |█                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.27)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 63.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 61.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.27)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=f09f17cbabd9dd9d1ffdcc3724d15e4d6939111880eb3d4918b70ba2d89c9600\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sentencepiece, sacremoses, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "outputId": "1d39c6c3-4e70-45d1-c9a7-adebf022210d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, without masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "# lbl_enc = preprocessing.LabelEncoder()\n",
        "# y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkqPiy6v2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, with masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Embeddings()\n",
        "\n",
        "X_text = []\n",
        "for sentence in tqdm(X):\n",
        "    X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQfIbHooxmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Test Time\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "X_old = df.old.astype('str')\n",
        "X_new = df.new.astype('str')\n",
        "\n",
        "model = Embeddings()\n",
        "\n",
        "X_old_text, X_new_text = [], []\n",
        "for sentence in tqdm(X_old):\n",
        "    X_old_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "for sentence in tqdm(X_new):\n",
        "    X_new_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_YrIamp5Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_old = pd.DataFrame(X_old_text)\n",
        "# X_old.to_csv('./gdrive/My Drive/DL/Style/X_old_Embedding.csv')\n",
        "\n",
        "# X_new = pd.DataFrame(X_new_text)\n",
        "# X_new.to_csv('./gdrive/My Drive/DL/Style/X_new_Embedding.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplxzc29EEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16TunKA3KVd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ad8c70dc-3039-4242-d696-c53bd38f694f"
      },
      "source": [
        "# For old Rand examples\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/rand_examples.csv'\n",
        "# df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "\n",
        "# X1 = df['Rand-donor-text'].astype('str')\n",
        "# X2 = df['Rand_117M_10000_Nabokov-All-3'].astype('str')\n",
        "# X3 = df['Rand-output-ngram'].astype('str')\n",
        "\n",
        "# model = Embeddings()\n",
        "\n",
        "# X1_text, X2_text, X3_text = [], [], []\n",
        "# for sentence in tqdm(X1):\n",
        "#     X1_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X2_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X3_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1184340.76B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 48256.47B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 36565607.31B/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.65it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.43it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQlnt96NhI9",
        "colab_type": "text"
      },
      "source": [
        "### Building A Atyle Alassifier on Top of DistilBert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "f42769ec-5bb7-4e62-b81c-38bdbb3a207a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUmYQdKgdUq",
        "colab_type": "code",
        "outputId": "4c48addf-fd8c-4cda-faa7-ff3505b01759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5614</th>\n",
              "      <td>-0.340161</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>0.602034</td>\n",
              "      <td>-0.272003</td>\n",
              "      <td>-0.341873</td>\n",
              "      <td>-0.120357</td>\n",
              "      <td>0.224259</td>\n",
              "      <td>0.206777</td>\n",
              "      <td>-0.050708</td>\n",
              "      <td>-0.413362</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.422804</td>\n",
              "      <td>-0.132413</td>\n",
              "      <td>0.146387</td>\n",
              "      <td>-0.105816</td>\n",
              "      <td>0.753637</td>\n",
              "      <td>0.414343</td>\n",
              "      <td>0.122540</td>\n",
              "      <td>0.179269</td>\n",
              "      <td>0.088668</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>-0.148916</td>\n",
              "      <td>0.095241</td>\n",
              "      <td>0.706351</td>\n",
              "      <td>0.180122</td>\n",
              "      <td>0.449534</td>\n",
              "      <td>0.147201</td>\n",
              "      <td>-0.237596</td>\n",
              "      <td>-0.151617</td>\n",
              "      <td>-0.037453</td>\n",
              "      <td>0.579897</td>\n",
              "      <td>0.074145</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>-0.265257</td>\n",
              "      <td>-0.118240</td>\n",
              "      <td>-0.280390</td>\n",
              "      <td>0.099969</td>\n",
              "      <td>-0.501312</td>\n",
              "      <td>0.068412</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.236783</td>\n",
              "      <td>-0.722360</td>\n",
              "      <td>-0.142746</td>\n",
              "      <td>-0.070288</td>\n",
              "      <td>-0.142651</td>\n",
              "      <td>-0.857589</td>\n",
              "      <td>-0.207489</td>\n",
              "      <td>0.172034</td>\n",
              "      <td>-0.133808</td>\n",
              "      <td>-0.344236</td>\n",
              "      <td>-0.061278</td>\n",
              "      <td>0.064547</td>\n",
              "      <td>0.122214</td>\n",
              "      <td>-0.097235</td>\n",
              "      <td>0.183206</td>\n",
              "      <td>-0.092817</td>\n",
              "      <td>-0.457735</td>\n",
              "      <td>0.098827</td>\n",
              "      <td>-0.223504</td>\n",
              "      <td>0.734053</td>\n",
              "      <td>-0.446106</td>\n",
              "      <td>0.604875</td>\n",
              "      <td>0.058997</td>\n",
              "      <td>-0.558629</td>\n",
              "      <td>-0.211270</td>\n",
              "      <td>-0.087967</td>\n",
              "      <td>-0.125183</td>\n",
              "      <td>-0.588484</td>\n",
              "      <td>-0.453498</td>\n",
              "      <td>-0.176318</td>\n",
              "      <td>-0.628320</td>\n",
              "      <td>0.045665</td>\n",
              "      <td>0.183669</td>\n",
              "      <td>-0.280129</td>\n",
              "      <td>-0.195073</td>\n",
              "      <td>-0.313454</td>\n",
              "      <td>-0.230915</td>\n",
              "      <td>-0.017511</td>\n",
              "      <td>0.452152</td>\n",
              "      <td>0.481604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2383</th>\n",
              "      <td>-0.391934</td>\n",
              "      <td>0.589659</td>\n",
              "      <td>0.081654</td>\n",
              "      <td>-0.364856</td>\n",
              "      <td>0.272116</td>\n",
              "      <td>-0.185212</td>\n",
              "      <td>-0.179081</td>\n",
              "      <td>0.248599</td>\n",
              "      <td>-0.116149</td>\n",
              "      <td>-0.085957</td>\n",
              "      <td>0.279195</td>\n",
              "      <td>-0.495050</td>\n",
              "      <td>-0.024453</td>\n",
              "      <td>0.470089</td>\n",
              "      <td>0.149684</td>\n",
              "      <td>1.075918</td>\n",
              "      <td>0.142027</td>\n",
              "      <td>-0.185910</td>\n",
              "      <td>0.287008</td>\n",
              "      <td>0.135054</td>\n",
              "      <td>0.798489</td>\n",
              "      <td>-0.099341</td>\n",
              "      <td>0.331279</td>\n",
              "      <td>0.542002</td>\n",
              "      <td>0.268069</td>\n",
              "      <td>0.279488</td>\n",
              "      <td>0.068858</td>\n",
              "      <td>-0.253427</td>\n",
              "      <td>-0.456926</td>\n",
              "      <td>0.170671</td>\n",
              "      <td>0.415524</td>\n",
              "      <td>-0.215536</td>\n",
              "      <td>-0.112373</td>\n",
              "      <td>-0.604337</td>\n",
              "      <td>0.137619</td>\n",
              "      <td>-0.606019</td>\n",
              "      <td>0.378203</td>\n",
              "      <td>-0.173980</td>\n",
              "      <td>0.747436</td>\n",
              "      <td>0.213383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.194351</td>\n",
              "      <td>-0.251167</td>\n",
              "      <td>-0.103786</td>\n",
              "      <td>-0.614198</td>\n",
              "      <td>-0.427270</td>\n",
              "      <td>-0.536838</td>\n",
              "      <td>-0.142121</td>\n",
              "      <td>0.175018</td>\n",
              "      <td>0.257659</td>\n",
              "      <td>0.063984</td>\n",
              "      <td>0.204867</td>\n",
              "      <td>0.264982</td>\n",
              "      <td>0.090459</td>\n",
              "      <td>-0.156936</td>\n",
              "      <td>0.328172</td>\n",
              "      <td>-0.267422</td>\n",
              "      <td>0.004332</td>\n",
              "      <td>0.437211</td>\n",
              "      <td>0.271880</td>\n",
              "      <td>0.837500</td>\n",
              "      <td>-0.222474</td>\n",
              "      <td>0.509900</td>\n",
              "      <td>-0.052957</td>\n",
              "      <td>-0.122060</td>\n",
              "      <td>0.306961</td>\n",
              "      <td>0.155461</td>\n",
              "      <td>0.102520</td>\n",
              "      <td>-0.191638</td>\n",
              "      <td>-0.222842</td>\n",
              "      <td>-0.347603</td>\n",
              "      <td>-0.569109</td>\n",
              "      <td>0.100797</td>\n",
              "      <td>-0.224095</td>\n",
              "      <td>0.094102</td>\n",
              "      <td>0.375749</td>\n",
              "      <td>0.166803</td>\n",
              "      <td>-0.509638</td>\n",
              "      <td>0.045654</td>\n",
              "      <td>0.267326</td>\n",
              "      <td>0.322551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>-0.311391</td>\n",
              "      <td>0.246223</td>\n",
              "      <td>0.322206</td>\n",
              "      <td>-0.626673</td>\n",
              "      <td>-0.120535</td>\n",
              "      <td>-0.328576</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>0.150335</td>\n",
              "      <td>-0.228633</td>\n",
              "      <td>-0.119519</td>\n",
              "      <td>0.234018</td>\n",
              "      <td>-0.455078</td>\n",
              "      <td>0.094305</td>\n",
              "      <td>0.039104</td>\n",
              "      <td>-0.354940</td>\n",
              "      <td>1.053825</td>\n",
              "      <td>0.437839</td>\n",
              "      <td>0.352679</td>\n",
              "      <td>0.163303</td>\n",
              "      <td>-0.222175</td>\n",
              "      <td>0.575262</td>\n",
              "      <td>-0.151778</td>\n",
              "      <td>0.039601</td>\n",
              "      <td>0.237160</td>\n",
              "      <td>0.136593</td>\n",
              "      <td>0.441844</td>\n",
              "      <td>0.216453</td>\n",
              "      <td>-0.411612</td>\n",
              "      <td>-0.499689</td>\n",
              "      <td>-0.125523</td>\n",
              "      <td>0.927454</td>\n",
              "      <td>-0.064041</td>\n",
              "      <td>0.280551</td>\n",
              "      <td>0.071708</td>\n",
              "      <td>-0.018094</td>\n",
              "      <td>-0.288303</td>\n",
              "      <td>0.428391</td>\n",
              "      <td>-0.406295</td>\n",
              "      <td>0.066329</td>\n",
              "      <td>0.261219</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063150</td>\n",
              "      <td>-0.204630</td>\n",
              "      <td>-0.137450</td>\n",
              "      <td>-0.170344</td>\n",
              "      <td>-0.314548</td>\n",
              "      <td>-0.759352</td>\n",
              "      <td>-0.347153</td>\n",
              "      <td>-0.266005</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0.089698</td>\n",
              "      <td>-0.136013</td>\n",
              "      <td>0.242091</td>\n",
              "      <td>0.530148</td>\n",
              "      <td>-0.121507</td>\n",
              "      <td>0.408584</td>\n",
              "      <td>-0.177793</td>\n",
              "      <td>-0.448586</td>\n",
              "      <td>-0.175234</td>\n",
              "      <td>0.130837</td>\n",
              "      <td>0.919979</td>\n",
              "      <td>-0.481680</td>\n",
              "      <td>0.259219</td>\n",
              "      <td>0.216071</td>\n",
              "      <td>-0.403066</td>\n",
              "      <td>-0.032478</td>\n",
              "      <td>0.047672</td>\n",
              "      <td>0.106018</td>\n",
              "      <td>-0.227465</td>\n",
              "      <td>-0.409935</td>\n",
              "      <td>-0.288559</td>\n",
              "      <td>-0.625146</td>\n",
              "      <td>0.146311</td>\n",
              "      <td>0.043609</td>\n",
              "      <td>0.026973</td>\n",
              "      <td>0.094333</td>\n",
              "      <td>-0.698623</td>\n",
              "      <td>-0.382027</td>\n",
              "      <td>-0.658689</td>\n",
              "      <td>0.190388</td>\n",
              "      <td>0.197619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8412</th>\n",
              "      <td>-0.157567</td>\n",
              "      <td>0.439942</td>\n",
              "      <td>0.159633</td>\n",
              "      <td>-0.716131</td>\n",
              "      <td>0.394165</td>\n",
              "      <td>-0.170059</td>\n",
              "      <td>0.290004</td>\n",
              "      <td>0.776767</td>\n",
              "      <td>-0.385152</td>\n",
              "      <td>-0.022196</td>\n",
              "      <td>0.137164</td>\n",
              "      <td>-0.146209</td>\n",
              "      <td>0.309307</td>\n",
              "      <td>-0.086701</td>\n",
              "      <td>-0.312970</td>\n",
              "      <td>0.535559</td>\n",
              "      <td>0.378450</td>\n",
              "      <td>0.155951</td>\n",
              "      <td>0.159643</td>\n",
              "      <td>-0.078866</td>\n",
              "      <td>0.229330</td>\n",
              "      <td>-0.042130</td>\n",
              "      <td>-0.046688</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>-0.052326</td>\n",
              "      <td>0.334858</td>\n",
              "      <td>0.294528</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>-0.752542</td>\n",
              "      <td>0.043778</td>\n",
              "      <td>0.562868</td>\n",
              "      <td>0.093217</td>\n",
              "      <td>-0.056954</td>\n",
              "      <td>-0.345783</td>\n",
              "      <td>-0.071456</td>\n",
              "      <td>-0.615525</td>\n",
              "      <td>0.299450</td>\n",
              "      <td>-0.361907</td>\n",
              "      <td>0.153372</td>\n",
              "      <td>0.102023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.160457</td>\n",
              "      <td>-0.595206</td>\n",
              "      <td>-0.231311</td>\n",
              "      <td>-0.040880</td>\n",
              "      <td>0.016651</td>\n",
              "      <td>-0.327501</td>\n",
              "      <td>-0.104689</td>\n",
              "      <td>-0.158801</td>\n",
              "      <td>-0.196048</td>\n",
              "      <td>0.014901</td>\n",
              "      <td>-0.336172</td>\n",
              "      <td>-0.002332</td>\n",
              "      <td>0.202591</td>\n",
              "      <td>-0.242723</td>\n",
              "      <td>0.059663</td>\n",
              "      <td>-0.198993</td>\n",
              "      <td>0.126893</td>\n",
              "      <td>0.143854</td>\n",
              "      <td>0.008069</td>\n",
              "      <td>0.378277</td>\n",
              "      <td>-0.390169</td>\n",
              "      <td>0.389232</td>\n",
              "      <td>0.494898</td>\n",
              "      <td>-0.393445</td>\n",
              "      <td>0.279485</td>\n",
              "      <td>0.022502</td>\n",
              "      <td>0.140657</td>\n",
              "      <td>0.021231</td>\n",
              "      <td>-0.466449</td>\n",
              "      <td>0.030109</td>\n",
              "      <td>-0.550439</td>\n",
              "      <td>0.104452</td>\n",
              "      <td>-0.086756</td>\n",
              "      <td>-0.266146</td>\n",
              "      <td>0.756947</td>\n",
              "      <td>-0.396499</td>\n",
              "      <td>-0.247555</td>\n",
              "      <td>-0.413154</td>\n",
              "      <td>0.299626</td>\n",
              "      <td>0.228981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>-0.166418</td>\n",
              "      <td>0.318898</td>\n",
              "      <td>0.330421</td>\n",
              "      <td>-0.459612</td>\n",
              "      <td>-0.162131</td>\n",
              "      <td>0.267957</td>\n",
              "      <td>0.236269</td>\n",
              "      <td>0.415457</td>\n",
              "      <td>-0.052450</td>\n",
              "      <td>-0.242916</td>\n",
              "      <td>0.365095</td>\n",
              "      <td>-0.367487</td>\n",
              "      <td>-0.144583</td>\n",
              "      <td>0.366747</td>\n",
              "      <td>-0.179655</td>\n",
              "      <td>0.657255</td>\n",
              "      <td>0.487739</td>\n",
              "      <td>0.288062</td>\n",
              "      <td>-0.025196</td>\n",
              "      <td>-0.025123</td>\n",
              "      <td>1.028283</td>\n",
              "      <td>0.087602</td>\n",
              "      <td>0.023006</td>\n",
              "      <td>0.461438</td>\n",
              "      <td>0.195829</td>\n",
              "      <td>0.166410</td>\n",
              "      <td>-0.029758</td>\n",
              "      <td>-0.221556</td>\n",
              "      <td>-0.417432</td>\n",
              "      <td>0.024154</td>\n",
              "      <td>0.216279</td>\n",
              "      <td>-0.060858</td>\n",
              "      <td>0.727997</td>\n",
              "      <td>-0.428053</td>\n",
              "      <td>0.269999</td>\n",
              "      <td>-0.264617</td>\n",
              "      <td>0.174458</td>\n",
              "      <td>-0.049764</td>\n",
              "      <td>0.479403</td>\n",
              "      <td>0.235735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.167074</td>\n",
              "      <td>-0.798584</td>\n",
              "      <td>-0.346666</td>\n",
              "      <td>-0.463683</td>\n",
              "      <td>-0.125071</td>\n",
              "      <td>-0.276027</td>\n",
              "      <td>-0.208865</td>\n",
              "      <td>0.138834</td>\n",
              "      <td>-0.086230</td>\n",
              "      <td>-0.017895</td>\n",
              "      <td>-0.248692</td>\n",
              "      <td>0.168226</td>\n",
              "      <td>0.456018</td>\n",
              "      <td>-0.261686</td>\n",
              "      <td>0.062923</td>\n",
              "      <td>-0.336427</td>\n",
              "      <td>-0.276823</td>\n",
              "      <td>0.119404</td>\n",
              "      <td>0.571929</td>\n",
              "      <td>0.441417</td>\n",
              "      <td>-0.024522</td>\n",
              "      <td>0.412292</td>\n",
              "      <td>-0.085683</td>\n",
              "      <td>-0.287694</td>\n",
              "      <td>-0.030651</td>\n",
              "      <td>0.387611</td>\n",
              "      <td>0.239172</td>\n",
              "      <td>-0.372846</td>\n",
              "      <td>-0.460792</td>\n",
              "      <td>-0.505841</td>\n",
              "      <td>-0.717796</td>\n",
              "      <td>0.123918</td>\n",
              "      <td>0.077422</td>\n",
              "      <td>0.259605</td>\n",
              "      <td>0.282041</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>-0.214877</td>\n",
              "      <td>-0.100879</td>\n",
              "      <td>0.318270</td>\n",
              "      <td>-0.004839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2512</th>\n",
              "      <td>-0.095390</td>\n",
              "      <td>0.449202</td>\n",
              "      <td>0.224358</td>\n",
              "      <td>-0.576839</td>\n",
              "      <td>0.287336</td>\n",
              "      <td>-0.409430</td>\n",
              "      <td>-0.004248</td>\n",
              "      <td>0.780078</td>\n",
              "      <td>-0.096692</td>\n",
              "      <td>-0.372438</td>\n",
              "      <td>-0.105739</td>\n",
              "      <td>-0.333202</td>\n",
              "      <td>0.027714</td>\n",
              "      <td>0.324660</td>\n",
              "      <td>-0.215557</td>\n",
              "      <td>0.960718</td>\n",
              "      <td>0.474020</td>\n",
              "      <td>0.017450</td>\n",
              "      <td>0.015001</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>0.432339</td>\n",
              "      <td>-0.218721</td>\n",
              "      <td>-0.218295</td>\n",
              "      <td>0.222339</td>\n",
              "      <td>0.328858</td>\n",
              "      <td>0.313048</td>\n",
              "      <td>0.265837</td>\n",
              "      <td>-0.011704</td>\n",
              "      <td>-0.654497</td>\n",
              "      <td>-0.086932</td>\n",
              "      <td>0.333828</td>\n",
              "      <td>0.368232</td>\n",
              "      <td>0.113449</td>\n",
              "      <td>-0.388320</td>\n",
              "      <td>-0.254458</td>\n",
              "      <td>-0.130834</td>\n",
              "      <td>0.342358</td>\n",
              "      <td>-0.447215</td>\n",
              "      <td>0.014278</td>\n",
              "      <td>-0.027058</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.537184</td>\n",
              "      <td>-0.904238</td>\n",
              "      <td>-0.234918</td>\n",
              "      <td>-0.608328</td>\n",
              "      <td>0.132864</td>\n",
              "      <td>-0.577511</td>\n",
              "      <td>-0.558329</td>\n",
              "      <td>-0.132951</td>\n",
              "      <td>-0.165845</td>\n",
              "      <td>-0.032519</td>\n",
              "      <td>-0.064881</td>\n",
              "      <td>0.362410</td>\n",
              "      <td>-0.008946</td>\n",
              "      <td>-0.385552</td>\n",
              "      <td>0.028843</td>\n",
              "      <td>-0.635229</td>\n",
              "      <td>-0.100598</td>\n",
              "      <td>0.389611</td>\n",
              "      <td>0.144955</td>\n",
              "      <td>0.429051</td>\n",
              "      <td>-0.211813</td>\n",
              "      <td>0.540881</td>\n",
              "      <td>0.100470</td>\n",
              "      <td>-0.189920</td>\n",
              "      <td>0.252190</td>\n",
              "      <td>0.060027</td>\n",
              "      <td>0.083821</td>\n",
              "      <td>-0.448317</td>\n",
              "      <td>-0.622622</td>\n",
              "      <td>-0.158499</td>\n",
              "      <td>-0.493352</td>\n",
              "      <td>0.116451</td>\n",
              "      <td>-0.185150</td>\n",
              "      <td>-0.215702</td>\n",
              "      <td>0.311338</td>\n",
              "      <td>0.104676</td>\n",
              "      <td>-0.270898</td>\n",
              "      <td>-0.086899</td>\n",
              "      <td>0.236654</td>\n",
              "      <td>0.390968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>-0.145079</td>\n",
              "      <td>0.475734</td>\n",
              "      <td>0.365407</td>\n",
              "      <td>-0.620131</td>\n",
              "      <td>0.328844</td>\n",
              "      <td>0.151111</td>\n",
              "      <td>0.108706</td>\n",
              "      <td>0.852761</td>\n",
              "      <td>-0.457996</td>\n",
              "      <td>-0.247563</td>\n",
              "      <td>-0.029036</td>\n",
              "      <td>-0.127879</td>\n",
              "      <td>0.112376</td>\n",
              "      <td>0.168221</td>\n",
              "      <td>-0.282487</td>\n",
              "      <td>0.590009</td>\n",
              "      <td>0.499115</td>\n",
              "      <td>0.254812</td>\n",
              "      <td>0.427291</td>\n",
              "      <td>-0.123747</td>\n",
              "      <td>0.638318</td>\n",
              "      <td>-0.070385</td>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.335780</td>\n",
              "      <td>0.097913</td>\n",
              "      <td>0.173838</td>\n",
              "      <td>0.614560</td>\n",
              "      <td>0.106443</td>\n",
              "      <td>-0.349500</td>\n",
              "      <td>-0.058919</td>\n",
              "      <td>0.505293</td>\n",
              "      <td>-0.064001</td>\n",
              "      <td>-0.172986</td>\n",
              "      <td>-0.075850</td>\n",
              "      <td>-0.132227</td>\n",
              "      <td>-0.384453</td>\n",
              "      <td>0.370201</td>\n",
              "      <td>-0.468317</td>\n",
              "      <td>-0.201656</td>\n",
              "      <td>0.048853</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.265857</td>\n",
              "      <td>-0.223197</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.323132</td>\n",
              "      <td>0.073406</td>\n",
              "      <td>-0.371595</td>\n",
              "      <td>-0.044837</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>0.018509</td>\n",
              "      <td>0.258279</td>\n",
              "      <td>-0.317874</td>\n",
              "      <td>0.332849</td>\n",
              "      <td>0.212638</td>\n",
              "      <td>0.056430</td>\n",
              "      <td>0.370784</td>\n",
              "      <td>-0.076809</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.098970</td>\n",
              "      <td>0.058247</td>\n",
              "      <td>0.772120</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.695418</td>\n",
              "      <td>0.071432</td>\n",
              "      <td>-0.349727</td>\n",
              "      <td>0.203655</td>\n",
              "      <td>0.144502</td>\n",
              "      <td>0.156430</td>\n",
              "      <td>-0.250324</td>\n",
              "      <td>-0.374293</td>\n",
              "      <td>-0.250994</td>\n",
              "      <td>-0.330648</td>\n",
              "      <td>-0.096069</td>\n",
              "      <td>0.097628</td>\n",
              "      <td>-0.629242</td>\n",
              "      <td>0.276352</td>\n",
              "      <td>-0.184843</td>\n",
              "      <td>-0.126158</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.549695</td>\n",
              "      <td>0.128526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>-0.090581</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.374877</td>\n",
              "      <td>-0.471052</td>\n",
              "      <td>0.101587</td>\n",
              "      <td>0.137666</td>\n",
              "      <td>0.020140</td>\n",
              "      <td>0.438660</td>\n",
              "      <td>-0.227439</td>\n",
              "      <td>-0.421619</td>\n",
              "      <td>-0.015464</td>\n",
              "      <td>-0.266985</td>\n",
              "      <td>-0.196794</td>\n",
              "      <td>0.108737</td>\n",
              "      <td>-0.082186</td>\n",
              "      <td>0.811929</td>\n",
              "      <td>0.190508</td>\n",
              "      <td>0.046835</td>\n",
              "      <td>-0.133266</td>\n",
              "      <td>-0.312746</td>\n",
              "      <td>0.837726</td>\n",
              "      <td>-0.376351</td>\n",
              "      <td>0.239629</td>\n",
              "      <td>0.392617</td>\n",
              "      <td>0.293395</td>\n",
              "      <td>0.253092</td>\n",
              "      <td>0.570215</td>\n",
              "      <td>-0.252462</td>\n",
              "      <td>-0.398234</td>\n",
              "      <td>-0.221244</td>\n",
              "      <td>0.518364</td>\n",
              "      <td>0.045404</td>\n",
              "      <td>0.120460</td>\n",
              "      <td>-0.335099</td>\n",
              "      <td>-0.117464</td>\n",
              "      <td>-0.292256</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>-0.173357</td>\n",
              "      <td>0.191883</td>\n",
              "      <td>0.055143</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.203764</td>\n",
              "      <td>-0.207143</td>\n",
              "      <td>-0.417275</td>\n",
              "      <td>-0.421830</td>\n",
              "      <td>0.229466</td>\n",
              "      <td>-0.182841</td>\n",
              "      <td>0.209093</td>\n",
              "      <td>-0.340755</td>\n",
              "      <td>-0.520002</td>\n",
              "      <td>0.335023</td>\n",
              "      <td>0.064961</td>\n",
              "      <td>0.460892</td>\n",
              "      <td>0.196377</td>\n",
              "      <td>0.173352</td>\n",
              "      <td>0.518383</td>\n",
              "      <td>0.143448</td>\n",
              "      <td>0.168927</td>\n",
              "      <td>-0.019226</td>\n",
              "      <td>-0.087348</td>\n",
              "      <td>0.389946</td>\n",
              "      <td>-0.294117</td>\n",
              "      <td>0.699093</td>\n",
              "      <td>0.011120</td>\n",
              "      <td>-0.384840</td>\n",
              "      <td>-0.081319</td>\n",
              "      <td>0.028739</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>-0.147006</td>\n",
              "      <td>-0.560598</td>\n",
              "      <td>-0.503186</td>\n",
              "      <td>-0.468980</td>\n",
              "      <td>0.193140</td>\n",
              "      <td>0.157212</td>\n",
              "      <td>-0.501378</td>\n",
              "      <td>0.166521</td>\n",
              "      <td>-0.202339</td>\n",
              "      <td>-0.203073</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>0.301909</td>\n",
              "      <td>0.507903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3886</th>\n",
              "      <td>-0.490045</td>\n",
              "      <td>0.403811</td>\n",
              "      <td>0.233433</td>\n",
              "      <td>-1.101597</td>\n",
              "      <td>0.230473</td>\n",
              "      <td>-0.140833</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>0.423244</td>\n",
              "      <td>-0.396777</td>\n",
              "      <td>-0.049143</td>\n",
              "      <td>0.206543</td>\n",
              "      <td>-0.689884</td>\n",
              "      <td>0.505751</td>\n",
              "      <td>-0.306384</td>\n",
              "      <td>-0.464576</td>\n",
              "      <td>0.924472</td>\n",
              "      <td>0.342408</td>\n",
              "      <td>0.543019</td>\n",
              "      <td>0.400223</td>\n",
              "      <td>-0.173314</td>\n",
              "      <td>0.342301</td>\n",
              "      <td>-0.150971</td>\n",
              "      <td>-0.109750</td>\n",
              "      <td>-0.179589</td>\n",
              "      <td>0.230435</td>\n",
              "      <td>0.035653</td>\n",
              "      <td>0.412052</td>\n",
              "      <td>-0.130912</td>\n",
              "      <td>-0.566213</td>\n",
              "      <td>-0.381750</td>\n",
              "      <td>0.798290</td>\n",
              "      <td>0.468334</td>\n",
              "      <td>0.196522</td>\n",
              "      <td>0.037070</td>\n",
              "      <td>0.054352</td>\n",
              "      <td>-0.617227</td>\n",
              "      <td>0.368253</td>\n",
              "      <td>-0.336719</td>\n",
              "      <td>0.157408</td>\n",
              "      <td>0.318117</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136145</td>\n",
              "      <td>-0.158449</td>\n",
              "      <td>0.145648</td>\n",
              "      <td>-0.330110</td>\n",
              "      <td>0.095605</td>\n",
              "      <td>-0.911029</td>\n",
              "      <td>-0.152521</td>\n",
              "      <td>-0.277227</td>\n",
              "      <td>-0.283700</td>\n",
              "      <td>0.005756</td>\n",
              "      <td>-0.165566</td>\n",
              "      <td>0.285174</td>\n",
              "      <td>0.500378</td>\n",
              "      <td>-0.387482</td>\n",
              "      <td>0.691950</td>\n",
              "      <td>-0.060325</td>\n",
              "      <td>-0.550582</td>\n",
              "      <td>0.222612</td>\n",
              "      <td>0.019106</td>\n",
              "      <td>1.069099</td>\n",
              "      <td>-0.530565</td>\n",
              "      <td>0.136196</td>\n",
              "      <td>0.366861</td>\n",
              "      <td>-0.426256</td>\n",
              "      <td>0.226655</td>\n",
              "      <td>0.038184</td>\n",
              "      <td>0.256335</td>\n",
              "      <td>0.157677</td>\n",
              "      <td>-0.489276</td>\n",
              "      <td>-0.687277</td>\n",
              "      <td>-0.630584</td>\n",
              "      <td>-0.016984</td>\n",
              "      <td>-0.350730</td>\n",
              "      <td>-0.172616</td>\n",
              "      <td>0.260950</td>\n",
              "      <td>-0.729053</td>\n",
              "      <td>-0.367372</td>\n",
              "      <td>-0.624519</td>\n",
              "      <td>0.301135</td>\n",
              "      <td>0.294430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7315</th>\n",
              "      <td>0.167208</td>\n",
              "      <td>0.299286</td>\n",
              "      <td>0.491610</td>\n",
              "      <td>-0.444910</td>\n",
              "      <td>-0.113778</td>\n",
              "      <td>-0.170792</td>\n",
              "      <td>0.454244</td>\n",
              "      <td>0.180629</td>\n",
              "      <td>-0.194229</td>\n",
              "      <td>-0.268134</td>\n",
              "      <td>-0.049930</td>\n",
              "      <td>-0.278949</td>\n",
              "      <td>0.090974</td>\n",
              "      <td>0.260803</td>\n",
              "      <td>-0.046972</td>\n",
              "      <td>0.854258</td>\n",
              "      <td>0.046183</td>\n",
              "      <td>0.218709</td>\n",
              "      <td>-0.057021</td>\n",
              "      <td>-0.069000</td>\n",
              "      <td>0.872898</td>\n",
              "      <td>-0.121805</td>\n",
              "      <td>0.250340</td>\n",
              "      <td>0.507785</td>\n",
              "      <td>0.031016</td>\n",
              "      <td>0.500577</td>\n",
              "      <td>0.406960</td>\n",
              "      <td>-0.329940</td>\n",
              "      <td>-0.440141</td>\n",
              "      <td>0.126961</td>\n",
              "      <td>0.761491</td>\n",
              "      <td>0.009389</td>\n",
              "      <td>0.311035</td>\n",
              "      <td>-0.478103</td>\n",
              "      <td>-0.139362</td>\n",
              "      <td>-0.231615</td>\n",
              "      <td>0.203378</td>\n",
              "      <td>-0.122280</td>\n",
              "      <td>0.229558</td>\n",
              "      <td>0.164038</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.222196</td>\n",
              "      <td>-0.372110</td>\n",
              "      <td>-0.287214</td>\n",
              "      <td>-0.214754</td>\n",
              "      <td>-0.091732</td>\n",
              "      <td>-0.311470</td>\n",
              "      <td>0.151199</td>\n",
              "      <td>-0.145242</td>\n",
              "      <td>-0.185514</td>\n",
              "      <td>0.069422</td>\n",
              "      <td>-0.001048</td>\n",
              "      <td>0.263794</td>\n",
              "      <td>0.176082</td>\n",
              "      <td>-0.525270</td>\n",
              "      <td>0.330416</td>\n",
              "      <td>-0.291158</td>\n",
              "      <td>-0.033593</td>\n",
              "      <td>0.270450</td>\n",
              "      <td>0.091955</td>\n",
              "      <td>0.530226</td>\n",
              "      <td>-0.025298</td>\n",
              "      <td>0.296416</td>\n",
              "      <td>0.235689</td>\n",
              "      <td>-0.678318</td>\n",
              "      <td>0.360752</td>\n",
              "      <td>0.281592</td>\n",
              "      <td>-0.151794</td>\n",
              "      <td>-0.625438</td>\n",
              "      <td>-0.146655</td>\n",
              "      <td>0.218224</td>\n",
              "      <td>-0.572947</td>\n",
              "      <td>-0.211265</td>\n",
              "      <td>0.197571</td>\n",
              "      <td>-0.295656</td>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.073510</td>\n",
              "      <td>-0.218864</td>\n",
              "      <td>-0.075352</td>\n",
              "      <td>0.194545</td>\n",
              "      <td>0.245993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7200 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2  ...       765       766       767\n",
              "Unnamed: 0                                ...                              \n",
              "5614       -0.340161 -0.148656  0.602034  ... -0.017511  0.452152  0.481604\n",
              "2383       -0.391934  0.589659  0.081654  ...  0.045654  0.267326  0.322551\n",
              "5448       -0.311391  0.246223  0.322206  ... -0.658689  0.190388  0.197619\n",
              "8412       -0.157567  0.439942  0.159633  ... -0.413154  0.299626  0.228981\n",
              "1448       -0.166418  0.318898  0.330421  ... -0.100879  0.318270 -0.004839\n",
              "...              ...       ...       ...  ...       ...       ...       ...\n",
              "2512       -0.095390  0.449202  0.224358  ... -0.086899  0.236654  0.390968\n",
              "8694       -0.145079  0.475734  0.365407  ...  0.001324  0.549695  0.128526\n",
              "8368       -0.090581  0.432085  0.374877  ...  0.063565  0.301909  0.507903\n",
              "3886       -0.490045  0.403811  0.233433  ... -0.624519  0.301135  0.294430\n",
              "7315        0.167208  0.299286  0.491610  ... -0.075352  0.194545  0.245993\n",
              "\n",
              "[7200 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "outputId": "87fb78ca-c846-47d3-ee3b-5b5a11147d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "outputId": "17046c8c-8df0-47b6-d9ce-9fc65880cf2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "outputId": "9bb8ea94-a7d3-40a8-8d92-f89e3ec60172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TBA: Hyperpameter Tuning Through lr, units & batch; TensorBoard for training process and/or model structure?\n",
        "\n",
        "units = 1024\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "batch = 32\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "c = concatenate([x, c])\n",
        "\n",
        "def FFUnit(c):\n",
        "  x = Dense(units)(c)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  c = concatenate([x, c])\n",
        "  return c\n",
        "\n",
        "for i in range(6):\n",
        "  c = FFUnit(c)\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=batch)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         787456      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024)         4096        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1024)         0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2048)         0           activation_1[0][0]               \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         2098176     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1024)         0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3072)         0           activation_2[0][0]               \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         3146752     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024)         4096        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1024)         0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 4096)         0           activation_3[0][0]               \n",
            "                                                                 concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1024)         0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 1024)         0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5120)         0           activation_4[0][0]               \n",
            "                                                                 concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         5243904     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 1024)         4096        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1024)         0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 1024)         0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 6144)         0           activation_5[0][0]               \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1024)         6292480     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 1024)         4096        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 1024)         0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 1024)         0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 7168)         0           activation_6[0][0]               \n",
            "                                                                 concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         7341056     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 1024)         4096        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1024)         0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 1024)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8192)         0           activation_7[0][0]               \n",
            "                                                                 concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 3)            24579       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 3)            12          dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 3)            0           batch_normalization_8[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 29,158,415\n",
            "Trainable params: 29,144,073\n",
            "Non-trainable params: 14,342\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3488 - acc: 0.9396 - val_loss: 0.2778 - val_acc: 0.9422\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 6s 772us/step - loss: 0.2478 - acc: 0.9656 - val_loss: 0.2351 - val_acc: 0.9756\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 6s 777us/step - loss: 0.2029 - acc: 0.9732 - val_loss: 0.2289 - val_acc: 0.9511\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 5s 757us/step - loss: 0.1750 - acc: 0.9746 - val_loss: 0.1970 - val_acc: 0.9600\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 6s 775us/step - loss: 0.1489 - acc: 0.9800 - val_loss: 0.1519 - val_acc: 0.9656\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1256 - acc: 0.9854 - val_loss: 0.1197 - val_acc: 0.9867\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 6s 773us/step - loss: 0.1106 - acc: 0.9901 - val_loss: 0.1250 - val_acc: 0.9867\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 6s 774us/step - loss: 0.1112 - acc: 0.9888 - val_loss: 0.1111 - val_acc: 0.9856\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 6s 768us/step - loss: 0.1021 - acc: 0.9892 - val_loss: 0.1196 - val_acc: 0.9856\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 6s 769us/step - loss: 0.0986 - acc: 0.9903 - val_loss: 0.1123 - val_acc: 0.9800\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 6s 765us/step - loss: 0.0886 - acc: 0.9931 - val_loss: 0.1063 - val_acc: 0.9822\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 6s 765us/step - loss: 0.0802 - acc: 0.9950 - val_loss: 0.1078 - val_acc: 0.9822\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 5s 752us/step - loss: 0.0832 - acc: 0.9918 - val_loss: 0.1037 - val_acc: 0.9811\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 5s 758us/step - loss: 0.0750 - acc: 0.9949 - val_loss: 0.0992 - val_acc: 0.9856\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 6s 765us/step - loss: 0.0711 - acc: 0.9946 - val_loss: 0.0864 - val_acc: 0.9856\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 5s 759us/step - loss: 0.0721 - acc: 0.9933 - val_loss: 0.0928 - val_acc: 0.9844\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 6s 773us/step - loss: 0.0637 - acc: 0.9946 - val_loss: 0.0837 - val_acc: 0.9867\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 6s 792us/step - loss: 0.0640 - acc: 0.9939 - val_loss: 0.0866 - val_acc: 0.9822\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 6s 782us/step - loss: 0.0545 - acc: 0.9967 - val_loss: 0.0867 - val_acc: 0.9833\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 6s 774us/step - loss: 0.0577 - acc: 0.9944 - val_loss: 0.0837 - val_acc: 0.9822\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 6s 772us/step - loss: 0.0487 - acc: 0.9976 - val_loss: 0.0821 - val_acc: 0.9844\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 6s 790us/step - loss: 0.0452 - acc: 0.9978 - val_loss: 0.0835 - val_acc: 0.9811\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 6s 800us/step - loss: 0.0477 - acc: 0.9962 - val_loss: 0.0716 - val_acc: 0.9856\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 6s 778us/step - loss: 0.0482 - acc: 0.9954 - val_loss: 0.0810 - val_acc: 0.9756\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 6s 776us/step - loss: 0.0397 - acc: 0.9983 - val_loss: 0.0655 - val_acc: 0.9833\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 6s 766us/step - loss: 0.0450 - acc: 0.9958 - val_loss: 0.0791 - val_acc: 0.9811\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 6s 786us/step - loss: 0.0391 - acc: 0.9974 - val_loss: 0.0644 - val_acc: 0.9844\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 6s 789us/step - loss: 0.0389 - acc: 0.9969 - val_loss: 0.0694 - val_acc: 0.9811\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 6s 790us/step - loss: 0.0387 - acc: 0.9960 - val_loss: 0.0735 - val_acc: 0.9822\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 6s 797us/step - loss: 0.0303 - acc: 0.9990 - val_loss: 0.0603 - val_acc: 0.9878\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 6s 828us/step - loss: 0.0305 - acc: 0.9982 - val_loss: 0.0688 - val_acc: 0.9811\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 6s 787us/step - loss: 0.0315 - acc: 0.9975 - val_loss: 0.0578 - val_acc: 0.9878\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 6s 776us/step - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0644 - val_acc: 0.9833\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 6s 784us/step - loss: 0.0255 - acc: 0.9989 - val_loss: 0.0597 - val_acc: 0.9856\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 6s 791us/step - loss: 0.0250 - acc: 0.9986 - val_loss: 0.0780 - val_acc: 0.9800\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 6s 786us/step - loss: 0.0263 - acc: 0.9979 - val_loss: 0.0737 - val_acc: 0.9822\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 6s 804us/step - loss: 0.0256 - acc: 0.9976 - val_loss: 0.0654 - val_acc: 0.9767\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0266 - acc: 0.9986 - val_loss: 0.0574 - val_acc: 0.9867\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 6s 793us/step - loss: 0.0245 - acc: 0.9992 - val_loss: 0.0545 - val_acc: 0.9889\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 6s 769us/step - loss: 0.0257 - acc: 0.9985 - val_loss: 0.0564 - val_acc: 0.9822\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 6s 769us/step - loss: 0.0254 - acc: 0.9985 - val_loss: 0.0730 - val_acc: 0.9800\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 6s 779us/step - loss: 0.0233 - acc: 0.9989 - val_loss: 0.0532 - val_acc: 0.9878\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 5s 763us/step - loss: 0.0250 - acc: 0.9982 - val_loss: 0.0524 - val_acc: 0.9856\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 6s 776us/step - loss: 0.0239 - acc: 0.9985 - val_loss: 0.0509 - val_acc: 0.9856\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 6s 772us/step - loss: 0.0216 - acc: 0.9986 - val_loss: 0.0551 - val_acc: 0.9844\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 6s 769us/step - loss: 0.0199 - acc: 0.9997 - val_loss: 0.0537 - val_acc: 0.9900\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 6s 786us/step - loss: 0.0209 - acc: 0.9989 - val_loss: 0.0520 - val_acc: 0.9900\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 6s 775us/step - loss: 0.0196 - acc: 0.9992 - val_loss: 0.0461 - val_acc: 0.9922\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 6s 785us/step - loss: 0.0186 - acc: 0.9992 - val_loss: 0.0464 - val_acc: 0.9911\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 6s 793us/step - loss: 0.0184 - acc: 0.9992 - val_loss: 0.0456 - val_acc: 0.9867\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 6s 774us/step - loss: 0.0160 - acc: 0.9997 - val_loss: 0.0454 - val_acc: 0.9867\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 5s 762us/step - loss: 0.0165 - acc: 0.9993 - val_loss: 0.0449 - val_acc: 0.9889\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 6s 772us/step - loss: 0.0162 - acc: 0.9993 - val_loss: 0.0496 - val_acc: 0.9844\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 6s 770us/step - loss: 0.0171 - acc: 0.9992 - val_loss: 0.0530 - val_acc: 0.9889\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 6s 779us/step - loss: 0.0161 - acc: 0.9990 - val_loss: 0.0453 - val_acc: 0.9867\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 6s 771us/step - loss: 0.0163 - acc: 0.9993 - val_loss: 0.0468 - val_acc: 0.9900\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 6s 784us/step - loss: 0.0175 - acc: 0.9985 - val_loss: 0.0504 - val_acc: 0.9889\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 107us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.059320129950841266, 0.9822222222222222]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_old_text = pd.DataFrame(X_old_text)\n",
        "X_new_text = pd.DataFrame(X_new_text)\n",
        "\n",
        "Pred_old = model.predict(X_old_text)\n",
        "Pred_new = model.predict(X_new_text)\n",
        "\n",
        "# X1_text = pd.DataFrame(X1_text)\n",
        "# X2_text = pd.DataFrame(X2_text)\n",
        "# X3_text = pd.DataFrame(X3_text)\n",
        "\n",
        "# Pred1 = model.predict(X1_text)[:, 2]\n",
        "# Pred2 = model.predict(X2_text)[:, 2]\n",
        "# Pred3 = model.predict(X3_text)[:, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X1GpzcNR9bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df['Rand_donor_style'] = Pred1\n",
        "# df['Rand_117M_10000_Nabokov_All_3_style'] = Pred2\n",
        "# df['Rand_unigram_style'] = Pred3\n",
        "# df.to_csv('./gdrive/My Drive/DL/Style/rand_examples.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGIYZt1SWc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "pred_old, pred_new, pred_delta = [], [], []\n",
        "for i in range(len(X_old)): \n",
        "  if df.label.iloc[i][2] == 'A':\n",
        "    pred_old.append(Pred_old[i][0])\n",
        "    pred_new.append(Pred_new[i][0])\n",
        "  elif df.label.iloc[i][2] == 'D':\n",
        "    pred_old.append(Pred_old[i][1])\n",
        "    pred_new.append(Pred_new[i][1])\n",
        "  else:\n",
        "    pred_old.append(Pred_old[i][2])\n",
        "    pred_new.append(Pred_new[i][2])\n",
        "  pred_delta.append(pred_new[i] - pred_old[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPspREI2S7_r",
        "colab_type": "code",
        "outputId": "17408b31-e342-48f7-9fe6-f8feb82f9731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_new"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00203248, 0.00404928, 0.9939182 ],\n",
              "       [0.00727922, 0.00358726, 0.98913354],\n",
              "       [0.7721936 , 0.0139337 , 0.2138727 ],\n",
              "       [0.00891649, 0.00448649, 0.986597  ],\n",
              "       [0.1125346 , 0.00660644, 0.8808589 ],\n",
              "       [0.00478178, 0.12722215, 0.86799604],\n",
              "       [0.00129496, 0.00179181, 0.9969132 ],\n",
              "       [0.9057036 , 0.0124633 , 0.08183308],\n",
              "       [0.0044654 , 0.01962554, 0.97590905],\n",
              "       [0.01275224, 0.37084013, 0.61640763],\n",
              "       [0.00131232, 0.00184203, 0.99684566],\n",
              "       [0.00157802, 0.00203662, 0.99638534],\n",
              "       [0.06523793, 0.00728321, 0.9274789 ],\n",
              "       [0.0049958 , 0.01522104, 0.97978324],\n",
              "       [0.00574131, 0.00443773, 0.98982096],\n",
              "       [0.00687255, 0.39102003, 0.60210735],\n",
              "       [0.00370327, 0.02648717, 0.96980953],\n",
              "       [0.4538185 , 0.5200149 , 0.02616665],\n",
              "       [0.00597845, 0.89414316, 0.09987843],\n",
              "       [0.00557098, 0.21227232, 0.7821567 ],\n",
              "       [0.00246911, 0.00275852, 0.9947724 ],\n",
              "       [0.0138345 , 0.00573398, 0.9804315 ],\n",
              "       [0.99641466, 0.00144496, 0.00214028],\n",
              "       [0.99653745, 0.00132717, 0.00213536],\n",
              "       [0.9788658 , 0.01504812, 0.00608605],\n",
              "       [0.9936702 , 0.00241597, 0.0039138 ],\n",
              "       [0.9908352 , 0.00381392, 0.00535089],\n",
              "       [0.99080664, 0.00326382, 0.00592948],\n",
              "       [0.9839093 , 0.00546658, 0.0106241 ],\n",
              "       [0.78060657, 0.01258628, 0.20680715],\n",
              "       [0.98345435, 0.00465805, 0.01188755],\n",
              "       [0.9651136 , 0.00677279, 0.0281136 ],\n",
              "       [0.3471334 , 0.00445799, 0.6484086 ],\n",
              "       [0.9960854 , 0.0016397 , 0.00227486],\n",
              "       [0.9703579 , 0.00355378, 0.02608839],\n",
              "       [0.13482103, 0.01107593, 0.854103  ],\n",
              "       [0.00599374, 0.9904757 , 0.00353051],\n",
              "       [0.00191586, 0.9896975 , 0.00838658],\n",
              "       [0.9291923 , 0.06178744, 0.00902024],\n",
              "       [0.26015538, 0.72951996, 0.01032462],\n",
              "       [0.36211988, 0.6236394 , 0.01424073],\n",
              "       [0.24055268, 0.750287  , 0.00916029],\n",
              "       [0.01278337, 0.9841526 , 0.003064  ],\n",
              "       [0.11760683, 0.8728448 , 0.00954831],\n",
              "       [0.10767117, 0.8812528 , 0.01107599],\n",
              "       [0.00217144, 0.9885666 , 0.00926204],\n",
              "       [0.00230702, 0.9871459 , 0.01054701],\n",
              "       [0.9927698 , 0.00266026, 0.00456993],\n",
              "       [0.00191262, 0.9965049 , 0.00158248],\n",
              "       [0.00338391, 0.99305266, 0.00356337],\n",
              "       [0.0410355 , 0.82982755, 0.129137  ],\n",
              "       [0.09062817, 0.8946161 , 0.01475569],\n",
              "       [0.0458078 , 0.80870193, 0.14549027],\n",
              "       [0.00164791, 0.9965995 , 0.00175255],\n",
              "       [0.00889827, 0.9310496 , 0.06005207]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLh1fA2cVQF",
        "colab_type": "code",
        "outputId": "bec3a638-6d58-457a-f169-f1658630209b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_old"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.7181025e-03, 2.0850466e-03, 9.9619687e-01],\n",
              "       [1.0057823e-03, 1.9193310e-03, 9.9707484e-01],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9726260e-01, 1.2140428e-03, 1.5233783e-03],\n",
              "       [9.7003227e-01, 5.3322855e-03, 2.4635507e-02],\n",
              "       [9.9176776e-01, 3.1889551e-03, 5.0432472e-03],\n",
              "       [9.9118304e-01, 3.5566376e-03, 5.2603288e-03],\n",
              "       [9.9802750e-01, 7.4436510e-04, 1.2280226e-03],\n",
              "       [9.9561691e-01, 1.7827221e-03, 2.6004734e-03],\n",
              "       [9.9689227e-01, 1.2624987e-03, 1.8452296e-03],\n",
              "       [7.7323711e-01, 2.1403381e-01, 1.2729074e-02],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [2.1795547e-03, 9.9624926e-01, 1.5712433e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHeYgeqDcpdP",
        "colab_type": "code",
        "outputId": "242fef9d-04b1-4be2-b45c-1992dee5e275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "pred_delta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0006469997,\n",
              " 0.0062674712,\n",
              " 0.7692856,\n",
              " 0.0075082732,\n",
              " 0.11091142,\n",
              " 0.0035563535,\n",
              " 0.0001149649,\n",
              " 0.8701338,\n",
              " 0.0013317978,\n",
              " 0.010962961,\n",
              " 0.0003645725,\n",
              " 0.0007117655,\n",
              " 0.005052993,\n",
              " 0.013393214,\n",
              " 0.0025015022,\n",
              " 0.38927066,\n",
              " 0.025058856,\n",
              " 0.5102656,\n",
              " 0.88618094,\n",
              " 0.20976229,\n",
              " 0.0006734743,\n",
              " 0.0038146484,\n",
              " -0.00038100418,\n",
              " -0.00039460126,\n",
              " 0.013222156,\n",
              " 0.0006942038,\n",
              " 0.0022851413,\n",
              " 0.0036058815,\n",
              " 0.009100723,\n",
              " 0.18217164,\n",
              " 0.0068443003,\n",
              " 0.02285327,\n",
              " 0.64718056,\n",
              " -0.00032561668,\n",
              " 0.024243161,\n",
              " 0.841374,\n",
              " 0.004301003,\n",
              " 0.00018604274,\n",
              " 0.92801166,\n",
              " 0.2587181,\n",
              " 0.36065552,\n",
              " 0.2392447,\n",
              " 0.010925005,\n",
              " 0.11558467,\n",
              " 0.10364332,\n",
              " 0.0069973934,\n",
              " 0.0052578673,\n",
              " 0.0022290314,\n",
              " 0.00016624876,\n",
              " 0.002378278,\n",
              " 0.1277139,\n",
              " 0.012937957,\n",
              " 0.1438997,\n",
              " -0.0009582578,\n",
              " 0.05848083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0byXlehgT07U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/arrow_plot_results.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['donor_authorship_score'] = pred_old\n",
        "df['style_authorship_score'] = pred_new\n",
        "df['authorship_delta'] = pred_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWN5M4qT74-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.to_csv('./gdrive/My Drive/DL/Style/all_scores.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miIRriOiJpQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For old examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KxX5ljnUVgL",
        "colab_type": "code",
        "outputId": "aaa462c2-521a-4620-8ec2-0378b62bebb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# Binarize the multiclass labels\n",
        "Y_bi = label_binarize(y, classes=['Austen', 'Dumas', 'Nabokov'])\n",
        "n_classes = Y_bi.shape[1]\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "lw = 2\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve for each class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAG5CAYAAAD2yo9EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNK3E0oOgpmmaVmLpQVDTNE0rsf4fu7jd/xC0L7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bvUsLAzabQU",
        "colab_type": "code",
        "outputId": "90448345-c52f-459a-ce6f-66c6db75d6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/all_scores.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>old</th>\n",
              "      <th>new</th>\n",
              "      <th>old_masked</th>\n",
              "      <th>new_masked</th>\n",
              "      <th>label</th>\n",
              "      <th>donor_naturalness_score</th>\n",
              "      <th>style_naturalness_score</th>\n",
              "      <th>naturalness_delta</th>\n",
              "      <th>donor_authorship_score</th>\n",
              "      <th>style_authorship_score</th>\n",
              "      <th>authorship_delta</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, to look at it, and his fem...</td>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, to look at it, and his fe...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.990052</td>\n",
              "      <td>0.987915</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>0.001385</td>\n",
              "      <td>0.002032</td>\n",
              "      <td>0.000647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pal Palych! How’re they biting?” He looked up...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked ...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.960177</td>\n",
              "      <td>0.992160</td>\n",
              "      <td>-0.031982</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.007279</td>\n",
              "      <td>0.006267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony above...</td>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony abov...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.986264</td>\n",
              "      <td>0.994932</td>\n",
              "      <td>-0.008668</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>0.772194</td>\n",
              "      <td>0.769286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter ...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.959302</td>\n",
              "      <td>0.990870</td>\n",
              "      <td>-0.031567</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.007508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not a Portrait of Lolita: the differences bet...</td>\n",
              "      <td>not a Portrait of Lolita: the differences betw...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.972211</td>\n",
              "      <td>0.994189</td>\n",
              "      <td>-0.021978</td>\n",
              "      <td>0.001623</td>\n",
              "      <td>0.112535</td>\n",
              "      <td>0.110911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoons...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.992910</td>\n",
              "      <td>0.992968</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.004782</td>\n",
              "      <td>0.003556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on ...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.975876</td>\n",
              "      <td>0.994505</td>\n",
              "      <td>-0.018629</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.000115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. A...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035570</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>0.870134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Ministe...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.967726</td>\n",
              "      <td>0.315845</td>\n",
              "      <td>0.651881</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>0.004465</td>\n",
              "      <td>0.001332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my sternum, an...</td>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my sternum, a...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.942770</td>\n",
              "      <td>0.995724</td>\n",
              "      <td>-0.052954</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.012752</td>\n",
              "      <td>0.010963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, with a sigh, and his femin...</td>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, with a sigh, and his femi...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.990052</td>\n",
              "      <td>0.877143</td>\n",
              "      <td>0.112909</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.000365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Pal Palych! How’re they biting?” He looked up...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked ...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.960177</td>\n",
              "      <td>0.986106</td>\n",
              "      <td>-0.025929</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.000712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony, and ...</td>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony, and...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.986264</td>\n",
              "      <td>0.991506</td>\n",
              "      <td>-0.005242</td>\n",
              "      <td>0.002230</td>\n",
              "      <td>0.007283</td>\n",
              "      <td>0.005053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter ...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.959302</td>\n",
              "      <td>0.986418</td>\n",
              "      <td>-0.027116</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.015221</td>\n",
              "      <td>0.013393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>not a Portrait of Lolita: the differences bet...</td>\n",
              "      <td>not a Portrait of Lolita: the differences betw...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.972211</td>\n",
              "      <td>0.993964</td>\n",
              "      <td>-0.021753</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.004438</td>\n",
              "      <td>0.002502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoons...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.992910</td>\n",
              "      <td>0.994314</td>\n",
              "      <td>-0.001403</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.391020</td>\n",
              "      <td>0.389271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. The king walked r...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. The king walked ...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.975876</td>\n",
              "      <td>0.968510</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.026487</td>\n",
              "      <td>0.025059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. A...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009749</td>\n",
              "      <td>0.520015</td>\n",
              "      <td>0.510266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Ministe...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.967726</td>\n",
              "      <td>0.884055</td>\n",
              "      <td>0.083671</td>\n",
              "      <td>0.007962</td>\n",
              "      <td>0.894143</td>\n",
              "      <td>0.886181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my legs stiffe...</td>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my legs stiff...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.942770</td>\n",
              "      <td>0.820391</td>\n",
              "      <td>0.122379</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.212272</td>\n",
              "      <td>0.209762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>time it slipped off) while you stand with outs...</td>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.994646</td>\n",
              "      <td>0.994646</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>0.002759</td>\n",
              "      <td>0.000673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>her finger on the switch—a half-interrogative,...</td>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.979933</td>\n",
              "      <td>0.962131</td>\n",
              "      <td>0.017801</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.005734</td>\n",
              "      <td>0.003815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2D1</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>-0.000381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. Two of her girls had been upon the roa...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. Two of her girls had been upon the ro...</td>\n",
              "      <td>A2D1</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.976373</td>\n",
              "      <td>-0.069334</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>-0.000395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2D2</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.977014</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.015048</td>\n",
              "      <td>0.013222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. A little while after she rose, dressed...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. A little while after she rose, dresse...</td>\n",
              "      <td>A2D2</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.994970</td>\n",
              "      <td>-0.087931</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.000694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.993180</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.003066</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.002285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. The other girl, Number Two, walked bri...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. The other girl, Number Two, walked br...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.995685</td>\n",
              "      <td>-0.088646</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>0.003606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>whose arrival was often as sudden, if not qui...</td>\n",
              "      <td>whose arrival was often as sudden, and as usua...</td>\n",
              "      <td>whose arrival was often as sudden, if not qui...</td>\n",
              "      <td>whose arrival was often as sudden, and as usu...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.991857</td>\n",
              "      <td>0.995415</td>\n",
              "      <td>-0.003558</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>0.010624</td>\n",
              "      <td>0.009101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Lydia's society she was of course carefully k...</td>\n",
              "      <td>Lydia's society she was of course carefully ke...</td>\n",
              "      <td>&lt;MASK&gt; 's society she was of course carefully...</td>\n",
              "      <td>&lt;MASK&gt; 's society she was of course carefully...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.995624</td>\n",
              "      <td>0.954530</td>\n",
              "      <td>0.041094</td>\n",
              "      <td>0.024636</td>\n",
              "      <td>0.206807</td>\n",
              "      <td>0.182172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>and positively declared, that he would still ...</td>\n",
              "      <td>and positively declared, �I�m going to love hi...</td>\n",
              "      <td>and positively declared, that he would still ...</td>\n",
              "      <td>and positively declared, �I�m going to love h...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.986645</td>\n",
              "      <td>0.955920</td>\n",
              "      <td>0.030725</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.011888</td>\n",
              "      <td>0.006844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>her, she might not have persuaded herself into...</td>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.989389</td>\n",
              "      <td>0.992236</td>\n",
              "      <td>-0.002847</td>\n",
              "      <td>0.005260</td>\n",
              "      <td>0.028114</td>\n",
              "      <td>0.022853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>by Fanny.  “I am very sorry,” said she; “it i...</td>\n",
              "      <td>by Fanny. �I�m terribly sorry, � said she; �it...</td>\n",
              "      <td>by &lt;MASK&gt;. “ I am very sorry, ” said she; “ i...</td>\n",
              "      <td>by &lt;MASK&gt;. �I�m terribly sorry, � said she; �...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.972956</td>\n",
              "      <td>0.995057</td>\n",
              "      <td>-0.022101</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.648409</td>\n",
              "      <td>0.647181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>daughters might do as they pleased.  But they...</td>\n",
              "      <td>daughters might do as they pleased. But they h...</td>\n",
              "      <td>daughters might do as they pleased. But they ...</td>\n",
              "      <td>daughters might do as they pleased. But they ...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.991757</td>\n",
              "      <td>0.991757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>-0.000326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>of _planning_ judged of by the day at Sotherto...</td>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.983739</td>\n",
              "      <td>0.679974</td>\n",
              "      <td>0.303764</td>\n",
              "      <td>0.001845</td>\n",
              "      <td>0.026088</td>\n",
              "      <td>0.024243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>plainly that it ended no better than it began....</td>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.996398</td>\n",
              "      <td>0.983931</td>\n",
              "      <td>0.012467</td>\n",
              "      <td>0.012729</td>\n",
              "      <td>0.854103</td>\n",
              "      <td>0.841374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>\"What! arrest you, my most faithful servant?\"...</td>\n",
              "      <td>What! If the officer is of the line, my most f...</td>\n",
              "      <td>`` What! arrest you, my most faithful servant...</td>\n",
              "      <td>What! If the officer is of the line, my most ...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.244483</td>\n",
              "      <td>0.992559</td>\n",
              "      <td>-0.748076</td>\n",
              "      <td>0.001693</td>\n",
              "      <td>0.005994</td>\n",
              "      <td>0.004301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>said D’Artagnan, “but as he is a thoughtful y...</td>\n",
              "      <td>said D ’ Artagnan, he is a very capable young ...</td>\n",
              "      <td>said D ’ &lt;MASK&gt;, “ but as he is a thoughtful ...</td>\n",
              "      <td>said &lt;MASK&gt; &lt;MASK&gt; &lt;MASK&gt;, he is a very capab...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.966850</td>\n",
              "      <td>-0.002492</td>\n",
              "      <td>0.001730</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.000186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>must be married in order to be respected.”  “...</td>\n",
              "      <td>must be married in order to be respected. So y...</td>\n",
              "      <td>must be married in order to be respected. ” “...</td>\n",
              "      <td>must be married in order to be respected. So ...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.985592</td>\n",
              "      <td>0.993826</td>\n",
              "      <td>-0.008234</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.929192</td>\n",
              "      <td>0.928012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and something more, had arranged the...</td>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and something more, had arranged th...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.960746</td>\n",
              "      <td>0.994518</td>\n",
              "      <td>-0.033772</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.260155</td>\n",
              "      <td>0.258718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why, ...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.996894</td>\n",
              "      <td>0.989974</td>\n",
              "      <td>0.006920</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.362120</td>\n",
              "      <td>0.360656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>you evaded giving me an answer.”  “And what r...</td>\n",
              "      <td>you evaded giving me an answer. ” “I cannot im...</td>\n",
              "      <td>you evaded giving me an answer. ” “ And what ...</td>\n",
              "      <td>you evaded giving me an answer. ” “ I can not...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.978038</td>\n",
              "      <td>0.998955</td>\n",
              "      <td>-0.020918</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.240553</td>\n",
              "      <td>0.239245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>\"Therefore no one inhabits it; only, you see...</td>\n",
              "      <td>Therefore no one inhabits it; I think I see no...</td>\n",
              "      <td>`` Therefore no one inhabits it; only, you se...</td>\n",
              "      <td>Therefore no one inhabits it; I think I see n...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.998624</td>\n",
              "      <td>0.955227</td>\n",
              "      <td>0.043397</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>0.010925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>while in Paris?”  “Arrest the Duke! Arrest th...</td>\n",
              "      <td>while in Paris? ” “Arrest the Duke! No, I shou...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! Arrest t...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! No, I sh...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.285652</td>\n",
              "      <td>0.808030</td>\n",
              "      <td>-0.522378</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.117607</td>\n",
              "      <td>0.115585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>need their assistance.  Meanwhile, carried aw...</td>\n",
              "      <td>need their assistance. And so, upon the subjec...</td>\n",
              "      <td>need their assistance. Meanwhile, carried awa...</td>\n",
              "      <td>need their assistance. And so, upon the subje...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.980184</td>\n",
              "      <td>0.990127</td>\n",
              "      <td>-0.009943</td>\n",
              "      <td>0.004028</td>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.103643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>\"What! arrest you, my most faithful servant?\"...</td>\n",
              "      <td>What! You�ll arrest me, my most faithful serva...</td>\n",
              "      <td>`` What! arrest you, my most faithful servant...</td>\n",
              "      <td>What! You�ll arrest me, my most faithful serv...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.244483</td>\n",
              "      <td>0.143257</td>\n",
              "      <td>0.101226</td>\n",
              "      <td>0.002265</td>\n",
              "      <td>0.009262</td>\n",
              "      <td>0.006997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>said D’Artagnan, “but as he is a thoughtful y...</td>\n",
              "      <td>said D � Artagnan, �but as he is a thoughtful ...</td>\n",
              "      <td>said D ’ &lt;MASK&gt;, “ but as he is a thoughtful ...</td>\n",
              "      <td>said &lt;MASK&gt; &lt;MASK&gt; &lt;MASK&gt;, �but as he is a th...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005289</td>\n",
              "      <td>0.010547</td>\n",
              "      <td>0.005258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>must be married in order to be respected.”  “...</td>\n",
              "      <td>must be married in order to be respected. � �F...</td>\n",
              "      <td>must be married in order to be respected. ” “...</td>\n",
              "      <td>must be married in order to be respected. � �...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.985592</td>\n",
              "      <td>0.991795</td>\n",
              "      <td>-0.006203</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.004570</td>\n",
              "      <td>0.002229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and that is all, had arranged the co...</td>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and that is all, had arranged the c...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.960746</td>\n",
              "      <td>0.977840</td>\n",
              "      <td>-0.017094</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.000166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why, ...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.996895</td>\n",
              "      <td>0.996386</td>\n",
              "      <td>0.000508</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>0.003563</td>\n",
              "      <td>0.002378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>you evaded giving me an answer.”  “And what r...</td>\n",
              "      <td>you evaded giving me an answer. � �Why did you...</td>\n",
              "      <td>you evaded giving me an answer. ” “ And what ...</td>\n",
              "      <td>you evaded giving me an answer. � �Why did yo...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.978038</td>\n",
              "      <td>0.878864</td>\n",
              "      <td>0.099174</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.129137</td>\n",
              "      <td>0.127714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>\"Therefore no one inhabits it; only, you see...</td>\n",
              "      <td>Therefore no one inhabits it; I, as you, know ...</td>\n",
              "      <td>`` Therefore no one inhabits it; only, you se...</td>\n",
              "      <td>Therefore no one inhabits it; I, as you, know...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.998624</td>\n",
              "      <td>0.957601</td>\n",
              "      <td>0.041023</td>\n",
              "      <td>0.001818</td>\n",
              "      <td>0.014756</td>\n",
              "      <td>0.012938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>while in Paris?”  “Arrest the Duke! Arrest th...</td>\n",
              "      <td>while in Paris? � �Arrest the Duke! � at the D...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! Arrest t...</td>\n",
              "      <td>while in Paris? � �Arrest the Duke! � at the ...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.285652</td>\n",
              "      <td>0.993423</td>\n",
              "      <td>-0.707771</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.145490</td>\n",
              "      <td>0.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>need their assistance.  Meanwhile, carried aw...</td>\n",
              "      <td>need their assistance. We know the way our nar...</td>\n",
              "      <td>need their assistance. Meanwhile, carried awa...</td>\n",
              "      <td>need their assistance. We know the way our na...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.980184</td>\n",
              "      <td>0.983349</td>\n",
              "      <td>-0.003164</td>\n",
              "      <td>0.002711</td>\n",
              "      <td>0.001753</td>\n",
              "      <td>-0.000958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>the alarm, the light was extinguished, the la...</td>\n",
              "      <td>the alarm, and the light went out, the ladder ...</td>\n",
              "      <td>the alarm, the light was extinguished, the la...</td>\n",
              "      <td>the alarm, and the light went out, the ladder...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.992641</td>\n",
              "      <td>0.984692</td>\n",
              "      <td>0.007949</td>\n",
              "      <td>0.001571</td>\n",
              "      <td>0.060052</td>\n",
              "      <td>0.058481</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          old  ... authorship_delta\n",
              "Unnamed: 0                                                     ...                 \n",
              "0            stepped back a bit, as if admiring it, and hi...  ...         0.000647\n",
              "1            Pal Palych! How’re they biting?” He looked up...  ...         0.006267\n",
              "2            but at least it revealed a small balcony one ...  ...         0.769286\n",
              "3            had not been placed together, told the waiter...  ...         0.007508\n",
              "4            not a Portrait of Lolita: the differences bet...  ...         0.110911\n",
              "5            his right, the way Anglo-Saxons do in cartoon...  ...         0.003556\n",
              "6            in a gigantic, joyous world. A tall pillar on...  ...         0.000115\n",
              "7            traversed by the black bend-let of a branch. ...  ...         0.870134\n",
              "8            cross the frontier. One of the Cabinet Minist...  ...         0.001332\n",
              "9            lawn, but within my thorax, and my organs swa...  ...         0.010963\n",
              "10           stepped back a bit, as if admiring it, and hi...  ...         0.000365\n",
              "11           Pal Palych! How’re they biting?” He looked up...  ...         0.000712\n",
              "12           but at least it revealed a small balcony one ...  ...         0.005053\n",
              "13           had not been placed together, told the waiter...  ...         0.013393\n",
              "14           not a Portrait of Lolita: the differences bet...  ...         0.002502\n",
              "15           his right, the way Anglo-Saxons do in cartoon...  ...         0.389271\n",
              "16           in a gigantic, joyous world. A tall pillar on...  ...         0.025059\n",
              "17           traversed by the black bend-let of a branch. ...  ...         0.510266\n",
              "18           cross the frontier. One of the Cabinet Minist...  ...         0.886181\n",
              "19           lawn, but within my thorax, and my organs swa...  ...         0.209762\n",
              "20           time it slipped off) while you stand with out...  ...         0.000673\n",
              "21           her finger on the switch—a half-interrogative...  ...         0.003815\n",
              "22           of Jane Fairfax! The interest he takes in her...  ...        -0.000381\n",
              "23           sister. Two of her girls had been upon the po...  ...        -0.000395\n",
              "24           of Jane Fairfax! The interest he takes in her...  ...         0.013222\n",
              "25           sister. Two of her girls had been upon the po...  ...         0.000694\n",
              "26           of Jane Fairfax! The interest he takes in her...  ...         0.002285\n",
              "27           sister. Two of her girls had been upon the po...  ...         0.003606\n",
              "28           whose arrival was often as sudden, if not qui...  ...         0.009101\n",
              "29           Lydia's society she was of course carefully k...  ...         0.182172\n",
              "30           and positively declared, that he would still ...  ...         0.006844\n",
              "31           her, she might not have persuaded herself int...  ...         0.022853\n",
              "32           by Fanny.  “I am very sorry,” said she; “it i...  ...         0.647181\n",
              "33           daughters might do as they pleased.  But they...  ...        -0.000326\n",
              "34           of _planning_ judged of by the day at Sothert...  ...         0.024243\n",
              "35           plainly that it ended no better than it began...  ...         0.841374\n",
              "36           \"What! arrest you, my most faithful servant?\"...  ...         0.004301\n",
              "37           said D’Artagnan, “but as he is a thoughtful y...  ...         0.000186\n",
              "38           must be married in order to be respected.”  “...  ...         0.928012\n",
              "39           of honor, and with all its accompanying detai...  ...         0.258718\n",
              "40           they so continuously occupied about her? Why,...  ...         0.360656\n",
              "41           you evaded giving me an answer.”  “And what r...  ...         0.239245\n",
              "42            \"Therefore no one inhabits it; only, you see...  ...         0.010925\n",
              "43           while in Paris?”  “Arrest the Duke! Arrest th...  ...         0.115585\n",
              "44           need their assistance.  Meanwhile, carried aw...  ...         0.103643\n",
              "45           \"What! arrest you, my most faithful servant?\"...  ...         0.006997\n",
              "46           said D’Artagnan, “but as he is a thoughtful y...  ...         0.005258\n",
              "47           must be married in order to be respected.”  “...  ...         0.002229\n",
              "48           of honor, and with all its accompanying detai...  ...         0.000166\n",
              "49           they so continuously occupied about her? Why,...  ...         0.002378\n",
              "50           you evaded giving me an answer.”  “And what r...  ...         0.127714\n",
              "51            \"Therefore no one inhabits it; only, you see...  ...         0.012938\n",
              "52           while in Paris?”  “Arrest the Duke! Arrest th...  ...         0.143900\n",
              "53           need their assistance.  Meanwhile, carried aw...  ...        -0.000958\n",
              "54           the alarm, the light was extinguished, the la...  ...         0.058481\n",
              "\n",
              "[55 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYs6lLFID3R",
        "colab_type": "code",
        "outputId": "f842f7bd-16e7-4e25-ebf1-dd226a7f77c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['style_naturalness_score'].values\n",
        "\n",
        "u = df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "#plt.axis('equal')\n",
        "plt.quiver(x,u,y,v)\n",
        "plt.xlabel('Naturalness')\n",
        "plt.ylabel('Style Shift')\n",
        "plt.xlim(left=0.89, right=1.01)\n",
        "plt.ylim(top=0.04, bottom=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEQCAYAAAB80zltAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0us2zadbdKG",
        "colab_type": "code",
        "outputId": "b6d2b023-7bcf-4d07-9e53-a7c9e93a3fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['donor_authorship_score'].values\n",
        "\n",
        "u = - df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "Arrows = plt.figure()\n",
        "for i, j, k, l in zip(x, y, u, v): \n",
        "  plt.arrow(i, j, k, l, head_width=0.02, head_length=0.04)\n",
        "plt.xlabel('Naturalness')\n",
        "plt.ylabel('Authorship')\n",
        "plt.xlim(left=0.1, right=1.05)\n",
        "plt.ylim(top=1, bottom=-0.1)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "plt.savefig('Arrows.pdf')\n",
        "files.download('Arrows.pdf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEQCAYAAABFtIg2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8yKRNf6C5eK",
        "colab_type": "code",
        "outputId": "a951cf2d-df1f-4ef5-e249-fac5eae68fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "plt.savefig(\"abc.png\")\n",
        "files.download(\"abc.png\") \n",
        "\n",
        "plt.savefig(('./gdrive/My Drive/DL/Style/arrows.pdf'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP-xHdpvHhBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}