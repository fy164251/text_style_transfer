{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert_masked.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "fecd384b-ac4e-4fed-fd55-9b628957799e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 3.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.7MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=4e8b64ce1ed71c93648002082680e82a5a861315c51d6a14f356b3eaacc84ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, regex, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.2)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "outputId": "9447dc55-dab1-4c83-9978-497c55bf4287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, without masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "# lbl_enc = preprocessing.LabelEncoder()\n",
        "# y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkqPiy6v2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, with masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "outputId": "203f0dfc-d526-4d0c-8d83-7864dbf965ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = Embeddings()\n",
        "\n",
        "X_text = []\n",
        "for sentence in tqdm(X):\n",
        "    X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2670653.60B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 76004.00B/s]\n",
            "100%|██████████| 440473133/440473133 [00:08<00:00, 50072121.61B/s]\n",
            "100%|██████████| 9000/9000 [1:19:40<00:00,  1.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQfIbHooxmk",
        "colab_type": "code",
        "outputId": "bba22d5d-f671-4bce-eba0-a3b471467736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# For Test Time\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "X_old = df.old.astype('str')\n",
        "X_new = df.new.astype('str')\n",
        "\n",
        "model = Embeddings()\n",
        "\n",
        "X_old_text, X_new_text = [], []\n",
        "for sentence in tqdm(X_old):\n",
        "    X_old_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "for sentence in tqdm(X_new):\n",
        "    X_new_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 55/55 [00:30<00:00,  1.72it/s]\n",
            "100%|██████████| 55/55 [00:34<00:00,  1.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_YrIamp5Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_old = pd.DataFrame(X_old_text)\n",
        "# X_old.to_csv('./gdrive/My Drive/DL/Style/X_old_Embedding.csv')\n",
        "\n",
        "# X_new = pd.DataFrame(X_new_text)\n",
        "# X_new.to_csv('./gdrive/My Drive/DL/Style/X_new_Embedding.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplxzc29EEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQlnt96NhI9",
        "colab_type": "text"
      },
      "source": [
        "### Building A Atyle Alassifier on Top of DistilBert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "ccc23ed1-9f0a-4fde-c139-b290b449f1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUmYQdKgdUq",
        "colab_type": "code",
        "outputId": "4c48addf-fd8c-4cda-faa7-ff3505b01759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5614</th>\n",
              "      <td>-0.340161</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>0.602034</td>\n",
              "      <td>-0.272003</td>\n",
              "      <td>-0.341873</td>\n",
              "      <td>-0.120357</td>\n",
              "      <td>0.224259</td>\n",
              "      <td>0.206777</td>\n",
              "      <td>-0.050708</td>\n",
              "      <td>-0.413362</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.422804</td>\n",
              "      <td>-0.132413</td>\n",
              "      <td>0.146387</td>\n",
              "      <td>-0.105816</td>\n",
              "      <td>0.753637</td>\n",
              "      <td>0.414343</td>\n",
              "      <td>0.122540</td>\n",
              "      <td>0.179269</td>\n",
              "      <td>0.088668</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>-0.148916</td>\n",
              "      <td>0.095241</td>\n",
              "      <td>0.706351</td>\n",
              "      <td>0.180122</td>\n",
              "      <td>0.449534</td>\n",
              "      <td>0.147201</td>\n",
              "      <td>-0.237596</td>\n",
              "      <td>-0.151617</td>\n",
              "      <td>-0.037453</td>\n",
              "      <td>0.579897</td>\n",
              "      <td>0.074145</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>-0.265257</td>\n",
              "      <td>-0.118240</td>\n",
              "      <td>-0.280390</td>\n",
              "      <td>0.099969</td>\n",
              "      <td>-0.501312</td>\n",
              "      <td>0.068412</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.236783</td>\n",
              "      <td>-0.722360</td>\n",
              "      <td>-0.142746</td>\n",
              "      <td>-0.070288</td>\n",
              "      <td>-0.142651</td>\n",
              "      <td>-0.857589</td>\n",
              "      <td>-0.207489</td>\n",
              "      <td>0.172034</td>\n",
              "      <td>-0.133808</td>\n",
              "      <td>-0.344236</td>\n",
              "      <td>-0.061278</td>\n",
              "      <td>0.064547</td>\n",
              "      <td>0.122214</td>\n",
              "      <td>-0.097235</td>\n",
              "      <td>0.183206</td>\n",
              "      <td>-0.092817</td>\n",
              "      <td>-0.457735</td>\n",
              "      <td>0.098827</td>\n",
              "      <td>-0.223504</td>\n",
              "      <td>0.734053</td>\n",
              "      <td>-0.446106</td>\n",
              "      <td>0.604875</td>\n",
              "      <td>0.058997</td>\n",
              "      <td>-0.558629</td>\n",
              "      <td>-0.211270</td>\n",
              "      <td>-0.087967</td>\n",
              "      <td>-0.125183</td>\n",
              "      <td>-0.588484</td>\n",
              "      <td>-0.453498</td>\n",
              "      <td>-0.176318</td>\n",
              "      <td>-0.628320</td>\n",
              "      <td>0.045665</td>\n",
              "      <td>0.183669</td>\n",
              "      <td>-0.280129</td>\n",
              "      <td>-0.195073</td>\n",
              "      <td>-0.313454</td>\n",
              "      <td>-0.230915</td>\n",
              "      <td>-0.017511</td>\n",
              "      <td>0.452152</td>\n",
              "      <td>0.481604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2383</th>\n",
              "      <td>-0.391934</td>\n",
              "      <td>0.589659</td>\n",
              "      <td>0.081654</td>\n",
              "      <td>-0.364856</td>\n",
              "      <td>0.272116</td>\n",
              "      <td>-0.185212</td>\n",
              "      <td>-0.179081</td>\n",
              "      <td>0.248599</td>\n",
              "      <td>-0.116149</td>\n",
              "      <td>-0.085957</td>\n",
              "      <td>0.279195</td>\n",
              "      <td>-0.495050</td>\n",
              "      <td>-0.024453</td>\n",
              "      <td>0.470089</td>\n",
              "      <td>0.149684</td>\n",
              "      <td>1.075918</td>\n",
              "      <td>0.142027</td>\n",
              "      <td>-0.185910</td>\n",
              "      <td>0.287008</td>\n",
              "      <td>0.135054</td>\n",
              "      <td>0.798489</td>\n",
              "      <td>-0.099341</td>\n",
              "      <td>0.331279</td>\n",
              "      <td>0.542002</td>\n",
              "      <td>0.268069</td>\n",
              "      <td>0.279488</td>\n",
              "      <td>0.068858</td>\n",
              "      <td>-0.253427</td>\n",
              "      <td>-0.456926</td>\n",
              "      <td>0.170671</td>\n",
              "      <td>0.415524</td>\n",
              "      <td>-0.215536</td>\n",
              "      <td>-0.112373</td>\n",
              "      <td>-0.604337</td>\n",
              "      <td>0.137619</td>\n",
              "      <td>-0.606019</td>\n",
              "      <td>0.378203</td>\n",
              "      <td>-0.173980</td>\n",
              "      <td>0.747436</td>\n",
              "      <td>0.213383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.194351</td>\n",
              "      <td>-0.251167</td>\n",
              "      <td>-0.103786</td>\n",
              "      <td>-0.614198</td>\n",
              "      <td>-0.427270</td>\n",
              "      <td>-0.536838</td>\n",
              "      <td>-0.142121</td>\n",
              "      <td>0.175018</td>\n",
              "      <td>0.257659</td>\n",
              "      <td>0.063984</td>\n",
              "      <td>0.204867</td>\n",
              "      <td>0.264982</td>\n",
              "      <td>0.090459</td>\n",
              "      <td>-0.156936</td>\n",
              "      <td>0.328172</td>\n",
              "      <td>-0.267422</td>\n",
              "      <td>0.004332</td>\n",
              "      <td>0.437211</td>\n",
              "      <td>0.271880</td>\n",
              "      <td>0.837500</td>\n",
              "      <td>-0.222474</td>\n",
              "      <td>0.509900</td>\n",
              "      <td>-0.052957</td>\n",
              "      <td>-0.122060</td>\n",
              "      <td>0.306961</td>\n",
              "      <td>0.155461</td>\n",
              "      <td>0.102520</td>\n",
              "      <td>-0.191638</td>\n",
              "      <td>-0.222842</td>\n",
              "      <td>-0.347603</td>\n",
              "      <td>-0.569109</td>\n",
              "      <td>0.100797</td>\n",
              "      <td>-0.224095</td>\n",
              "      <td>0.094102</td>\n",
              "      <td>0.375749</td>\n",
              "      <td>0.166803</td>\n",
              "      <td>-0.509638</td>\n",
              "      <td>0.045654</td>\n",
              "      <td>0.267326</td>\n",
              "      <td>0.322551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>-0.311391</td>\n",
              "      <td>0.246223</td>\n",
              "      <td>0.322206</td>\n",
              "      <td>-0.626673</td>\n",
              "      <td>-0.120535</td>\n",
              "      <td>-0.328576</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>0.150335</td>\n",
              "      <td>-0.228633</td>\n",
              "      <td>-0.119519</td>\n",
              "      <td>0.234018</td>\n",
              "      <td>-0.455078</td>\n",
              "      <td>0.094305</td>\n",
              "      <td>0.039104</td>\n",
              "      <td>-0.354940</td>\n",
              "      <td>1.053825</td>\n",
              "      <td>0.437839</td>\n",
              "      <td>0.352679</td>\n",
              "      <td>0.163303</td>\n",
              "      <td>-0.222175</td>\n",
              "      <td>0.575262</td>\n",
              "      <td>-0.151778</td>\n",
              "      <td>0.039601</td>\n",
              "      <td>0.237160</td>\n",
              "      <td>0.136593</td>\n",
              "      <td>0.441844</td>\n",
              "      <td>0.216453</td>\n",
              "      <td>-0.411612</td>\n",
              "      <td>-0.499689</td>\n",
              "      <td>-0.125523</td>\n",
              "      <td>0.927454</td>\n",
              "      <td>-0.064041</td>\n",
              "      <td>0.280551</td>\n",
              "      <td>0.071708</td>\n",
              "      <td>-0.018094</td>\n",
              "      <td>-0.288303</td>\n",
              "      <td>0.428391</td>\n",
              "      <td>-0.406295</td>\n",
              "      <td>0.066329</td>\n",
              "      <td>0.261219</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063150</td>\n",
              "      <td>-0.204630</td>\n",
              "      <td>-0.137450</td>\n",
              "      <td>-0.170344</td>\n",
              "      <td>-0.314548</td>\n",
              "      <td>-0.759352</td>\n",
              "      <td>-0.347153</td>\n",
              "      <td>-0.266005</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0.089698</td>\n",
              "      <td>-0.136013</td>\n",
              "      <td>0.242091</td>\n",
              "      <td>0.530148</td>\n",
              "      <td>-0.121507</td>\n",
              "      <td>0.408584</td>\n",
              "      <td>-0.177793</td>\n",
              "      <td>-0.448586</td>\n",
              "      <td>-0.175234</td>\n",
              "      <td>0.130837</td>\n",
              "      <td>0.919979</td>\n",
              "      <td>-0.481680</td>\n",
              "      <td>0.259219</td>\n",
              "      <td>0.216071</td>\n",
              "      <td>-0.403066</td>\n",
              "      <td>-0.032478</td>\n",
              "      <td>0.047672</td>\n",
              "      <td>0.106018</td>\n",
              "      <td>-0.227465</td>\n",
              "      <td>-0.409935</td>\n",
              "      <td>-0.288559</td>\n",
              "      <td>-0.625146</td>\n",
              "      <td>0.146311</td>\n",
              "      <td>0.043609</td>\n",
              "      <td>0.026973</td>\n",
              "      <td>0.094333</td>\n",
              "      <td>-0.698623</td>\n",
              "      <td>-0.382027</td>\n",
              "      <td>-0.658689</td>\n",
              "      <td>0.190388</td>\n",
              "      <td>0.197619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8412</th>\n",
              "      <td>-0.157567</td>\n",
              "      <td>0.439942</td>\n",
              "      <td>0.159633</td>\n",
              "      <td>-0.716131</td>\n",
              "      <td>0.394165</td>\n",
              "      <td>-0.170059</td>\n",
              "      <td>0.290004</td>\n",
              "      <td>0.776767</td>\n",
              "      <td>-0.385152</td>\n",
              "      <td>-0.022196</td>\n",
              "      <td>0.137164</td>\n",
              "      <td>-0.146209</td>\n",
              "      <td>0.309307</td>\n",
              "      <td>-0.086701</td>\n",
              "      <td>-0.312970</td>\n",
              "      <td>0.535559</td>\n",
              "      <td>0.378450</td>\n",
              "      <td>0.155951</td>\n",
              "      <td>0.159643</td>\n",
              "      <td>-0.078866</td>\n",
              "      <td>0.229330</td>\n",
              "      <td>-0.042130</td>\n",
              "      <td>-0.046688</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>-0.052326</td>\n",
              "      <td>0.334858</td>\n",
              "      <td>0.294528</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>-0.752542</td>\n",
              "      <td>0.043778</td>\n",
              "      <td>0.562868</td>\n",
              "      <td>0.093217</td>\n",
              "      <td>-0.056954</td>\n",
              "      <td>-0.345783</td>\n",
              "      <td>-0.071456</td>\n",
              "      <td>-0.615525</td>\n",
              "      <td>0.299450</td>\n",
              "      <td>-0.361907</td>\n",
              "      <td>0.153372</td>\n",
              "      <td>0.102023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.160457</td>\n",
              "      <td>-0.595206</td>\n",
              "      <td>-0.231311</td>\n",
              "      <td>-0.040880</td>\n",
              "      <td>0.016651</td>\n",
              "      <td>-0.327501</td>\n",
              "      <td>-0.104689</td>\n",
              "      <td>-0.158801</td>\n",
              "      <td>-0.196048</td>\n",
              "      <td>0.014901</td>\n",
              "      <td>-0.336172</td>\n",
              "      <td>-0.002332</td>\n",
              "      <td>0.202591</td>\n",
              "      <td>-0.242723</td>\n",
              "      <td>0.059663</td>\n",
              "      <td>-0.198993</td>\n",
              "      <td>0.126893</td>\n",
              "      <td>0.143854</td>\n",
              "      <td>0.008069</td>\n",
              "      <td>0.378277</td>\n",
              "      <td>-0.390169</td>\n",
              "      <td>0.389232</td>\n",
              "      <td>0.494898</td>\n",
              "      <td>-0.393445</td>\n",
              "      <td>0.279485</td>\n",
              "      <td>0.022502</td>\n",
              "      <td>0.140657</td>\n",
              "      <td>0.021231</td>\n",
              "      <td>-0.466449</td>\n",
              "      <td>0.030109</td>\n",
              "      <td>-0.550439</td>\n",
              "      <td>0.104452</td>\n",
              "      <td>-0.086756</td>\n",
              "      <td>-0.266146</td>\n",
              "      <td>0.756947</td>\n",
              "      <td>-0.396499</td>\n",
              "      <td>-0.247555</td>\n",
              "      <td>-0.413154</td>\n",
              "      <td>0.299626</td>\n",
              "      <td>0.228981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>-0.166418</td>\n",
              "      <td>0.318898</td>\n",
              "      <td>0.330421</td>\n",
              "      <td>-0.459612</td>\n",
              "      <td>-0.162131</td>\n",
              "      <td>0.267957</td>\n",
              "      <td>0.236269</td>\n",
              "      <td>0.415457</td>\n",
              "      <td>-0.052450</td>\n",
              "      <td>-0.242916</td>\n",
              "      <td>0.365095</td>\n",
              "      <td>-0.367487</td>\n",
              "      <td>-0.144583</td>\n",
              "      <td>0.366747</td>\n",
              "      <td>-0.179655</td>\n",
              "      <td>0.657255</td>\n",
              "      <td>0.487739</td>\n",
              "      <td>0.288062</td>\n",
              "      <td>-0.025196</td>\n",
              "      <td>-0.025123</td>\n",
              "      <td>1.028283</td>\n",
              "      <td>0.087602</td>\n",
              "      <td>0.023006</td>\n",
              "      <td>0.461438</td>\n",
              "      <td>0.195829</td>\n",
              "      <td>0.166410</td>\n",
              "      <td>-0.029758</td>\n",
              "      <td>-0.221556</td>\n",
              "      <td>-0.417432</td>\n",
              "      <td>0.024154</td>\n",
              "      <td>0.216279</td>\n",
              "      <td>-0.060858</td>\n",
              "      <td>0.727997</td>\n",
              "      <td>-0.428053</td>\n",
              "      <td>0.269999</td>\n",
              "      <td>-0.264617</td>\n",
              "      <td>0.174458</td>\n",
              "      <td>-0.049764</td>\n",
              "      <td>0.479403</td>\n",
              "      <td>0.235735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.167074</td>\n",
              "      <td>-0.798584</td>\n",
              "      <td>-0.346666</td>\n",
              "      <td>-0.463683</td>\n",
              "      <td>-0.125071</td>\n",
              "      <td>-0.276027</td>\n",
              "      <td>-0.208865</td>\n",
              "      <td>0.138834</td>\n",
              "      <td>-0.086230</td>\n",
              "      <td>-0.017895</td>\n",
              "      <td>-0.248692</td>\n",
              "      <td>0.168226</td>\n",
              "      <td>0.456018</td>\n",
              "      <td>-0.261686</td>\n",
              "      <td>0.062923</td>\n",
              "      <td>-0.336427</td>\n",
              "      <td>-0.276823</td>\n",
              "      <td>0.119404</td>\n",
              "      <td>0.571929</td>\n",
              "      <td>0.441417</td>\n",
              "      <td>-0.024522</td>\n",
              "      <td>0.412292</td>\n",
              "      <td>-0.085683</td>\n",
              "      <td>-0.287694</td>\n",
              "      <td>-0.030651</td>\n",
              "      <td>0.387611</td>\n",
              "      <td>0.239172</td>\n",
              "      <td>-0.372846</td>\n",
              "      <td>-0.460792</td>\n",
              "      <td>-0.505841</td>\n",
              "      <td>-0.717796</td>\n",
              "      <td>0.123918</td>\n",
              "      <td>0.077422</td>\n",
              "      <td>0.259605</td>\n",
              "      <td>0.282041</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>-0.214877</td>\n",
              "      <td>-0.100879</td>\n",
              "      <td>0.318270</td>\n",
              "      <td>-0.004839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2512</th>\n",
              "      <td>-0.095390</td>\n",
              "      <td>0.449202</td>\n",
              "      <td>0.224358</td>\n",
              "      <td>-0.576839</td>\n",
              "      <td>0.287336</td>\n",
              "      <td>-0.409430</td>\n",
              "      <td>-0.004248</td>\n",
              "      <td>0.780078</td>\n",
              "      <td>-0.096692</td>\n",
              "      <td>-0.372438</td>\n",
              "      <td>-0.105739</td>\n",
              "      <td>-0.333202</td>\n",
              "      <td>0.027714</td>\n",
              "      <td>0.324660</td>\n",
              "      <td>-0.215557</td>\n",
              "      <td>0.960718</td>\n",
              "      <td>0.474020</td>\n",
              "      <td>0.017450</td>\n",
              "      <td>0.015001</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>0.432339</td>\n",
              "      <td>-0.218721</td>\n",
              "      <td>-0.218295</td>\n",
              "      <td>0.222339</td>\n",
              "      <td>0.328858</td>\n",
              "      <td>0.313048</td>\n",
              "      <td>0.265837</td>\n",
              "      <td>-0.011704</td>\n",
              "      <td>-0.654497</td>\n",
              "      <td>-0.086932</td>\n",
              "      <td>0.333828</td>\n",
              "      <td>0.368232</td>\n",
              "      <td>0.113449</td>\n",
              "      <td>-0.388320</td>\n",
              "      <td>-0.254458</td>\n",
              "      <td>-0.130834</td>\n",
              "      <td>0.342358</td>\n",
              "      <td>-0.447215</td>\n",
              "      <td>0.014278</td>\n",
              "      <td>-0.027058</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.537184</td>\n",
              "      <td>-0.904238</td>\n",
              "      <td>-0.234918</td>\n",
              "      <td>-0.608328</td>\n",
              "      <td>0.132864</td>\n",
              "      <td>-0.577511</td>\n",
              "      <td>-0.558329</td>\n",
              "      <td>-0.132951</td>\n",
              "      <td>-0.165845</td>\n",
              "      <td>-0.032519</td>\n",
              "      <td>-0.064881</td>\n",
              "      <td>0.362410</td>\n",
              "      <td>-0.008946</td>\n",
              "      <td>-0.385552</td>\n",
              "      <td>0.028843</td>\n",
              "      <td>-0.635229</td>\n",
              "      <td>-0.100598</td>\n",
              "      <td>0.389611</td>\n",
              "      <td>0.144955</td>\n",
              "      <td>0.429051</td>\n",
              "      <td>-0.211813</td>\n",
              "      <td>0.540881</td>\n",
              "      <td>0.100470</td>\n",
              "      <td>-0.189920</td>\n",
              "      <td>0.252190</td>\n",
              "      <td>0.060027</td>\n",
              "      <td>0.083821</td>\n",
              "      <td>-0.448317</td>\n",
              "      <td>-0.622622</td>\n",
              "      <td>-0.158499</td>\n",
              "      <td>-0.493352</td>\n",
              "      <td>0.116451</td>\n",
              "      <td>-0.185150</td>\n",
              "      <td>-0.215702</td>\n",
              "      <td>0.311338</td>\n",
              "      <td>0.104676</td>\n",
              "      <td>-0.270898</td>\n",
              "      <td>-0.086899</td>\n",
              "      <td>0.236654</td>\n",
              "      <td>0.390968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>-0.145079</td>\n",
              "      <td>0.475734</td>\n",
              "      <td>0.365407</td>\n",
              "      <td>-0.620131</td>\n",
              "      <td>0.328844</td>\n",
              "      <td>0.151111</td>\n",
              "      <td>0.108706</td>\n",
              "      <td>0.852761</td>\n",
              "      <td>-0.457996</td>\n",
              "      <td>-0.247563</td>\n",
              "      <td>-0.029036</td>\n",
              "      <td>-0.127879</td>\n",
              "      <td>0.112376</td>\n",
              "      <td>0.168221</td>\n",
              "      <td>-0.282487</td>\n",
              "      <td>0.590009</td>\n",
              "      <td>0.499115</td>\n",
              "      <td>0.254812</td>\n",
              "      <td>0.427291</td>\n",
              "      <td>-0.123747</td>\n",
              "      <td>0.638318</td>\n",
              "      <td>-0.070385</td>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.335780</td>\n",
              "      <td>0.097913</td>\n",
              "      <td>0.173838</td>\n",
              "      <td>0.614560</td>\n",
              "      <td>0.106443</td>\n",
              "      <td>-0.349500</td>\n",
              "      <td>-0.058919</td>\n",
              "      <td>0.505293</td>\n",
              "      <td>-0.064001</td>\n",
              "      <td>-0.172986</td>\n",
              "      <td>-0.075850</td>\n",
              "      <td>-0.132227</td>\n",
              "      <td>-0.384453</td>\n",
              "      <td>0.370201</td>\n",
              "      <td>-0.468317</td>\n",
              "      <td>-0.201656</td>\n",
              "      <td>0.048853</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.265857</td>\n",
              "      <td>-0.223197</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.323132</td>\n",
              "      <td>0.073406</td>\n",
              "      <td>-0.371595</td>\n",
              "      <td>-0.044837</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>0.018509</td>\n",
              "      <td>0.258279</td>\n",
              "      <td>-0.317874</td>\n",
              "      <td>0.332849</td>\n",
              "      <td>0.212638</td>\n",
              "      <td>0.056430</td>\n",
              "      <td>0.370784</td>\n",
              "      <td>-0.076809</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.098970</td>\n",
              "      <td>0.058247</td>\n",
              "      <td>0.772120</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.695418</td>\n",
              "      <td>0.071432</td>\n",
              "      <td>-0.349727</td>\n",
              "      <td>0.203655</td>\n",
              "      <td>0.144502</td>\n",
              "      <td>0.156430</td>\n",
              "      <td>-0.250324</td>\n",
              "      <td>-0.374293</td>\n",
              "      <td>-0.250994</td>\n",
              "      <td>-0.330648</td>\n",
              "      <td>-0.096069</td>\n",
              "      <td>0.097628</td>\n",
              "      <td>-0.629242</td>\n",
              "      <td>0.276352</td>\n",
              "      <td>-0.184843</td>\n",
              "      <td>-0.126158</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.549695</td>\n",
              "      <td>0.128526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>-0.090581</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.374877</td>\n",
              "      <td>-0.471052</td>\n",
              "      <td>0.101587</td>\n",
              "      <td>0.137666</td>\n",
              "      <td>0.020140</td>\n",
              "      <td>0.438660</td>\n",
              "      <td>-0.227439</td>\n",
              "      <td>-0.421619</td>\n",
              "      <td>-0.015464</td>\n",
              "      <td>-0.266985</td>\n",
              "      <td>-0.196794</td>\n",
              "      <td>0.108737</td>\n",
              "      <td>-0.082186</td>\n",
              "      <td>0.811929</td>\n",
              "      <td>0.190508</td>\n",
              "      <td>0.046835</td>\n",
              "      <td>-0.133266</td>\n",
              "      <td>-0.312746</td>\n",
              "      <td>0.837726</td>\n",
              "      <td>-0.376351</td>\n",
              "      <td>0.239629</td>\n",
              "      <td>0.392617</td>\n",
              "      <td>0.293395</td>\n",
              "      <td>0.253092</td>\n",
              "      <td>0.570215</td>\n",
              "      <td>-0.252462</td>\n",
              "      <td>-0.398234</td>\n",
              "      <td>-0.221244</td>\n",
              "      <td>0.518364</td>\n",
              "      <td>0.045404</td>\n",
              "      <td>0.120460</td>\n",
              "      <td>-0.335099</td>\n",
              "      <td>-0.117464</td>\n",
              "      <td>-0.292256</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>-0.173357</td>\n",
              "      <td>0.191883</td>\n",
              "      <td>0.055143</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.203764</td>\n",
              "      <td>-0.207143</td>\n",
              "      <td>-0.417275</td>\n",
              "      <td>-0.421830</td>\n",
              "      <td>0.229466</td>\n",
              "      <td>-0.182841</td>\n",
              "      <td>0.209093</td>\n",
              "      <td>-0.340755</td>\n",
              "      <td>-0.520002</td>\n",
              "      <td>0.335023</td>\n",
              "      <td>0.064961</td>\n",
              "      <td>0.460892</td>\n",
              "      <td>0.196377</td>\n",
              "      <td>0.173352</td>\n",
              "      <td>0.518383</td>\n",
              "      <td>0.143448</td>\n",
              "      <td>0.168927</td>\n",
              "      <td>-0.019226</td>\n",
              "      <td>-0.087348</td>\n",
              "      <td>0.389946</td>\n",
              "      <td>-0.294117</td>\n",
              "      <td>0.699093</td>\n",
              "      <td>0.011120</td>\n",
              "      <td>-0.384840</td>\n",
              "      <td>-0.081319</td>\n",
              "      <td>0.028739</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>-0.147006</td>\n",
              "      <td>-0.560598</td>\n",
              "      <td>-0.503186</td>\n",
              "      <td>-0.468980</td>\n",
              "      <td>0.193140</td>\n",
              "      <td>0.157212</td>\n",
              "      <td>-0.501378</td>\n",
              "      <td>0.166521</td>\n",
              "      <td>-0.202339</td>\n",
              "      <td>-0.203073</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>0.301909</td>\n",
              "      <td>0.507903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3886</th>\n",
              "      <td>-0.490045</td>\n",
              "      <td>0.403811</td>\n",
              "      <td>0.233433</td>\n",
              "      <td>-1.101597</td>\n",
              "      <td>0.230473</td>\n",
              "      <td>-0.140833</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>0.423244</td>\n",
              "      <td>-0.396777</td>\n",
              "      <td>-0.049143</td>\n",
              "      <td>0.206543</td>\n",
              "      <td>-0.689884</td>\n",
              "      <td>0.505751</td>\n",
              "      <td>-0.306384</td>\n",
              "      <td>-0.464576</td>\n",
              "      <td>0.924472</td>\n",
              "      <td>0.342408</td>\n",
              "      <td>0.543019</td>\n",
              "      <td>0.400223</td>\n",
              "      <td>-0.173314</td>\n",
              "      <td>0.342301</td>\n",
              "      <td>-0.150971</td>\n",
              "      <td>-0.109750</td>\n",
              "      <td>-0.179589</td>\n",
              "      <td>0.230435</td>\n",
              "      <td>0.035653</td>\n",
              "      <td>0.412052</td>\n",
              "      <td>-0.130912</td>\n",
              "      <td>-0.566213</td>\n",
              "      <td>-0.381750</td>\n",
              "      <td>0.798290</td>\n",
              "      <td>0.468334</td>\n",
              "      <td>0.196522</td>\n",
              "      <td>0.037070</td>\n",
              "      <td>0.054352</td>\n",
              "      <td>-0.617227</td>\n",
              "      <td>0.368253</td>\n",
              "      <td>-0.336719</td>\n",
              "      <td>0.157408</td>\n",
              "      <td>0.318117</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136145</td>\n",
              "      <td>-0.158449</td>\n",
              "      <td>0.145648</td>\n",
              "      <td>-0.330110</td>\n",
              "      <td>0.095605</td>\n",
              "      <td>-0.911029</td>\n",
              "      <td>-0.152521</td>\n",
              "      <td>-0.277227</td>\n",
              "      <td>-0.283700</td>\n",
              "      <td>0.005756</td>\n",
              "      <td>-0.165566</td>\n",
              "      <td>0.285174</td>\n",
              "      <td>0.500378</td>\n",
              "      <td>-0.387482</td>\n",
              "      <td>0.691950</td>\n",
              "      <td>-0.060325</td>\n",
              "      <td>-0.550582</td>\n",
              "      <td>0.222612</td>\n",
              "      <td>0.019106</td>\n",
              "      <td>1.069099</td>\n",
              "      <td>-0.530565</td>\n",
              "      <td>0.136196</td>\n",
              "      <td>0.366861</td>\n",
              "      <td>-0.426256</td>\n",
              "      <td>0.226655</td>\n",
              "      <td>0.038184</td>\n",
              "      <td>0.256335</td>\n",
              "      <td>0.157677</td>\n",
              "      <td>-0.489276</td>\n",
              "      <td>-0.687277</td>\n",
              "      <td>-0.630584</td>\n",
              "      <td>-0.016984</td>\n",
              "      <td>-0.350730</td>\n",
              "      <td>-0.172616</td>\n",
              "      <td>0.260950</td>\n",
              "      <td>-0.729053</td>\n",
              "      <td>-0.367372</td>\n",
              "      <td>-0.624519</td>\n",
              "      <td>0.301135</td>\n",
              "      <td>0.294430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7315</th>\n",
              "      <td>0.167208</td>\n",
              "      <td>0.299286</td>\n",
              "      <td>0.491610</td>\n",
              "      <td>-0.444910</td>\n",
              "      <td>-0.113778</td>\n",
              "      <td>-0.170792</td>\n",
              "      <td>0.454244</td>\n",
              "      <td>0.180629</td>\n",
              "      <td>-0.194229</td>\n",
              "      <td>-0.268134</td>\n",
              "      <td>-0.049930</td>\n",
              "      <td>-0.278949</td>\n",
              "      <td>0.090974</td>\n",
              "      <td>0.260803</td>\n",
              "      <td>-0.046972</td>\n",
              "      <td>0.854258</td>\n",
              "      <td>0.046183</td>\n",
              "      <td>0.218709</td>\n",
              "      <td>-0.057021</td>\n",
              "      <td>-0.069000</td>\n",
              "      <td>0.872898</td>\n",
              "      <td>-0.121805</td>\n",
              "      <td>0.250340</td>\n",
              "      <td>0.507785</td>\n",
              "      <td>0.031016</td>\n",
              "      <td>0.500577</td>\n",
              "      <td>0.406960</td>\n",
              "      <td>-0.329940</td>\n",
              "      <td>-0.440141</td>\n",
              "      <td>0.126961</td>\n",
              "      <td>0.761491</td>\n",
              "      <td>0.009389</td>\n",
              "      <td>0.311035</td>\n",
              "      <td>-0.478103</td>\n",
              "      <td>-0.139362</td>\n",
              "      <td>-0.231615</td>\n",
              "      <td>0.203378</td>\n",
              "      <td>-0.122280</td>\n",
              "      <td>0.229558</td>\n",
              "      <td>0.164038</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.222196</td>\n",
              "      <td>-0.372110</td>\n",
              "      <td>-0.287214</td>\n",
              "      <td>-0.214754</td>\n",
              "      <td>-0.091732</td>\n",
              "      <td>-0.311470</td>\n",
              "      <td>0.151199</td>\n",
              "      <td>-0.145242</td>\n",
              "      <td>-0.185514</td>\n",
              "      <td>0.069422</td>\n",
              "      <td>-0.001048</td>\n",
              "      <td>0.263794</td>\n",
              "      <td>0.176082</td>\n",
              "      <td>-0.525270</td>\n",
              "      <td>0.330416</td>\n",
              "      <td>-0.291158</td>\n",
              "      <td>-0.033593</td>\n",
              "      <td>0.270450</td>\n",
              "      <td>0.091955</td>\n",
              "      <td>0.530226</td>\n",
              "      <td>-0.025298</td>\n",
              "      <td>0.296416</td>\n",
              "      <td>0.235689</td>\n",
              "      <td>-0.678318</td>\n",
              "      <td>0.360752</td>\n",
              "      <td>0.281592</td>\n",
              "      <td>-0.151794</td>\n",
              "      <td>-0.625438</td>\n",
              "      <td>-0.146655</td>\n",
              "      <td>0.218224</td>\n",
              "      <td>-0.572947</td>\n",
              "      <td>-0.211265</td>\n",
              "      <td>0.197571</td>\n",
              "      <td>-0.295656</td>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.073510</td>\n",
              "      <td>-0.218864</td>\n",
              "      <td>-0.075352</td>\n",
              "      <td>0.194545</td>\n",
              "      <td>0.245993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7200 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2  ...       765       766       767\n",
              "Unnamed: 0                                ...                              \n",
              "5614       -0.340161 -0.148656  0.602034  ... -0.017511  0.452152  0.481604\n",
              "2383       -0.391934  0.589659  0.081654  ...  0.045654  0.267326  0.322551\n",
              "5448       -0.311391  0.246223  0.322206  ... -0.658689  0.190388  0.197619\n",
              "8412       -0.157567  0.439942  0.159633  ... -0.413154  0.299626  0.228981\n",
              "1448       -0.166418  0.318898  0.330421  ... -0.100879  0.318270 -0.004839\n",
              "...              ...       ...       ...  ...       ...       ...       ...\n",
              "2512       -0.095390  0.449202  0.224358  ... -0.086899  0.236654  0.390968\n",
              "8694       -0.145079  0.475734  0.365407  ...  0.001324  0.549695  0.128526\n",
              "8368       -0.090581  0.432085  0.374877  ...  0.063565  0.301909  0.507903\n",
              "3886       -0.490045  0.403811  0.233433  ... -0.624519  0.301135  0.294430\n",
              "7315        0.167208  0.299286  0.491610  ... -0.075352  0.194545  0.245993\n",
              "\n",
              "[7200 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "outputId": "87fb78ca-c846-47d3-ee3b-5b5a11147d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "outputId": "7456d1fd-1b7c-4c96-abe6-8f47a2356510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9e28ba9-c9f0-44bb-cba9-26087e5525a2"
      },
      "source": [
        "# TBA: Hyperpameter Tuning Through lr, units & batch; TensorBoard for training process and/or model structure?\n",
        "\n",
        "units = 2048\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "batch = 16\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "c = concatenate([x, c])\n",
        "\n",
        "def FFUnit(c):\n",
        "  x = Dense(units)(c)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  c = concatenate([x, c])\n",
        "  return c\n",
        "\n",
        "for i in range(6):\n",
        "  c = FFUnit(c)\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=batch)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 2048)         1574912     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 2048)         8192        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 2048)         0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 2048)         0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4096)         0           activation_9[0][0]               \n",
            "                                                                 dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 2048)         8390656     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 2048)         8192        dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 2048)         0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 2048)         0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 6144)         0           activation_10[0][0]              \n",
            "                                                                 concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 2048)         12584960    concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 2048)         8192        dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 2048)         0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 2048)         0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 8192)         0           activation_11[0][0]              \n",
            "                                                                 concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 2048)         16779264    concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 2048)         8192        dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 2048)         0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 2048)         0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 10240)        0           activation_12[0][0]              \n",
            "                                                                 concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 2048)         20973568    concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 2048)         8192        dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 2048)         0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 2048)         0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 12288)        0           activation_13[0][0]              \n",
            "                                                                 concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 2048)         25167872    concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 2048)         8192        dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 2048)         0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 2048)         0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 14336)        0           activation_14[0][0]              \n",
            "                                                                 concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 2048)         29362176    concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 2048)         8192        dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 2048)         0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 2048)         0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16384)        0           activation_15[0][0]              \n",
            "                                                                 concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 3)            49155       concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 3)            12          dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 3)            0           batch_normalization_16[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 114,939,919\n",
            "Trainable params: 114,911,241\n",
            "Non-trainable params: 28,678\n",
            "__________________________________________________________________________________________________\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "7200/7200 [==============================] - 30s 4ms/step - loss: 0.3399 - acc: 0.9269 - val_loss: 0.2022 - val_acc: 0.9722\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.2477 - acc: 0.9510 - val_loss: 0.3138 - val_acc: 0.8967\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.2045 - acc: 0.9567 - val_loss: 0.1783 - val_acc: 0.9644\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.1673 - acc: 0.9646 - val_loss: 0.1484 - val_acc: 0.9789\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.1378 - acc: 0.9712 - val_loss: 0.0970 - val_acc: 0.9822\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 31s 4ms/step - loss: 0.1135 - acc: 0.9785 - val_loss: 0.1037 - val_acc: 0.9833\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0960 - acc: 0.9838 - val_loss: 0.0981 - val_acc: 0.9856\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0916 - acc: 0.9858 - val_loss: 0.0908 - val_acc: 0.9878\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0805 - acc: 0.9883 - val_loss: 0.0842 - val_acc: 0.9900\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0739 - acc: 0.9889 - val_loss: 0.0827 - val_acc: 0.9867\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0690 - acc: 0.9907 - val_loss: 0.0876 - val_acc: 0.9878\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 25s 4ms/step - loss: 0.0594 - acc: 0.9946 - val_loss: 0.0727 - val_acc: 0.9867\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0534 - acc: 0.9946 - val_loss: 0.0721 - val_acc: 0.9889\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0509 - acc: 0.9942 - val_loss: 0.0660 - val_acc: 0.9911\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0454 - acc: 0.9957 - val_loss: 0.0851 - val_acc: 0.9767\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0401 - acc: 0.9972 - val_loss: 0.0615 - val_acc: 0.9844\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0425 - acc: 0.9954 - val_loss: 0.0580 - val_acc: 0.9856\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0402 - acc: 0.9960 - val_loss: 0.0751 - val_acc: 0.9811\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0354 - acc: 0.9968 - val_loss: 0.0638 - val_acc: 0.9856\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 25s 4ms/step - loss: 0.0388 - acc: 0.9953 - val_loss: 0.0649 - val_acc: 0.9800\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0296 - acc: 0.9972 - val_loss: 0.0554 - val_acc: 0.9844\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0296 - acc: 0.9965 - val_loss: 0.0624 - val_acc: 0.9844\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0277 - acc: 0.9974 - val_loss: 0.0490 - val_acc: 0.9900\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0301 - acc: 0.9960 - val_loss: 0.0612 - val_acc: 0.9844\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0250 - acc: 0.9976 - val_loss: 0.0598 - val_acc: 0.9811\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0235 - acc: 0.9976 - val_loss: 0.0582 - val_acc: 0.9844\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0220 - acc: 0.9974 - val_loss: 0.0618 - val_acc: 0.9867\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0211 - acc: 0.9979 - val_loss: 0.0640 - val_acc: 0.9822\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 31s 4ms/step - loss: 0.0231 - acc: 0.9982 - val_loss: 0.0470 - val_acc: 0.9878\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0202 - acc: 0.9987 - val_loss: 0.0490 - val_acc: 0.9900\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0222 - acc: 0.9981 - val_loss: 0.0500 - val_acc: 0.9856\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0233 - acc: 0.9982 - val_loss: 0.0534 - val_acc: 0.9878\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0189 - acc: 0.9989 - val_loss: 0.0475 - val_acc: 0.9878\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 25s 4ms/step - loss: 0.0180 - acc: 0.9990 - val_loss: 0.0455 - val_acc: 0.9889\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 26s 4ms/step - loss: 0.0220 - acc: 0.9982 - val_loss: 0.0470 - val_acc: 0.9878\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 26s 4ms/step - loss: 0.0164 - acc: 0.9990 - val_loss: 0.0430 - val_acc: 0.9867\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 26s 4ms/step - loss: 0.0186 - acc: 0.9989 - val_loss: 0.0462 - val_acc: 0.9878\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 26s 4ms/step - loss: 0.0145 - acc: 0.9992 - val_loss: 0.0535 - val_acc: 0.9856\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 26s 4ms/step - loss: 0.0181 - acc: 0.9986 - val_loss: 0.0600 - val_acc: 0.9800\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 25s 4ms/step - loss: 0.0209 - acc: 0.9974 - val_loss: 0.0467 - val_acc: 0.9878\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0126 - acc: 0.9994 - val_loss: 0.0421 - val_acc: 0.9878\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0184 - acc: 0.9982 - val_loss: 0.0420 - val_acc: 0.9878\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0140 - acc: 0.9987 - val_loss: 0.0401 - val_acc: 0.9889\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0152 - acc: 0.9985 - val_loss: 0.0465 - val_acc: 0.9822\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0160 - acc: 0.9989 - val_loss: 0.0465 - val_acc: 0.9867\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0132 - acc: 0.9994 - val_loss: 0.0426 - val_acc: 0.9911\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0117 - acc: 0.9997 - val_loss: 0.0390 - val_acc: 0.9911\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0167 - acc: 0.9975 - val_loss: 0.0388 - val_acc: 0.9900\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 24s 3ms/step - loss: 0.0120 - acc: 0.9993 - val_loss: 0.0409 - val_acc: 0.9878\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0129 - acc: 0.9992 - val_loss: 0.0423 - val_acc: 0.9889\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0125 - acc: 0.9990 - val_loss: 0.0413 - val_acc: 0.9911\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0124 - acc: 0.9989 - val_loss: 0.0518 - val_acc: 0.9844\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.0139 - acc: 0.9985 - val_loss: 0.0461 - val_acc: 0.9889\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 167us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03340546187427309, 0.99]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_old_text = pd.DataFrame(X_old_text)\n",
        "X_new_text = pd.DataFrame(X_new_text)\n",
        "\n",
        "Pred_old = model.predict(X_old_text)\n",
        "Pred_new = model.predict(X_new_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGIYZt1SWc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "pred_old, pred_new, pred_delta = [], [], []\n",
        "for i in range(len(X_old)): \n",
        "  if df.label.iloc[i][2] == 'A':\n",
        "    pred_old.append(Pred_old[i][0])\n",
        "    pred_new.append(Pred_new[i][0])\n",
        "  elif df.label.iloc[i][2] == 'D':\n",
        "    pred_old.append(Pred_old[i][1])\n",
        "    pred_new.append(Pred_new[i][1])\n",
        "  else:\n",
        "    pred_old.append(Pred_old[i][2])\n",
        "    pred_new.append(Pred_new[i][2])\n",
        "  pred_delta.append(pred_new[i] - pred_old[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPspREI2S7_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "17408b31-e342-48f7-9fe6-f8feb82f9731"
      },
      "source": [
        "Pred_new"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00203248, 0.00404928, 0.9939182 ],\n",
              "       [0.00727922, 0.00358726, 0.98913354],\n",
              "       [0.7721936 , 0.0139337 , 0.2138727 ],\n",
              "       [0.00891649, 0.00448649, 0.986597  ],\n",
              "       [0.1125346 , 0.00660644, 0.8808589 ],\n",
              "       [0.00478178, 0.12722215, 0.86799604],\n",
              "       [0.00129496, 0.00179181, 0.9969132 ],\n",
              "       [0.9057036 , 0.0124633 , 0.08183308],\n",
              "       [0.0044654 , 0.01962554, 0.97590905],\n",
              "       [0.01275224, 0.37084013, 0.61640763],\n",
              "       [0.00131232, 0.00184203, 0.99684566],\n",
              "       [0.00157802, 0.00203662, 0.99638534],\n",
              "       [0.06523793, 0.00728321, 0.9274789 ],\n",
              "       [0.0049958 , 0.01522104, 0.97978324],\n",
              "       [0.00574131, 0.00443773, 0.98982096],\n",
              "       [0.00687255, 0.39102003, 0.60210735],\n",
              "       [0.00370327, 0.02648717, 0.96980953],\n",
              "       [0.4538185 , 0.5200149 , 0.02616665],\n",
              "       [0.00597845, 0.89414316, 0.09987843],\n",
              "       [0.00557098, 0.21227232, 0.7821567 ],\n",
              "       [0.00246911, 0.00275852, 0.9947724 ],\n",
              "       [0.0138345 , 0.00573398, 0.9804315 ],\n",
              "       [0.99641466, 0.00144496, 0.00214028],\n",
              "       [0.99653745, 0.00132717, 0.00213536],\n",
              "       [0.9788658 , 0.01504812, 0.00608605],\n",
              "       [0.9936702 , 0.00241597, 0.0039138 ],\n",
              "       [0.9908352 , 0.00381392, 0.00535089],\n",
              "       [0.99080664, 0.00326382, 0.00592948],\n",
              "       [0.9839093 , 0.00546658, 0.0106241 ],\n",
              "       [0.78060657, 0.01258628, 0.20680715],\n",
              "       [0.98345435, 0.00465805, 0.01188755],\n",
              "       [0.9651136 , 0.00677279, 0.0281136 ],\n",
              "       [0.3471334 , 0.00445799, 0.6484086 ],\n",
              "       [0.9960854 , 0.0016397 , 0.00227486],\n",
              "       [0.9703579 , 0.00355378, 0.02608839],\n",
              "       [0.13482103, 0.01107593, 0.854103  ],\n",
              "       [0.00599374, 0.9904757 , 0.00353051],\n",
              "       [0.00191586, 0.9896975 , 0.00838658],\n",
              "       [0.9291923 , 0.06178744, 0.00902024],\n",
              "       [0.26015538, 0.72951996, 0.01032462],\n",
              "       [0.36211988, 0.6236394 , 0.01424073],\n",
              "       [0.24055268, 0.750287  , 0.00916029],\n",
              "       [0.01278337, 0.9841526 , 0.003064  ],\n",
              "       [0.11760683, 0.8728448 , 0.00954831],\n",
              "       [0.10767117, 0.8812528 , 0.01107599],\n",
              "       [0.00217144, 0.9885666 , 0.00926204],\n",
              "       [0.00230702, 0.9871459 , 0.01054701],\n",
              "       [0.9927698 , 0.00266026, 0.00456993],\n",
              "       [0.00191262, 0.9965049 , 0.00158248],\n",
              "       [0.00338391, 0.99305266, 0.00356337],\n",
              "       [0.0410355 , 0.82982755, 0.129137  ],\n",
              "       [0.09062817, 0.8946161 , 0.01475569],\n",
              "       [0.0458078 , 0.80870193, 0.14549027],\n",
              "       [0.00164791, 0.9965995 , 0.00175255],\n",
              "       [0.00889827, 0.9310496 , 0.06005207]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLh1fA2cVQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "bec3a638-6d58-457a-f169-f1658630209b"
      },
      "source": [
        "Pred_old"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.7181025e-03, 2.0850466e-03, 9.9619687e-01],\n",
              "       [1.0057823e-03, 1.9193310e-03, 9.9707484e-01],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9726260e-01, 1.2140428e-03, 1.5233783e-03],\n",
              "       [9.7003227e-01, 5.3322855e-03, 2.4635507e-02],\n",
              "       [9.9176776e-01, 3.1889551e-03, 5.0432472e-03],\n",
              "       [9.9118304e-01, 3.5566376e-03, 5.2603288e-03],\n",
              "       [9.9802750e-01, 7.4436510e-04, 1.2280226e-03],\n",
              "       [9.9561691e-01, 1.7827221e-03, 2.6004734e-03],\n",
              "       [9.9689227e-01, 1.2624987e-03, 1.8452296e-03],\n",
              "       [7.7323711e-01, 2.1403381e-01, 1.2729074e-02],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [2.1795547e-03, 9.9624926e-01, 1.5712433e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHeYgeqDcpdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "242fef9d-04b1-4be2-b45c-1992dee5e275"
      },
      "source": [
        "pred_delta"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0006469997,\n",
              " 0.0062674712,\n",
              " 0.7692856,\n",
              " 0.0075082732,\n",
              " 0.11091142,\n",
              " 0.0035563535,\n",
              " 0.0001149649,\n",
              " 0.8701338,\n",
              " 0.0013317978,\n",
              " 0.010962961,\n",
              " 0.0003645725,\n",
              " 0.0007117655,\n",
              " 0.005052993,\n",
              " 0.013393214,\n",
              " 0.0025015022,\n",
              " 0.38927066,\n",
              " 0.025058856,\n",
              " 0.5102656,\n",
              " 0.88618094,\n",
              " 0.20976229,\n",
              " 0.0006734743,\n",
              " 0.0038146484,\n",
              " -0.00038100418,\n",
              " -0.00039460126,\n",
              " 0.013222156,\n",
              " 0.0006942038,\n",
              " 0.0022851413,\n",
              " 0.0036058815,\n",
              " 0.009100723,\n",
              " 0.18217164,\n",
              " 0.0068443003,\n",
              " 0.02285327,\n",
              " 0.64718056,\n",
              " -0.00032561668,\n",
              " 0.024243161,\n",
              " 0.841374,\n",
              " 0.004301003,\n",
              " 0.00018604274,\n",
              " 0.92801166,\n",
              " 0.2587181,\n",
              " 0.36065552,\n",
              " 0.2392447,\n",
              " 0.010925005,\n",
              " 0.11558467,\n",
              " 0.10364332,\n",
              " 0.0069973934,\n",
              " 0.0052578673,\n",
              " 0.0022290314,\n",
              " 0.00016624876,\n",
              " 0.002378278,\n",
              " 0.1277139,\n",
              " 0.012937957,\n",
              " 0.1438997,\n",
              " -0.0009582578,\n",
              " 0.05848083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0byXlehgT07U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/arrow_plot_results.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['donor_authorship_score'] = pred_old\n",
        "df['style_authorship_score'] = pred_new\n",
        "df['authorship_delta'] = pred_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWN5M4qT74-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.to_csv('./gdrive/My Drive/DL/Style/all_scores.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KxX5ljnUVgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}