{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert_masked.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sI6DKVblH_CD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "01741599-412c-4dd8-c529-add47e071c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\r\u001b[K     |█                               | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 26.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 31.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 34.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 36.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 39.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 71kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 81kB 41.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 92kB 43.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 112kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 122kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 143kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 153kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 163kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 174kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 184kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 194kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 215kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 225kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 235kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 245kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 256kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 266kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 276kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 286kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 296kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 307kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 317kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 327kB 45.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 337kB 45.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 348kB 45.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 358kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.32)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 54.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.32)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=a5ac1c1df4d38fff65e019c6a69be5c3f00e08c5aaa4f4c97a32f23b2abbcd9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, regex, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "outputId": "fec4f0f6-90cf-4ea0-f016-33b5666a8a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, without masking\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "# X = df.text.astype('str')\n",
        "# y = df.author.astype('category')\n",
        "\n",
        "# # lbl_enc = preprocessing.LabelEncoder()\n",
        "# # y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "# y = np.asarray(y)\n",
        "# onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "# encoded = y.reshape(len(y), 1)\n",
        "# y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkqPiy6v2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, with masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "# lbl_enc = preprocessing.LabelEncoder()\n",
        "# y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Embeddings()\n",
        "\n",
        "# X_text = []\n",
        "# for sentence in tqdm(X):\n",
        "#     X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQfIbHooxmk",
        "colab_type": "code",
        "outputId": "053df7e1-bc17-40aa-d3b5-a25c897c7ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# For Test Time\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "dft = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "X_old = dft.old.astype('str')\n",
        "X_new = dft.new.astype('str')\n",
        "\n",
        "model = Embeddings()\n",
        "\n",
        "X_old_text, X_new_text = [], []\n",
        "for sentence in tqdm(X_old):\n",
        "    X_old_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "for sentence in tqdm(X_new):\n",
        "    X_new_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 925756.54B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 74216.58B/s]\n",
            "100%|██████████| 440473133/440473133 [00:16<00:00, 26964703.15B/s]\n",
            "100%|██████████| 55/55 [00:31<00:00,  1.69it/s]\n",
            "100%|██████████| 55/55 [00:34<00:00,  1.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_YrIamp5Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_old = pd.DataFrame(X_old_text)\n",
        "# X_old.to_csv('./gdrive/My Drive/DL/Style/X_old_Embedding.csv')\n",
        "\n",
        "# X_new = pd.DataFrame(X_new_text)\n",
        "# X_new.to_csv('./gdrive/My Drive/DL/Style/X_new_Embedding.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplxzc29EEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16TunKA3KVd1",
        "colab_type": "code",
        "outputId": "ad8c70dc-3039-4242-d696-c53bd38f694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# For old Rand examples\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/rand_examples.csv'\n",
        "# df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "\n",
        "# X1 = df['Rand-donor-text'].astype('str')\n",
        "# X2 = df['Rand_117M_10000_Nabokov-All-3'].astype('str')\n",
        "# X3 = df['Rand-output-ngram'].astype('str')\n",
        "\n",
        "# model = Embeddings()\n",
        "\n",
        "# X1_text, X2_text, X3_text = [], [], []\n",
        "# for sentence in tqdm(X1):\n",
        "#     X1_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X2_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X3_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1184340.76B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 48256.47B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 36565607.31B/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.65it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.43it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQlnt96NhI9",
        "colab_type": "text"
      },
      "source": [
        "### Building A Atyle Alassifier on Top of DistilBert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uDpIahUMo8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64095b39-c23e-4b12-e604-c4147d51a844"
      },
      "source": [
        "# For Baselines\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "xvalid, xtest, yvalid, ytest = train_test_split(xvalid, yvalid, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(xtrain.shape, xvalid.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200,) (900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "dfe48d77-b955-4195-ccbb-f27b9024a8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "outputId": "87fb78ca-c846-47d3-ee3b-5b5a11147d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI6DKVblH_CD",
        "colab_type": "text"
      },
      "source": [
        "### Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03oQGXQJIIhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4fb27158-b71b-4480-ddea-e8586b11d7f6"
      },
      "source": [
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow_hub as hub\n",
        "%matplotlib inline\n",
        "plt.xkcd()\n",
        "\n",
        "# Defining Multi-Class Log Loss\n",
        "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
        "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with class predictions, one probability per class\n",
        "    \"\"\"\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i, val in enumerate(actual):\n",
        "            actual2[i, val] = 1\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alCe8WVxNjK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "d1fb67fe-9006-4a39-9645-d4d78cc4a15b"
      },
      "source": [
        "#TF-IDF features\n",
        "tfv = TfidfVectorizer(min_df=3,\n",
        "                      max_features=None,\n",
        "                      strip_accents='unicode',\n",
        "                      analyzer='word',\n",
        "                      token_pattern=r'\\w{1,}',\n",
        "                      ngram_range=(1,3),\n",
        "                      use_idf=1,\n",
        "                      smooth_idf=1,\n",
        "                      sublinear_tf=1,\n",
        "                      stop_words='english')\n",
        "\n",
        "tfv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_tfv = tfv.transform(xtrain)\n",
        "xvalid_tfv = tfv.transform(xvalid)\n",
        "\n",
        "print(\"The size of the learnt vocabulary is \", len(tfv.vocabulary_))\n",
        "\n",
        "def display_scores(vectorizer, tfidf_result):\n",
        "    scores = zip(vectorizer.get_feature_names(), np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    count = 0\n",
        "    for item in sorted_scores: \n",
        "        print(item[0], item[1])\n",
        "        count += 1\n",
        "        if count>=20:\n",
        "            break\n",
        "\n",
        "#See top 10 TF-IDF scores\n",
        "print('Top 20 TF-IDF scores are --- \\n')\n",
        "display_scores(tfv,xvalid_tfv)\n",
        "\n",
        "\n",
        "\n",
        "#Logistic Regression on TF-IDF\n",
        "clf_tfidf = LogisticRegression(C=1.0)\n",
        "clf_tfidf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf_tfidf.predict_proba(xvalid_tfv)\n",
        "pred = clf_tfidf.predict(xvalid_tfv)\n",
        "print(\"logloss for TF-IDF + LR : \" + str(multiclass_logloss(yvalid, predictions)))\n",
        "print(classification_report(yvalid, pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the learnt vocabulary is  24138\n",
            "Top 20 TF-IDF scores are --- \n",
            "\n",
            "mask 42.3373247275727\n",
            "mask mask 19.49016758000067\n",
            "s 19.264278950832434\n",
            "said 18.40952044299222\n",
            "mask s 11.89549140304362\n",
            "said mask 10.86659421311315\n",
            "time 10.83122311068303\n",
            "d 9.641361705463183\n",
            "did 9.465040800253758\n",
            "like 9.374128794686039\n",
            "good 9.262415173964058\n",
            "know 9.161314605106275\n",
            "little 9.152136391660441\n",
            "d mask 8.698420964845504\n",
            "man 8.46385578455\n",
            "mask mask mask 8.412836404979416\n",
            "mr 8.283823936742474\n",
            "shall 8.257588996039829\n",
            "king 8.07012618864858\n",
            "great 7.981449294301742\n",
            "logloss for TF-IDF + LR : 0.3384619160096547\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       301\n",
            "           1       0.98      0.95      0.97       306\n",
            "           2       0.93      0.98      0.95       293\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.96      0.96      0.96       900\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySv4hX9IOzpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "3be96d3a-349e-43eb-b3fb-b62c67f2474b"
      },
      "source": [
        "ctv = CountVectorizer(analyzer='word',\n",
        "                      token_pattern=r'\\w{1,}',\n",
        "                      ngram_range=(1,3),\n",
        "                      stop_words='english')\n",
        "\n",
        "ctv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_ctv = ctv.transform(xtrain)\n",
        "xvalid_ctv = ctv.transform(xvalid)\n",
        "\n",
        "clf_ctv = LogisticRegression(C=1.0)\n",
        "clf_ctv.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf_ctv.predict_proba(xvalid_ctv)\n",
        "predictions_rounded = clf_ctv.predict(xvalid_ctv)\n",
        "print(\"logloss for CVectorizer + LR : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logloss for CVectorizer + LR :  0.13450918346659163\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       301\n",
            "           1       0.98      0.95      0.96       306\n",
            "           2       0.94      0.97      0.95       293\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.96      0.96      0.96       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHoVJDM7ICKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afb66a7f-ffa5-474c-c591-bec8440e96d0"
      },
      "source": [
        "# TF-IDF + NB\n",
        "#Multinomial Naive Bayes on TF-IDF\n",
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "predictions_rounded = clf.predict(xvalid_tfv)\n",
        "print(\"logloss for Multinomial Naive Bayes: \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss for Multinomial Naive Bayes:  0.23024957811771907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       301\n",
            "           1       0.98      0.98      0.98       306\n",
            "           2       0.98      0.97      0.97       293\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TCgHhrhPUBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8d5245be-6732-42a3-e5af-7333e3cb9886"
      },
      "source": [
        "#Multinomial Naive Bayes on Count Vectors\n",
        "clf.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv)\n",
        "predictions_rounded = clf.predict(xvalid_ctv)\n",
        "print(\"logloss for Multinomial Naive Bayes with count vectorizer : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss for Multinomial Naive Bayes with count vectorizer :  0.18638961017304032\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       301\n",
            "           1       0.95      0.99      0.97       306\n",
            "           2       0.99      0.94      0.97       293\n",
            "\n",
            "    accuracy                           0.97       900\n",
            "   macro avg       0.98      0.97      0.97       900\n",
            "weighted avg       0.97      0.97      0.97       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCYSay6-PdKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "714841d8-a409-4c9d-f77f-fe6be926f62a"
      },
      "source": [
        "# SVD on TF-IDF --> SVM\n",
        "\n",
        "svd = decomposition.TruncatedSVD(n_components=100)\n",
        "svd.fit(xtrain_tfv)\n",
        "xtrain_svd = svd.transform(xtrain_tfv)\n",
        "xvalid_svd = svd.transform(xvalid_tfv)\n",
        "\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(xtrain_svd)\n",
        "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
        "xvalid_svd_scl = scl.transform(xvalid_svd)\n",
        "\n",
        "clf = SVC(C=1.0, probability=True)\n",
        "clf.fit(xtrain_svd_scl, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd_scl)\n",
        "predictions_rounded = clf.predict(xvalid_svd_scl)\n",
        "print(\" SVM logloss : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM logloss :  0.16455732235237291\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95       301\n",
            "           1       0.98      0.92      0.95       306\n",
            "           2       0.90      0.99      0.94       293\n",
            "\n",
            "    accuracy                           0.95       900\n",
            "   macro avg       0.95      0.95      0.95       900\n",
            "weighted avg       0.95      0.95      0.95       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88k8iwQ6V5FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XGBoost on SVD features\n",
        "clf = xgb.XGBClassifier()\n",
        "clf.fit(xtrain_svd, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd)\n",
        "predictions_rounded = clf.predict(xvalid_svd)\n",
        "\n",
        "print (\"XGBOOST logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlgV4SWjWS3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3e191da6-c1c1-477d-a00c-869a3c90c81b"
      },
      "source": [
        "# !wget http://www-nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 21:52:41--  http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving www-nlp.stanford.edu (www-nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to www-nlp.stanford.edu (www-nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2019-12-07 21:52:42--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2019-12-07 21:52:42--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.94MB/s    in 16m 58s \n",
            "\n",
            "2019-12-07 22:09:40 (2.04 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opAaqtHuWdIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "753cf871-44cf-41cd-e95d-331aecfeb75e"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('/content/gdrive/My Drive/DL/NLP/GloVe/glove.6B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs        \n",
        "    except ValueError:\n",
        "        pass\n",
        "    \n",
        "f.close()\n",
        "print(\"Found \",len(embeddings_index),\"vectors\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:33, 11767.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found  400000 vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvmsZcBEWiIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03d87fc8-6d3f-4d1c-d87a-887e2267e495"
      },
      "source": [
        "# generate weighted sentence embeddings\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(embeddings_index[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    if type(v) != np.ndarray:\n",
        "        return np.zeros(300)\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
        "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7200/7200 [00:08<00:00, 861.22it/s]\n",
            "100%|██████████| 900/900 [00:01<00:00, 873.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crwv4UzQWl93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "297346c5-6883-401e-9c9b-5fdf6ddd8d90"
      },
      "source": [
        "xtrain_glove = np.array(xtrain_glove)\n",
        "xvalid_glove = np.array(xvalid_glove)\n",
        "\n",
        "clf = LogisticRegression(C=1.0)\n",
        "clf.fit(xtrain_glove, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_glove)\n",
        "predictions_rounded = clf.predict(xvalid_glove)\n",
        "print(\"logloss : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logloss :  0.39487923541037245\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92       301\n",
            "           1       0.93      0.87      0.90       306\n",
            "           2       0.90      0.91      0.91       293\n",
            "\n",
            "    accuracy                           0.91       900\n",
            "   macro avg       0.91      0.91      0.91       900\n",
            "weighted avg       0.91      0.91      0.91       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDkSeVMZWpbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale the data before any neural net:\n",
        "scl = preprocessing.StandardScaler()\n",
        "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
        "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
        "\n",
        "\n",
        "# we need to binarize the labels for the neural net\n",
        "ytrain_enc = np_utils.to_categorical(ytrain)\n",
        "yvalid_enc = np_utils.to_categorical(yvalid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2YuApUWs34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "507eac17-4a5a-4ba2-a915-2c56488d8f2f"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam')\n",
        "\n",
        "model.fit(xtrain_glove_scl,\n",
        "          y=ytrain_enc,\n",
        "          batch_size=64, \n",
        "          epochs=5,\n",
        "          verbose=1, \n",
        "          validation_data=(xvalid_glove_scl, yvalid_enc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.4825 - val_loss: 0.2730\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 1s 115us/step - loss: 0.2642 - val_loss: 0.2489\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 1s 112us/step - loss: 0.1991 - val_loss: 0.2546\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 1s 104us/step - loss: 0.1679 - val_loss: 0.2335\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 1s 118us/step - loss: 0.1395 - val_loss: 0.2456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e1cf424e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfp3KKgWvm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b419c40b-4459-461a-8254-d7e324f79c54"
      },
      "source": [
        "# LSTM\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 70\n",
        "\n",
        "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "\n",
        "# zero pad the sequences\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "word_index = token.word_index\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# A simple LSTM with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(xtrain_pad,\n",
        "          y=ytrain_enc,\n",
        "          batch_size=512,\n",
        "          epochs=100, \n",
        "          verbose=1,\n",
        "          validation_data=(xvalid_pad, yvalid_enc),\n",
        "          callbacks=[earlystop])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31382/31382 [00:00<00:00, 522007.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/100\n",
            "7200/7200 [==============================] - 3s 382us/step - loss: 1.1068 - val_loss: 1.0563\n",
            "Epoch 2/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 1.0561 - val_loss: 0.9654\n",
            "Epoch 3/100\n",
            "7200/7200 [==============================] - 2s 260us/step - loss: 0.9717 - val_loss: 0.6923\n",
            "Epoch 4/100\n",
            "7200/7200 [==============================] - 2s 278us/step - loss: 0.8319 - val_loss: 0.5786\n",
            "Epoch 5/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.7711 - val_loss: 0.4971\n",
            "Epoch 6/100\n",
            "7200/7200 [==============================] - 2s 250us/step - loss: 0.7074 - val_loss: 0.4626\n",
            "Epoch 7/100\n",
            "7200/7200 [==============================] - 2s 274us/step - loss: 0.6805 - val_loss: 0.4425\n",
            "Epoch 8/100\n",
            "7200/7200 [==============================] - 2s 271us/step - loss: 0.6346 - val_loss: 0.4172\n",
            "Epoch 9/100\n",
            "7200/7200 [==============================] - 2s 252us/step - loss: 0.5946 - val_loss: 0.3981\n",
            "Epoch 10/100\n",
            "7200/7200 [==============================] - 2s 241us/step - loss: 0.5709 - val_loss: 0.3899\n",
            "Epoch 11/100\n",
            "7200/7200 [==============================] - 2s 235us/step - loss: 0.5328 - val_loss: 0.3478\n",
            "Epoch 12/100\n",
            "7200/7200 [==============================] - 2s 239us/step - loss: 0.5056 - val_loss: 0.3146\n",
            "Epoch 13/100\n",
            "7200/7200 [==============================] - 2s 239us/step - loss: 0.4807 - val_loss: 0.3046\n",
            "Epoch 14/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.4791 - val_loss: 0.2895\n",
            "Epoch 15/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.4403 - val_loss: 0.2560\n",
            "Epoch 16/100\n",
            "7200/7200 [==============================] - 2s 245us/step - loss: 0.4251 - val_loss: 0.2448\n",
            "Epoch 17/100\n",
            "7200/7200 [==============================] - 2s 298us/step - loss: 0.3958 - val_loss: 0.2441\n",
            "Epoch 18/100\n",
            "7200/7200 [==============================] - 2s 277us/step - loss: 0.3798 - val_loss: 0.2228\n",
            "Epoch 19/100\n",
            "7200/7200 [==============================] - 2s 256us/step - loss: 0.3824 - val_loss: 0.2161\n",
            "Epoch 20/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.3623 - val_loss: 0.2141\n",
            "Epoch 21/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.3569 - val_loss: 0.2075\n",
            "Epoch 22/100\n",
            "7200/7200 [==============================] - 2s 234us/step - loss: 0.3372 - val_loss: 0.2110\n",
            "Epoch 23/100\n",
            "7200/7200 [==============================] - 2s 224us/step - loss: 0.3218 - val_loss: 0.2027\n",
            "Epoch 24/100\n",
            "7200/7200 [==============================] - 2s 252us/step - loss: 0.3062 - val_loss: 0.1906\n",
            "Epoch 25/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.3218 - val_loss: 0.1975\n",
            "Epoch 26/100\n",
            "7200/7200 [==============================] - 2s 234us/step - loss: 0.3058 - val_loss: 0.1901\n",
            "Epoch 27/100\n",
            "7200/7200 [==============================] - 2s 265us/step - loss: 0.2871 - val_loss: 0.2021\n",
            "Epoch 28/100\n",
            "7200/7200 [==============================] - 2s 260us/step - loss: 0.2815 - val_loss: 0.1804\n",
            "Epoch 29/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.2887 - val_loss: 0.1852\n",
            "Epoch 30/100\n",
            "7200/7200 [==============================] - 2s 257us/step - loss: 0.2713 - val_loss: 0.1781\n",
            "Epoch 31/100\n",
            "7200/7200 [==============================] - 2s 243us/step - loss: 0.2607 - val_loss: 0.1715\n",
            "Epoch 32/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.2563 - val_loss: 0.1760\n",
            "Epoch 33/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.2510 - val_loss: 0.1699\n",
            "Epoch 34/100\n",
            "7200/7200 [==============================] - 2s 272us/step - loss: 0.2465 - val_loss: 0.1658\n",
            "Epoch 35/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2491 - val_loss: 0.1653\n",
            "Epoch 36/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.2361 - val_loss: 0.1600\n",
            "Epoch 37/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2294 - val_loss: 0.1592\n",
            "Epoch 38/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2382 - val_loss: 0.1677\n",
            "Epoch 39/100\n",
            "7200/7200 [==============================] - 2s 258us/step - loss: 0.2212 - val_loss: 0.1586\n",
            "Epoch 40/100\n",
            "7200/7200 [==============================] - 2s 267us/step - loss: 0.2263 - val_loss: 0.1584\n",
            "Epoch 41/100\n",
            "7200/7200 [==============================] - 2s 242us/step - loss: 0.2031 - val_loss: 0.1578\n",
            "Epoch 42/100\n",
            "7200/7200 [==============================] - 2s 245us/step - loss: 0.2258 - val_loss: 0.1636\n",
            "Epoch 43/100\n",
            "7200/7200 [==============================] - 2s 228us/step - loss: 0.2088 - val_loss: 0.1568\n",
            "Epoch 44/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.1948 - val_loss: 0.1562\n",
            "Epoch 45/100\n",
            "7200/7200 [==============================] - 2s 250us/step - loss: 0.2050 - val_loss: 0.1651\n",
            "Epoch 46/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.2001 - val_loss: 0.1634\n",
            "Epoch 47/100\n",
            "7200/7200 [==============================] - 2s 257us/step - loss: 0.1953 - val_loss: 0.1686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4db432a240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW-rgdWqWiRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7edafa02-717f-4bf3-ea87-5a95b74835f4"
      },
      "source": [
        "# GRU with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/100\n",
            "7200/7200 [==============================] - 6s 793us/step - loss: 1.1332 - val_loss: 1.0500\n",
            "Epoch 2/100\n",
            "7200/7200 [==============================] - 4s 546us/step - loss: 1.0780 - val_loss: 0.9561\n",
            "Epoch 3/100\n",
            "7200/7200 [==============================] - 4s 533us/step - loss: 1.0011 - val_loss: 0.8831\n",
            "Epoch 4/100\n",
            "7200/7200 [==============================] - 4s 541us/step - loss: 0.9277 - val_loss: 0.7337\n",
            "Epoch 5/100\n",
            "7200/7200 [==============================] - 4s 547us/step - loss: 0.8354 - val_loss: 0.6572\n",
            "Epoch 6/100\n",
            "7200/7200 [==============================] - 4s 546us/step - loss: 0.7598 - val_loss: 0.5399\n",
            "Epoch 7/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.6445 - val_loss: 0.4082\n",
            "Epoch 8/100\n",
            "7200/7200 [==============================] - 4s 548us/step - loss: 0.5345 - val_loss: 0.3432\n",
            "Epoch 9/100\n",
            "7200/7200 [==============================] - 4s 553us/step - loss: 0.4750 - val_loss: 0.2897\n",
            "Epoch 10/100\n",
            "7200/7200 [==============================] - 4s 538us/step - loss: 0.4398 - val_loss: 0.2734\n",
            "Epoch 11/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.4032 - val_loss: 0.2955\n",
            "Epoch 12/100\n",
            "7200/7200 [==============================] - 4s 537us/step - loss: 0.4045 - val_loss: 0.2466\n",
            "Epoch 13/100\n",
            "7200/7200 [==============================] - 4s 535us/step - loss: 0.3705 - val_loss: 0.2323\n",
            "Epoch 14/100\n",
            "7200/7200 [==============================] - 4s 543us/step - loss: 0.3373 - val_loss: 0.2190\n",
            "Epoch 15/100\n",
            "7200/7200 [==============================] - 4s 550us/step - loss: 0.3305 - val_loss: 0.2263\n",
            "Epoch 16/100\n",
            "7200/7200 [==============================] - 4s 543us/step - loss: 0.3240 - val_loss: 0.2140\n",
            "Epoch 17/100\n",
            "7200/7200 [==============================] - 4s 558us/step - loss: 0.3058 - val_loss: 0.2241\n",
            "Epoch 18/100\n",
            "7200/7200 [==============================] - 4s 536us/step - loss: 0.2989 - val_loss: 0.1893\n",
            "Epoch 19/100\n",
            "7200/7200 [==============================] - 4s 556us/step - loss: 0.2914 - val_loss: 0.1994\n",
            "Epoch 20/100\n",
            "7200/7200 [==============================] - 4s 557us/step - loss: 0.2815 - val_loss: 0.1965\n",
            "Epoch 21/100\n",
            "7200/7200 [==============================] - 4s 538us/step - loss: 0.2637 - val_loss: 0.1775\n",
            "Epoch 22/100\n",
            "7200/7200 [==============================] - 4s 560us/step - loss: 0.2500 - val_loss: 0.1876\n",
            "Epoch 23/100\n",
            "7200/7200 [==============================] - 4s 570us/step - loss: 0.2512 - val_loss: 0.2291\n",
            "Epoch 24/100\n",
            "7200/7200 [==============================] - 4s 562us/step - loss: 0.2691 - val_loss: 0.1772\n",
            "Epoch 25/100\n",
            "7200/7200 [==============================] - 4s 540us/step - loss: 0.2403 - val_loss: 0.1788\n",
            "Epoch 26/100\n",
            "7200/7200 [==============================] - 4s 536us/step - loss: 0.2352 - val_loss: 0.1684\n",
            "Epoch 27/100\n",
            "7200/7200 [==============================] - 4s 548us/step - loss: 0.2170 - val_loss: 0.1610\n",
            "Epoch 28/100\n",
            "7200/7200 [==============================] - 4s 566us/step - loss: 0.1988 - val_loss: 0.1645\n",
            "Epoch 29/100\n",
            "7200/7200 [==============================] - 4s 549us/step - loss: 0.2000 - val_loss: 0.1597\n",
            "Epoch 30/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.2069 - val_loss: 0.1662\n",
            "Epoch 31/100\n",
            "7200/7200 [==============================] - 4s 544us/step - loss: 0.2036 - val_loss: 0.1629\n",
            "Epoch 32/100\n",
            "7200/7200 [==============================] - 4s 552us/step - loss: 0.1987 - val_loss: 0.1628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4da737bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCBDANgiPeCU",
        "colab_type": "text"
      },
      "source": [
        "### DistilBERT + DNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "outputId": "333bb2b4-dee5-4f88-fa15-2fd7c9df9d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "outputId": "6c18256f-f877-40d2-c8ca-78d7ddf51f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TBA: Hyperpameter Tuning Through lr, units & batch; TensorBoard for training process and/or model structure?\n",
        "\n",
        "units = 1024\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "batch = 32\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "c = concatenate([x, c])\n",
        "\n",
        "def FFUnit(c):\n",
        "  x = Dense(units)(c)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  c = concatenate([x, c])\n",
        "  return c\n",
        "\n",
        "for i in range(6):\n",
        "  c = FFUnit(c)\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=batch)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         787456      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024)         4096        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1024)         0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2048)         0           activation_1[0][0]               \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         2098176     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1024)         0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3072)         0           activation_2[0][0]               \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         3146752     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024)         4096        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1024)         0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 4096)         0           activation_3[0][0]               \n",
            "                                                                 concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1024)         0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 1024)         0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5120)         0           activation_4[0][0]               \n",
            "                                                                 concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         5243904     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 1024)         4096        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1024)         0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 1024)         0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 6144)         0           activation_5[0][0]               \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1024)         6292480     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 1024)         4096        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 1024)         0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 1024)         0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 7168)         0           activation_6[0][0]               \n",
            "                                                                 concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         7341056     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 1024)         4096        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1024)         0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 1024)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8192)         0           activation_7[0][0]               \n",
            "                                                                 concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 3)            24579       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 3)            12          dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 3)            0           batch_normalization_8[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 29,158,415\n",
            "Trainable params: 29,144,073\n",
            "Non-trainable params: 14,342\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7200/7200 [==============================] - 25s 3ms/step - loss: 0.3440 - acc: 0.9404 - val_loss: 0.2854 - val_acc: 0.9378\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2489 - acc: 0.9668 - val_loss: 0.2179 - val_acc: 0.9533\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2022 - acc: 0.9724 - val_loss: 0.1889 - val_acc: 0.9656\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1704 - acc: 0.9783 - val_loss: 0.2088 - val_acc: 0.9556\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1460 - acc: 0.9822 - val_loss: 0.1360 - val_acc: 0.9800\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1284 - acc: 0.9854 - val_loss: 0.1362 - val_acc: 0.9811\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1164 - acc: 0.9892 - val_loss: 0.1212 - val_acc: 0.9800\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1125 - acc: 0.9882 - val_loss: 0.1088 - val_acc: 0.9811\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0984 - acc: 0.9914 - val_loss: 0.1208 - val_acc: 0.9800\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0956 - acc: 0.9911 - val_loss: 0.1107 - val_acc: 0.9833\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0892 - acc: 0.9931 - val_loss: 0.0993 - val_acc: 0.9856\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0836 - acc: 0.9926 - val_loss: 0.0973 - val_acc: 0.9844\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0766 - acc: 0.9947 - val_loss: 0.0937 - val_acc: 0.9833\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0799 - acc: 0.9926 - val_loss: 0.0898 - val_acc: 0.9867\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0687 - acc: 0.9956 - val_loss: 0.0875 - val_acc: 0.9833\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0713 - acc: 0.9926 - val_loss: 0.0982 - val_acc: 0.9767\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0610 - acc: 0.9968 - val_loss: 0.1008 - val_acc: 0.9789\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0587 - acc: 0.9942 - val_loss: 0.0855 - val_acc: 0.9822\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0597 - acc: 0.9950 - val_loss: 0.0866 - val_acc: 0.9844\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0545 - acc: 0.9961 - val_loss: 0.0728 - val_acc: 0.9878\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0519 - acc: 0.9960 - val_loss: 0.0724 - val_acc: 0.9844\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0488 - acc: 0.9969 - val_loss: 0.0712 - val_acc: 0.9878\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0512 - acc: 0.9956 - val_loss: 0.0731 - val_acc: 0.9844\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0500 - acc: 0.9949 - val_loss: 0.0845 - val_acc: 0.9822\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0419 - acc: 0.9974 - val_loss: 0.0653 - val_acc: 0.9867\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0412 - acc: 0.9965 - val_loss: 0.0956 - val_acc: 0.9767\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0412 - acc: 0.9962 - val_loss: 0.0755 - val_acc: 0.9844\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0363 - acc: 0.9981 - val_loss: 0.0693 - val_acc: 0.9844\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0395 - acc: 0.9962 - val_loss: 0.0640 - val_acc: 0.9856\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0366 - acc: 0.9961 - val_loss: 0.0637 - val_acc: 0.9822\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0326 - acc: 0.9975 - val_loss: 0.0609 - val_acc: 0.9867\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0289 - acc: 0.9983 - val_loss: 0.0946 - val_acc: 0.9756\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0288 - acc: 0.9981 - val_loss: 0.0621 - val_acc: 0.9844\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0290 - acc: 0.9976 - val_loss: 0.0760 - val_acc: 0.9778\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0288 - acc: 0.9968 - val_loss: 0.0621 - val_acc: 0.9822\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0243 - acc: 0.9979 - val_loss: 0.0538 - val_acc: 0.9889\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0252 - acc: 0.9978 - val_loss: 0.0528 - val_acc: 0.9844\n",
            "Epoch 33/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0246 - acc: 0.9976 - val_loss: 0.0640 - val_acc: 0.9867\n",
            "Epoch 34/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0207 - acc: 0.9992 - val_loss: 0.0484 - val_acc: 0.9867\n",
            "Epoch 35/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0206 - acc: 0.9981 - val_loss: 0.0610 - val_acc: 0.9767\n",
            "Epoch 36/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0218 - acc: 0.9975 - val_loss: 0.0651 - val_acc: 0.9822\n",
            "Epoch 37/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0200 - acc: 0.9983 - val_loss: 0.0654 - val_acc: 0.9811\n",
            "Epoch 38/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0175 - acc: 0.9993 - val_loss: 0.0620 - val_acc: 0.9811\n",
            "Epoch 39/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0182 - acc: 0.9983 - val_loss: 0.0559 - val_acc: 0.9856\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 17s 2ms/step - loss: 0.0218 - acc: 0.9979 - val_loss: 0.0466 - val_acc: 0.9900\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0199 - acc: 0.9989 - val_loss: 0.0466 - val_acc: 0.9867\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0178 - acc: 0.9994 - val_loss: 0.0509 - val_acc: 0.9878\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0170 - acc: 0.9992 - val_loss: 0.0484 - val_acc: 0.9844\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0174 - acc: 0.9988 - val_loss: 0.0533 - val_acc: 0.9856\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 13s 2ms/step - loss: 0.0159 - acc: 0.9994 - val_loss: 0.0490 - val_acc: 0.9867\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 183us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05900631822645664, 0.9844444444444445]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_old_text = pd.DataFrame(X_old_text)\n",
        "X_new_text = pd.DataFrame(X_new_text)\n",
        "\n",
        "Pred_old = model.predict(X_old_text)\n",
        "Pred_new = model.predict(X_new_text)\n",
        "\n",
        "# X1_text = pd.DataFrame(X1_text)\n",
        "# X2_text = pd.DataFrame(X2_text)\n",
        "# X3_text = pd.DataFrame(X3_text)\n",
        "\n",
        "# Pred1 = model.predict(X1_text)[:, 2]\n",
        "# Pred2 = model.predict(X2_text)[:, 2]\n",
        "# Pred3 = model.predict(X3_text)[:, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X1GpzcNR9bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df['Rand_donor_style'] = Pred1\n",
        "# df['Rand_117M_10000_Nabokov_All_3_style'] = Pred2\n",
        "# df['Rand_unigram_style'] = Pred3\n",
        "# df.to_csv('./gdrive/My Drive/DL/Style/rand_examples.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGIYZt1SWc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "pred_old, pred_new, pred_delta = [], [], []\n",
        "for i in range(len(X_old)): \n",
        "  if df.label.iloc[i][2] == 'A':\n",
        "    pred_old.append(Pred_old[i][0])\n",
        "    pred_new.append(Pred_new[i][0])\n",
        "  elif df.label.iloc[i][2] == 'D':\n",
        "    pred_old.append(Pred_old[i][1])\n",
        "    pred_new.append(Pred_new[i][1])\n",
        "  else:\n",
        "    pred_old.append(Pred_old[i][2])\n",
        "    pred_new.append(Pred_new[i][2])\n",
        "  pred_delta.append(pred_new[i] - pred_old[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPspREI2S7_r",
        "colab_type": "code",
        "outputId": "17408b31-e342-48f7-9fe6-f8feb82f9731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_new"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00203248, 0.00404928, 0.9939182 ],\n",
              "       [0.00727922, 0.00358726, 0.98913354],\n",
              "       [0.7721936 , 0.0139337 , 0.2138727 ],\n",
              "       [0.00891649, 0.00448649, 0.986597  ],\n",
              "       [0.1125346 , 0.00660644, 0.8808589 ],\n",
              "       [0.00478178, 0.12722215, 0.86799604],\n",
              "       [0.00129496, 0.00179181, 0.9969132 ],\n",
              "       [0.9057036 , 0.0124633 , 0.08183308],\n",
              "       [0.0044654 , 0.01962554, 0.97590905],\n",
              "       [0.01275224, 0.37084013, 0.61640763],\n",
              "       [0.00131232, 0.00184203, 0.99684566],\n",
              "       [0.00157802, 0.00203662, 0.99638534],\n",
              "       [0.06523793, 0.00728321, 0.9274789 ],\n",
              "       [0.0049958 , 0.01522104, 0.97978324],\n",
              "       [0.00574131, 0.00443773, 0.98982096],\n",
              "       [0.00687255, 0.39102003, 0.60210735],\n",
              "       [0.00370327, 0.02648717, 0.96980953],\n",
              "       [0.4538185 , 0.5200149 , 0.02616665],\n",
              "       [0.00597845, 0.89414316, 0.09987843],\n",
              "       [0.00557098, 0.21227232, 0.7821567 ],\n",
              "       [0.00246911, 0.00275852, 0.9947724 ],\n",
              "       [0.0138345 , 0.00573398, 0.9804315 ],\n",
              "       [0.99641466, 0.00144496, 0.00214028],\n",
              "       [0.99653745, 0.00132717, 0.00213536],\n",
              "       [0.9788658 , 0.01504812, 0.00608605],\n",
              "       [0.9936702 , 0.00241597, 0.0039138 ],\n",
              "       [0.9908352 , 0.00381392, 0.00535089],\n",
              "       [0.99080664, 0.00326382, 0.00592948],\n",
              "       [0.9839093 , 0.00546658, 0.0106241 ],\n",
              "       [0.78060657, 0.01258628, 0.20680715],\n",
              "       [0.98345435, 0.00465805, 0.01188755],\n",
              "       [0.9651136 , 0.00677279, 0.0281136 ],\n",
              "       [0.3471334 , 0.00445799, 0.6484086 ],\n",
              "       [0.9960854 , 0.0016397 , 0.00227486],\n",
              "       [0.9703579 , 0.00355378, 0.02608839],\n",
              "       [0.13482103, 0.01107593, 0.854103  ],\n",
              "       [0.00599374, 0.9904757 , 0.00353051],\n",
              "       [0.00191586, 0.9896975 , 0.00838658],\n",
              "       [0.9291923 , 0.06178744, 0.00902024],\n",
              "       [0.26015538, 0.72951996, 0.01032462],\n",
              "       [0.36211988, 0.6236394 , 0.01424073],\n",
              "       [0.24055268, 0.750287  , 0.00916029],\n",
              "       [0.01278337, 0.9841526 , 0.003064  ],\n",
              "       [0.11760683, 0.8728448 , 0.00954831],\n",
              "       [0.10767117, 0.8812528 , 0.01107599],\n",
              "       [0.00217144, 0.9885666 , 0.00926204],\n",
              "       [0.00230702, 0.9871459 , 0.01054701],\n",
              "       [0.9927698 , 0.00266026, 0.00456993],\n",
              "       [0.00191262, 0.9965049 , 0.00158248],\n",
              "       [0.00338391, 0.99305266, 0.00356337],\n",
              "       [0.0410355 , 0.82982755, 0.129137  ],\n",
              "       [0.09062817, 0.8946161 , 0.01475569],\n",
              "       [0.0458078 , 0.80870193, 0.14549027],\n",
              "       [0.00164791, 0.9965995 , 0.00175255],\n",
              "       [0.00889827, 0.9310496 , 0.06005207]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLh1fA2cVQF",
        "colab_type": "code",
        "outputId": "bec3a638-6d58-457a-f169-f1658630209b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_old"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.7181025e-03, 2.0850466e-03, 9.9619687e-01],\n",
              "       [1.0057823e-03, 1.9193310e-03, 9.9707484e-01],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9726260e-01, 1.2140428e-03, 1.5233783e-03],\n",
              "       [9.7003227e-01, 5.3322855e-03, 2.4635507e-02],\n",
              "       [9.9176776e-01, 3.1889551e-03, 5.0432472e-03],\n",
              "       [9.9118304e-01, 3.5566376e-03, 5.2603288e-03],\n",
              "       [9.9802750e-01, 7.4436510e-04, 1.2280226e-03],\n",
              "       [9.9561691e-01, 1.7827221e-03, 2.6004734e-03],\n",
              "       [9.9689227e-01, 1.2624987e-03, 1.8452296e-03],\n",
              "       [7.7323711e-01, 2.1403381e-01, 1.2729074e-02],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [2.1795547e-03, 9.9624926e-01, 1.5712433e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHeYgeqDcpdP",
        "colab_type": "code",
        "outputId": "242fef9d-04b1-4be2-b45c-1992dee5e275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "pred_delta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0006469997,\n",
              " 0.0062674712,\n",
              " 0.7692856,\n",
              " 0.0075082732,\n",
              " 0.11091142,\n",
              " 0.0035563535,\n",
              " 0.0001149649,\n",
              " 0.8701338,\n",
              " 0.0013317978,\n",
              " 0.010962961,\n",
              " 0.0003645725,\n",
              " 0.0007117655,\n",
              " 0.005052993,\n",
              " 0.013393214,\n",
              " 0.0025015022,\n",
              " 0.38927066,\n",
              " 0.025058856,\n",
              " 0.5102656,\n",
              " 0.88618094,\n",
              " 0.20976229,\n",
              " 0.0006734743,\n",
              " 0.0038146484,\n",
              " -0.00038100418,\n",
              " -0.00039460126,\n",
              " 0.013222156,\n",
              " 0.0006942038,\n",
              " 0.0022851413,\n",
              " 0.0036058815,\n",
              " 0.009100723,\n",
              " 0.18217164,\n",
              " 0.0068443003,\n",
              " 0.02285327,\n",
              " 0.64718056,\n",
              " -0.00032561668,\n",
              " 0.024243161,\n",
              " 0.841374,\n",
              " 0.004301003,\n",
              " 0.00018604274,\n",
              " 0.92801166,\n",
              " 0.2587181,\n",
              " 0.36065552,\n",
              " 0.2392447,\n",
              " 0.010925005,\n",
              " 0.11558467,\n",
              " 0.10364332,\n",
              " 0.0069973934,\n",
              " 0.0052578673,\n",
              " 0.0022290314,\n",
              " 0.00016624876,\n",
              " 0.002378278,\n",
              " 0.1277139,\n",
              " 0.012937957,\n",
              " 0.1438997,\n",
              " -0.0009582578,\n",
              " 0.05848083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0byXlehgT07U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/arrow_plot_results.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['donor_authorship_score'] = pred_old\n",
        "df['style_authorship_score'] = pred_new\n",
        "df['authorship_delta'] = pred_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWN5M4qT74-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.to_csv('./gdrive/My Drive/DL/Style/all_scores.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KxX5ljnUVgL",
        "colab_type": "code",
        "outputId": "b378a588-b41c-4247-f75f-6d804100da3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "AUCfig = plt.figure()\n",
        "\n",
        "# Binarize the multiclass labels\n",
        "Y_bi = label_binarize(y, classes=['Austen', 'Dumas', 'Nabokov'])\n",
        "n_classes = Y_bi.shape[1]\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "lw = 2\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.3f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# plt.title('ROC curve for each class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('Arrows.pdf')\n",
        "files.download('Arrows.pdf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bvUsLAzabQU",
        "colab_type": "code",
        "outputId": "95a91ac5-282d-4719-a986-54e9497e1a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/all_scores.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>old</th>\n",
              "      <th>new</th>\n",
              "      <th>old_masked</th>\n",
              "      <th>new_masked</th>\n",
              "      <th>label</th>\n",
              "      <th>donor_naturalness_score</th>\n",
              "      <th>style_naturalness_score</th>\n",
              "      <th>naturalness_delta</th>\n",
              "      <th>donor_authorship_score</th>\n",
              "      <th>style_authorship_score</th>\n",
              "      <th>authorship_delta</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, to look at it, and his fem...</td>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, to look at it, and his fe...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.990052</td>\n",
              "      <td>0.987915</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>0.001385</td>\n",
              "      <td>0.002032</td>\n",
              "      <td>0.000647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pal Palych! How’re they biting?” He looked up...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked ...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.960177</td>\n",
              "      <td>0.992160</td>\n",
              "      <td>-0.031982</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.007279</td>\n",
              "      <td>0.006267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony above...</td>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony abov...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.986264</td>\n",
              "      <td>0.994932</td>\n",
              "      <td>-0.008668</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>0.772194</td>\n",
              "      <td>0.769286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter ...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.959302</td>\n",
              "      <td>0.990870</td>\n",
              "      <td>-0.031567</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.007508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not a Portrait of Lolita: the differences bet...</td>\n",
              "      <td>not a Portrait of Lolita: the differences betw...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.972211</td>\n",
              "      <td>0.994189</td>\n",
              "      <td>-0.021978</td>\n",
              "      <td>0.001623</td>\n",
              "      <td>0.112535</td>\n",
              "      <td>0.110911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoons...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.992910</td>\n",
              "      <td>0.992968</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.004782</td>\n",
              "      <td>0.003556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on ...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.975876</td>\n",
              "      <td>0.994505</td>\n",
              "      <td>-0.018629</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.000115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. A...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035570</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>0.870134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Ministe...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.967726</td>\n",
              "      <td>0.315845</td>\n",
              "      <td>0.651881</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>0.004465</td>\n",
              "      <td>0.001332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my sternum, an...</td>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my sternum, a...</td>\n",
              "      <td>N2A</td>\n",
              "      <td>0.942770</td>\n",
              "      <td>0.995724</td>\n",
              "      <td>-0.052954</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.012752</td>\n",
              "      <td>0.010963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, with a sigh, and his femin...</td>\n",
              "      <td>stepped back a bit, as if admiring it, and hi...</td>\n",
              "      <td>stepped back a bit, with a sigh, and his femi...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.990052</td>\n",
              "      <td>0.877143</td>\n",
              "      <td>0.112909</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.000365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Pal Palych! How’re they biting?” He looked up...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked ...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>Pal Palych! How ’ re they biting? ” He looked...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.960177</td>\n",
              "      <td>0.986106</td>\n",
              "      <td>-0.025929</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.000712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony, and ...</td>\n",
              "      <td>but at least it revealed a small balcony one ...</td>\n",
              "      <td>but at least it revealed a small balcony, and...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.986264</td>\n",
              "      <td>0.991506</td>\n",
              "      <td>-0.005242</td>\n",
              "      <td>0.002230</td>\n",
              "      <td>0.007283</td>\n",
              "      <td>0.005053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter ...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>had not been placed together, told the waiter...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.959302</td>\n",
              "      <td>0.986418</td>\n",
              "      <td>-0.027116</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.015221</td>\n",
              "      <td>0.013393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>not a Portrait of Lolita: the differences bet...</td>\n",
              "      <td>not a Portrait of Lolita: the differences betw...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>not a Portrait of &lt;MASK&gt;: the differences bet...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.972211</td>\n",
              "      <td>0.993964</td>\n",
              "      <td>-0.021753</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.004438</td>\n",
              "      <td>0.002502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoons...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>his right, the way Anglo-Saxons do in cartoon...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.992910</td>\n",
              "      <td>0.994314</td>\n",
              "      <td>-0.001403</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.391020</td>\n",
              "      <td>0.389271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. The king walked r...</td>\n",
              "      <td>in a gigantic, joyous world. A tall pillar on...</td>\n",
              "      <td>in a gigantic, joyous world. The king walked ...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.975876</td>\n",
              "      <td>0.968510</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.026487</td>\n",
              "      <td>0.025059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. A...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>traversed by the black bend-let of a branch. ...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009749</td>\n",
              "      <td>0.520015</td>\n",
              "      <td>0.510266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Ministe...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>cross the frontier. One of the Cabinet Minist...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.967726</td>\n",
              "      <td>0.884055</td>\n",
              "      <td>0.083671</td>\n",
              "      <td>0.007962</td>\n",
              "      <td>0.894143</td>\n",
              "      <td>0.886181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my legs stiffe...</td>\n",
              "      <td>lawn, but within my thorax, and my organs swa...</td>\n",
              "      <td>lawn, but within my thorax, and my legs stiff...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.942770</td>\n",
              "      <td>0.820391</td>\n",
              "      <td>0.122379</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.212272</td>\n",
              "      <td>0.209762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>time it slipped off) while you stand with outs...</td>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>time it slipped off) while you stand with out...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.994646</td>\n",
              "      <td>0.994646</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>0.002759</td>\n",
              "      <td>0.000673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>her finger on the switch—a half-interrogative,...</td>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>her finger on the switch—a half-interrogative...</td>\n",
              "      <td>N2D</td>\n",
              "      <td>0.979933</td>\n",
              "      <td>0.962131</td>\n",
              "      <td>0.017801</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.005734</td>\n",
              "      <td>0.003815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2D1</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.993718</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>-0.000381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. Two of her girls had been upon the roa...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. Two of her girls had been upon the ro...</td>\n",
              "      <td>A2D1</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.976373</td>\n",
              "      <td>-0.069334</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>-0.000395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2D2</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.977014</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.015048</td>\n",
              "      <td>0.013222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. A little while after she rose, dressed...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. A little while after she rose, dresse...</td>\n",
              "      <td>A2D2</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.994970</td>\n",
              "      <td>-0.087931</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.000694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>of Jane Fairfax! The interest he takes in her...</td>\n",
              "      <td>of Jane Fairfax! The interest he takes in her ...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>of &lt;MASK&gt; &lt;MASK&gt;! The interest he takes in he...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.994255</td>\n",
              "      <td>0.993180</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.003066</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.002285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. The other girl, Number Two, walked bri...</td>\n",
              "      <td>sister. Two of her girls had been upon the po...</td>\n",
              "      <td>sister. The other girl, Number Two, walked br...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.907039</td>\n",
              "      <td>0.995685</td>\n",
              "      <td>-0.088646</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>0.003606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>whose arrival was often as sudden, if not qui...</td>\n",
              "      <td>whose arrival was often as sudden, and as usua...</td>\n",
              "      <td>whose arrival was often as sudden, if not qui...</td>\n",
              "      <td>whose arrival was often as sudden, and as usu...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.991857</td>\n",
              "      <td>0.995415</td>\n",
              "      <td>-0.003558</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>0.010624</td>\n",
              "      <td>0.009101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Lydia's society she was of course carefully k...</td>\n",
              "      <td>Lydia's society she was of course carefully ke...</td>\n",
              "      <td>&lt;MASK&gt; 's society she was of course carefully...</td>\n",
              "      <td>&lt;MASK&gt; 's society she was of course carefully...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.995624</td>\n",
              "      <td>0.954530</td>\n",
              "      <td>0.041094</td>\n",
              "      <td>0.024636</td>\n",
              "      <td>0.206807</td>\n",
              "      <td>0.182172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>and positively declared, that he would still ...</td>\n",
              "      <td>and positively declared, �I�m going to love hi...</td>\n",
              "      <td>and positively declared, that he would still ...</td>\n",
              "      <td>and positively declared, �I�m going to love h...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.986645</td>\n",
              "      <td>0.955920</td>\n",
              "      <td>0.030725</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.011888</td>\n",
              "      <td>0.006844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>her, she might not have persuaded herself into...</td>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>her, she might not have persuaded herself int...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.989389</td>\n",
              "      <td>0.992236</td>\n",
              "      <td>-0.002847</td>\n",
              "      <td>0.005260</td>\n",
              "      <td>0.028114</td>\n",
              "      <td>0.022853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>by Fanny.  “I am very sorry,” said she; “it i...</td>\n",
              "      <td>by Fanny. �I�m terribly sorry, � said she; �it...</td>\n",
              "      <td>by &lt;MASK&gt;. “ I am very sorry, ” said she; “ i...</td>\n",
              "      <td>by &lt;MASK&gt;. �I�m terribly sorry, � said she; �...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.972956</td>\n",
              "      <td>0.995057</td>\n",
              "      <td>-0.022101</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.648409</td>\n",
              "      <td>0.647181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>daughters might do as they pleased.  But they...</td>\n",
              "      <td>daughters might do as they pleased. But they h...</td>\n",
              "      <td>daughters might do as they pleased. But they ...</td>\n",
              "      <td>daughters might do as they pleased. But they ...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.991757</td>\n",
              "      <td>0.991757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>-0.000326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>of _planning_ judged of by the day at Sotherto...</td>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>of _planning_ judged of by the day at Sothert...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.983739</td>\n",
              "      <td>0.679974</td>\n",
              "      <td>0.303764</td>\n",
              "      <td>0.001845</td>\n",
              "      <td>0.026088</td>\n",
              "      <td>0.024243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>plainly that it ended no better than it began....</td>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>plainly that it ended no better than it began...</td>\n",
              "      <td>A2N</td>\n",
              "      <td>0.996398</td>\n",
              "      <td>0.983931</td>\n",
              "      <td>0.012467</td>\n",
              "      <td>0.012729</td>\n",
              "      <td>0.854103</td>\n",
              "      <td>0.841374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>\"What! arrest you, my most faithful servant?\"...</td>\n",
              "      <td>What! If the officer is of the line, my most f...</td>\n",
              "      <td>`` What! arrest you, my most faithful servant...</td>\n",
              "      <td>What! If the officer is of the line, my most ...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.244483</td>\n",
              "      <td>0.992559</td>\n",
              "      <td>-0.748076</td>\n",
              "      <td>0.001693</td>\n",
              "      <td>0.005994</td>\n",
              "      <td>0.004301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>said D’Artagnan, “but as he is a thoughtful y...</td>\n",
              "      <td>said D ’ Artagnan, he is a very capable young ...</td>\n",
              "      <td>said D ’ &lt;MASK&gt;, “ but as he is a thoughtful ...</td>\n",
              "      <td>said &lt;MASK&gt; &lt;MASK&gt; &lt;MASK&gt;, he is a very capab...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.966850</td>\n",
              "      <td>-0.002492</td>\n",
              "      <td>0.001730</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.000186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>must be married in order to be respected.”  “...</td>\n",
              "      <td>must be married in order to be respected. So y...</td>\n",
              "      <td>must be married in order to be respected. ” “...</td>\n",
              "      <td>must be married in order to be respected. So ...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.985592</td>\n",
              "      <td>0.993826</td>\n",
              "      <td>-0.008234</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.929192</td>\n",
              "      <td>0.928012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and something more, had arranged the...</td>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and something more, had arranged th...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.960746</td>\n",
              "      <td>0.994518</td>\n",
              "      <td>-0.033772</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.260155</td>\n",
              "      <td>0.258718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why, ...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.996894</td>\n",
              "      <td>0.989974</td>\n",
              "      <td>0.006920</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.362120</td>\n",
              "      <td>0.360656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>you evaded giving me an answer.”  “And what r...</td>\n",
              "      <td>you evaded giving me an answer. ” “I cannot im...</td>\n",
              "      <td>you evaded giving me an answer. ” “ And what ...</td>\n",
              "      <td>you evaded giving me an answer. ” “ I can not...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.978038</td>\n",
              "      <td>0.998955</td>\n",
              "      <td>-0.020918</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.240553</td>\n",
              "      <td>0.239245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>\"Therefore no one inhabits it; only, you see...</td>\n",
              "      <td>Therefore no one inhabits it; I think I see no...</td>\n",
              "      <td>`` Therefore no one inhabits it; only, you se...</td>\n",
              "      <td>Therefore no one inhabits it; I think I see n...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.998624</td>\n",
              "      <td>0.955227</td>\n",
              "      <td>0.043397</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>0.010925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>while in Paris?”  “Arrest the Duke! Arrest th...</td>\n",
              "      <td>while in Paris? ” “Arrest the Duke! No, I shou...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! Arrest t...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! No, I sh...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.285652</td>\n",
              "      <td>0.808030</td>\n",
              "      <td>-0.522378</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.117607</td>\n",
              "      <td>0.115585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>need their assistance.  Meanwhile, carried aw...</td>\n",
              "      <td>need their assistance. And so, upon the subjec...</td>\n",
              "      <td>need their assistance. Meanwhile, carried awa...</td>\n",
              "      <td>need their assistance. And so, upon the subje...</td>\n",
              "      <td>D2A</td>\n",
              "      <td>0.980184</td>\n",
              "      <td>0.990127</td>\n",
              "      <td>-0.009943</td>\n",
              "      <td>0.004028</td>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.103643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>\"What! arrest you, my most faithful servant?\"...</td>\n",
              "      <td>What! You�ll arrest me, my most faithful serva...</td>\n",
              "      <td>`` What! arrest you, my most faithful servant...</td>\n",
              "      <td>What! You�ll arrest me, my most faithful serv...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.244483</td>\n",
              "      <td>0.143257</td>\n",
              "      <td>0.101226</td>\n",
              "      <td>0.002265</td>\n",
              "      <td>0.009262</td>\n",
              "      <td>0.006997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>said D’Artagnan, “but as he is a thoughtful y...</td>\n",
              "      <td>said D � Artagnan, �but as he is a thoughtful ...</td>\n",
              "      <td>said D ’ &lt;MASK&gt;, “ but as he is a thoughtful ...</td>\n",
              "      <td>said &lt;MASK&gt; &lt;MASK&gt; &lt;MASK&gt;, �but as he is a th...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.964358</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005289</td>\n",
              "      <td>0.010547</td>\n",
              "      <td>0.005258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>must be married in order to be respected.”  “...</td>\n",
              "      <td>must be married in order to be respected. � �F...</td>\n",
              "      <td>must be married in order to be respected. ” “...</td>\n",
              "      <td>must be married in order to be respected. � �...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.985592</td>\n",
              "      <td>0.991795</td>\n",
              "      <td>-0.006203</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.004570</td>\n",
              "      <td>0.002229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and that is all, had arranged the co...</td>\n",
              "      <td>of honor, and with all its accompanying detai...</td>\n",
              "      <td>of honor, and that is all, had arranged the c...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.960746</td>\n",
              "      <td>0.977840</td>\n",
              "      <td>-0.017094</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.000166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why, ...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>they so continuously occupied about her? Why,...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.996895</td>\n",
              "      <td>0.996386</td>\n",
              "      <td>0.000508</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>0.003563</td>\n",
              "      <td>0.002378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>you evaded giving me an answer.”  “And what r...</td>\n",
              "      <td>you evaded giving me an answer. � �Why did you...</td>\n",
              "      <td>you evaded giving me an answer. ” “ And what ...</td>\n",
              "      <td>you evaded giving me an answer. � �Why did yo...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.978038</td>\n",
              "      <td>0.878864</td>\n",
              "      <td>0.099174</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.129137</td>\n",
              "      <td>0.127714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>\"Therefore no one inhabits it; only, you see...</td>\n",
              "      <td>Therefore no one inhabits it; I, as you, know ...</td>\n",
              "      <td>`` Therefore no one inhabits it; only, you se...</td>\n",
              "      <td>Therefore no one inhabits it; I, as you, know...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.998624</td>\n",
              "      <td>0.957601</td>\n",
              "      <td>0.041023</td>\n",
              "      <td>0.001818</td>\n",
              "      <td>0.014756</td>\n",
              "      <td>0.012938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>while in Paris?”  “Arrest the Duke! Arrest th...</td>\n",
              "      <td>while in Paris? � �Arrest the Duke! � at the D...</td>\n",
              "      <td>while in Paris? ” “ Arrest the Duke! Arrest t...</td>\n",
              "      <td>while in Paris? � �Arrest the Duke! � at the ...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.285652</td>\n",
              "      <td>0.993423</td>\n",
              "      <td>-0.707771</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.145490</td>\n",
              "      <td>0.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>need their assistance.  Meanwhile, carried aw...</td>\n",
              "      <td>need their assistance. We know the way our nar...</td>\n",
              "      <td>need their assistance. Meanwhile, carried awa...</td>\n",
              "      <td>need their assistance. We know the way our na...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.980184</td>\n",
              "      <td>0.983349</td>\n",
              "      <td>-0.003164</td>\n",
              "      <td>0.002711</td>\n",
              "      <td>0.001753</td>\n",
              "      <td>-0.000958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>the alarm, the light was extinguished, the la...</td>\n",
              "      <td>the alarm, and the light went out, the ladder ...</td>\n",
              "      <td>the alarm, the light was extinguished, the la...</td>\n",
              "      <td>the alarm, and the light went out, the ladder...</td>\n",
              "      <td>D2N</td>\n",
              "      <td>0.992641</td>\n",
              "      <td>0.984692</td>\n",
              "      <td>0.007949</td>\n",
              "      <td>0.001571</td>\n",
              "      <td>0.060052</td>\n",
              "      <td>0.058481</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          old  ... authorship_delta\n",
              "Unnamed: 0                                                     ...                 \n",
              "0            stepped back a bit, as if admiring it, and hi...  ...         0.000647\n",
              "1            Pal Palych! How’re they biting?” He looked up...  ...         0.006267\n",
              "2            but at least it revealed a small balcony one ...  ...         0.769286\n",
              "3            had not been placed together, told the waiter...  ...         0.007508\n",
              "4            not a Portrait of Lolita: the differences bet...  ...         0.110911\n",
              "5            his right, the way Anglo-Saxons do in cartoon...  ...         0.003556\n",
              "6            in a gigantic, joyous world. A tall pillar on...  ...         0.000115\n",
              "7            traversed by the black bend-let of a branch. ...  ...         0.870134\n",
              "8            cross the frontier. One of the Cabinet Minist...  ...         0.001332\n",
              "9            lawn, but within my thorax, and my organs swa...  ...         0.010963\n",
              "10           stepped back a bit, as if admiring it, and hi...  ...         0.000365\n",
              "11           Pal Palych! How’re they biting?” He looked up...  ...         0.000712\n",
              "12           but at least it revealed a small balcony one ...  ...         0.005053\n",
              "13           had not been placed together, told the waiter...  ...         0.013393\n",
              "14           not a Portrait of Lolita: the differences bet...  ...         0.002502\n",
              "15           his right, the way Anglo-Saxons do in cartoon...  ...         0.389271\n",
              "16           in a gigantic, joyous world. A tall pillar on...  ...         0.025059\n",
              "17           traversed by the black bend-let of a branch. ...  ...         0.510266\n",
              "18           cross the frontier. One of the Cabinet Minist...  ...         0.886181\n",
              "19           lawn, but within my thorax, and my organs swa...  ...         0.209762\n",
              "20           time it slipped off) while you stand with out...  ...         0.000673\n",
              "21           her finger on the switch—a half-interrogative...  ...         0.003815\n",
              "22           of Jane Fairfax! The interest he takes in her...  ...        -0.000381\n",
              "23           sister. Two of her girls had been upon the po...  ...        -0.000395\n",
              "24           of Jane Fairfax! The interest he takes in her...  ...         0.013222\n",
              "25           sister. Two of her girls had been upon the po...  ...         0.000694\n",
              "26           of Jane Fairfax! The interest he takes in her...  ...         0.002285\n",
              "27           sister. Two of her girls had been upon the po...  ...         0.003606\n",
              "28           whose arrival was often as sudden, if not qui...  ...         0.009101\n",
              "29           Lydia's society she was of course carefully k...  ...         0.182172\n",
              "30           and positively declared, that he would still ...  ...         0.006844\n",
              "31           her, she might not have persuaded herself int...  ...         0.022853\n",
              "32           by Fanny.  “I am very sorry,” said she; “it i...  ...         0.647181\n",
              "33           daughters might do as they pleased.  But they...  ...        -0.000326\n",
              "34           of _planning_ judged of by the day at Sothert...  ...         0.024243\n",
              "35           plainly that it ended no better than it began...  ...         0.841374\n",
              "36           \"What! arrest you, my most faithful servant?\"...  ...         0.004301\n",
              "37           said D’Artagnan, “but as he is a thoughtful y...  ...         0.000186\n",
              "38           must be married in order to be respected.”  “...  ...         0.928012\n",
              "39           of honor, and with all its accompanying detai...  ...         0.258718\n",
              "40           they so continuously occupied about her? Why,...  ...         0.360656\n",
              "41           you evaded giving me an answer.”  “And what r...  ...         0.239245\n",
              "42            \"Therefore no one inhabits it; only, you see...  ...         0.010925\n",
              "43           while in Paris?”  “Arrest the Duke! Arrest th...  ...         0.115585\n",
              "44           need their assistance.  Meanwhile, carried aw...  ...         0.103643\n",
              "45           \"What! arrest you, my most faithful servant?\"...  ...         0.006997\n",
              "46           said D’Artagnan, “but as he is a thoughtful y...  ...         0.005258\n",
              "47           must be married in order to be respected.”  “...  ...         0.002229\n",
              "48           of honor, and with all its accompanying detai...  ...         0.000166\n",
              "49           they so continuously occupied about her? Why,...  ...         0.002378\n",
              "50           you evaded giving me an answer.”  “And what r...  ...         0.127714\n",
              "51            \"Therefore no one inhabits it; only, you see...  ...         0.012938\n",
              "52           while in Paris?”  “Arrest the Duke! Arrest th...  ...         0.143900\n",
              "53           need their assistance.  Meanwhile, carried aw...  ...        -0.000958\n",
              "54           the alarm, the light was extinguished, the la...  ...         0.058481\n",
              "\n",
              "[55 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYs6lLFID3R",
        "colab_type": "code",
        "outputId": "f842f7bd-16e7-4e25-ebf1-dd226a7f77c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['style_naturalness_score'].values\n",
        "\n",
        "u = df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "#plt.axis('equal')\n",
        "plt.quiver(x,u,y,v)\n",
        "plt.xlabel('Naturalness')\n",
        "plt.ylabel('Style Shift')\n",
        "plt.xlim(left=0.89, right=1.01)\n",
        "plt.ylim(top=0.04, bottom=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEQCAYAAAB80zltAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0us2zadbdKG",
        "colab_type": "code",
        "outputId": "65236382-8f96-4f6b-a66d-be6ae2c5972a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['donor_authorship_score'].values\n",
        "\n",
        "u = - df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "Arrows = plt.figure()\n",
        "for i, j, k, l in zip(x, y, u, v): \n",
        "  plt.arrow(i, j, k, l, head_width=0.02, head_length=0.04)\n",
        "plt.xlabel('Naturalness Score')\n",
        "plt.ylabel('Style Score')\n",
        "plt.xlim(left=0.1, right=1.05)\n",
        "plt.ylim(top=1, bottom=-0.1)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "plt.savefig('Arrows.pdf')\n",
        "files.download('Arrows.pdf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEQCAYAAABFtIg2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8yKRNf6C5eK",
        "colab_type": "code",
        "outputId": "a951cf2d-df1f-4ef5-e249-fac5eae68fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "plt.savefig(\"abc.png\")\n",
        "files.download(\"abc.png\") \n",
        "\n",
        "plt.savefig(('./gdrive/My Drive/DL/Style/arrows.pdf'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP-xHdpvHhBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}