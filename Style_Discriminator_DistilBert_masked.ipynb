{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert_masked.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "718e313b-8786-4da3-d4b7-002563c30076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\r\u001b[K     |█                               | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 41.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=8868965e6eda000795aeb4917754f4a2f3f70dfd51865c88fc569849364ab8e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, regex, sentencepiece, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "150b1a0d-1528-4fb3-e8fa-f7f7d86eb035"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "outputId": "41ac38ff-64ec-4fc1-f927-dcd4c6d1b818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, without masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "# lbl_enc = preprocessing.LabelEncoder()\n",
        "# y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkqPiy6v2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, with masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "203f0dfc-d526-4d0c-8d83-7864dbf965ba"
      },
      "source": [
        "model = Embeddings()\n",
        "\n",
        "X_text = []\n",
        "for sentence in tqdm(X):\n",
        "    X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2670653.60B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 76004.00B/s]\n",
            "100%|██████████| 440473133/440473133 [00:08<00:00, 50072121.61B/s]\n",
            "100%|██████████| 9000/9000 [1:19:40<00:00,  1.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplxzc29EEzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "035dcc0a-837d-4a20-dc00-61fe1f69cfee"
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv')\n",
        "X_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.089255</td>\n",
              "      <td>0.474704</td>\n",
              "      <td>0.171598</td>\n",
              "      <td>-0.445951</td>\n",
              "      <td>-0.139031</td>\n",
              "      <td>-0.095083</td>\n",
              "      <td>0.398022</td>\n",
              "      <td>0.262700</td>\n",
              "      <td>0.213528</td>\n",
              "      <td>-0.004183</td>\n",
              "      <td>0.576964</td>\n",
              "      <td>-0.121572</td>\n",
              "      <td>-0.239475</td>\n",
              "      <td>0.293347</td>\n",
              "      <td>-0.084161</td>\n",
              "      <td>0.894064</td>\n",
              "      <td>0.147231</td>\n",
              "      <td>0.401929</td>\n",
              "      <td>0.129791</td>\n",
              "      <td>0.258735</td>\n",
              "      <td>0.685502</td>\n",
              "      <td>0.079295</td>\n",
              "      <td>0.334323</td>\n",
              "      <td>1.034874</td>\n",
              "      <td>0.093368</td>\n",
              "      <td>0.347640</td>\n",
              "      <td>0.212769</td>\n",
              "      <td>-0.384932</td>\n",
              "      <td>-0.288673</td>\n",
              "      <td>-0.026614</td>\n",
              "      <td>0.530695</td>\n",
              "      <td>-0.004031</td>\n",
              "      <td>0.158224</td>\n",
              "      <td>-0.237739</td>\n",
              "      <td>0.388905</td>\n",
              "      <td>-0.054335</td>\n",
              "      <td>0.544653</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>0.523372</td>\n",
              "      <td>0.383997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037189</td>\n",
              "      <td>-0.414413</td>\n",
              "      <td>-0.182625</td>\n",
              "      <td>-0.996050</td>\n",
              "      <td>-0.053274</td>\n",
              "      <td>-0.579955</td>\n",
              "      <td>0.225314</td>\n",
              "      <td>0.360031</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>0.022793</td>\n",
              "      <td>0.268636</td>\n",
              "      <td>0.278417</td>\n",
              "      <td>-0.433382</td>\n",
              "      <td>0.136677</td>\n",
              "      <td>-0.082261</td>\n",
              "      <td>-0.230011</td>\n",
              "      <td>-0.054747</td>\n",
              "      <td>0.223735</td>\n",
              "      <td>0.856815</td>\n",
              "      <td>-0.240603</td>\n",
              "      <td>0.567785</td>\n",
              "      <td>-0.011137</td>\n",
              "      <td>-0.196050</td>\n",
              "      <td>0.296334</td>\n",
              "      <td>0.254625</td>\n",
              "      <td>-0.015677</td>\n",
              "      <td>-0.149860</td>\n",
              "      <td>-0.363567</td>\n",
              "      <td>-0.522562</td>\n",
              "      <td>-0.720226</td>\n",
              "      <td>-0.107708</td>\n",
              "      <td>0.265608</td>\n",
              "      <td>-0.207027</td>\n",
              "      <td>0.666353</td>\n",
              "      <td>-0.288064</td>\n",
              "      <td>-0.198666</td>\n",
              "      <td>0.036834</td>\n",
              "      <td>0.211623</td>\n",
              "      <td>0.179178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.340932</td>\n",
              "      <td>0.221050</td>\n",
              "      <td>0.782805</td>\n",
              "      <td>-0.581847</td>\n",
              "      <td>0.461820</td>\n",
              "      <td>-0.240045</td>\n",
              "      <td>0.203510</td>\n",
              "      <td>0.163061</td>\n",
              "      <td>0.228730</td>\n",
              "      <td>-0.404142</td>\n",
              "      <td>-0.079556</td>\n",
              "      <td>-0.300071</td>\n",
              "      <td>-0.163420</td>\n",
              "      <td>0.303376</td>\n",
              "      <td>0.099836</td>\n",
              "      <td>1.423623</td>\n",
              "      <td>0.011883</td>\n",
              "      <td>0.281750</td>\n",
              "      <td>0.211207</td>\n",
              "      <td>0.114722</td>\n",
              "      <td>1.000818</td>\n",
              "      <td>-0.080286</td>\n",
              "      <td>0.418081</td>\n",
              "      <td>0.432044</td>\n",
              "      <td>0.134941</td>\n",
              "      <td>0.329491</td>\n",
              "      <td>0.002067</td>\n",
              "      <td>-0.246614</td>\n",
              "      <td>-0.725172</td>\n",
              "      <td>0.290682</td>\n",
              "      <td>0.589696</td>\n",
              "      <td>-0.288656</td>\n",
              "      <td>0.250666</td>\n",
              "      <td>-0.276592</td>\n",
              "      <td>0.094576</td>\n",
              "      <td>-0.686130</td>\n",
              "      <td>0.035607</td>\n",
              "      <td>-0.318936</td>\n",
              "      <td>-0.025617</td>\n",
              "      <td>0.274376</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.250307</td>\n",
              "      <td>-0.626264</td>\n",
              "      <td>-0.209951</td>\n",
              "      <td>-0.134688</td>\n",
              "      <td>-0.218366</td>\n",
              "      <td>-0.712746</td>\n",
              "      <td>-0.106775</td>\n",
              "      <td>-0.393539</td>\n",
              "      <td>-0.257189</td>\n",
              "      <td>0.242331</td>\n",
              "      <td>0.257851</td>\n",
              "      <td>-0.012872</td>\n",
              "      <td>0.346436</td>\n",
              "      <td>0.140753</td>\n",
              "      <td>0.183796</td>\n",
              "      <td>-0.812609</td>\n",
              "      <td>0.096131</td>\n",
              "      <td>0.087510</td>\n",
              "      <td>0.073986</td>\n",
              "      <td>0.639240</td>\n",
              "      <td>-0.432956</td>\n",
              "      <td>0.641596</td>\n",
              "      <td>0.018408</td>\n",
              "      <td>-0.177756</td>\n",
              "      <td>-0.095648</td>\n",
              "      <td>0.193993</td>\n",
              "      <td>0.135293</td>\n",
              "      <td>-0.477258</td>\n",
              "      <td>-0.690213</td>\n",
              "      <td>-0.110558</td>\n",
              "      <td>-0.294730</td>\n",
              "      <td>-0.006305</td>\n",
              "      <td>-0.257251</td>\n",
              "      <td>-0.614889</td>\n",
              "      <td>0.277610</td>\n",
              "      <td>-0.254113</td>\n",
              "      <td>-0.314011</td>\n",
              "      <td>-0.177580</td>\n",
              "      <td>0.440142</td>\n",
              "      <td>0.300442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.335891</td>\n",
              "      <td>0.215750</td>\n",
              "      <td>0.134236</td>\n",
              "      <td>-0.398985</td>\n",
              "      <td>0.100357</td>\n",
              "      <td>-0.276152</td>\n",
              "      <td>0.012297</td>\n",
              "      <td>0.451921</td>\n",
              "      <td>-0.033935</td>\n",
              "      <td>-0.082510</td>\n",
              "      <td>0.433583</td>\n",
              "      <td>-0.274968</td>\n",
              "      <td>0.025606</td>\n",
              "      <td>0.025651</td>\n",
              "      <td>-0.230578</td>\n",
              "      <td>0.716468</td>\n",
              "      <td>0.404485</td>\n",
              "      <td>0.144631</td>\n",
              "      <td>0.428208</td>\n",
              "      <td>0.019562</td>\n",
              "      <td>0.691490</td>\n",
              "      <td>0.314824</td>\n",
              "      <td>0.169805</td>\n",
              "      <td>0.501763</td>\n",
              "      <td>0.271473</td>\n",
              "      <td>0.199180</td>\n",
              "      <td>0.172446</td>\n",
              "      <td>-0.116188</td>\n",
              "      <td>-0.452280</td>\n",
              "      <td>0.228855</td>\n",
              "      <td>0.681822</td>\n",
              "      <td>0.008252</td>\n",
              "      <td>-0.115546</td>\n",
              "      <td>-0.551180</td>\n",
              "      <td>0.012454</td>\n",
              "      <td>-0.525111</td>\n",
              "      <td>0.252605</td>\n",
              "      <td>-0.403585</td>\n",
              "      <td>0.586612</td>\n",
              "      <td>0.120770</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129724</td>\n",
              "      <td>-0.507280</td>\n",
              "      <td>-0.156877</td>\n",
              "      <td>-0.334767</td>\n",
              "      <td>-0.140801</td>\n",
              "      <td>-0.423644</td>\n",
              "      <td>0.266033</td>\n",
              "      <td>0.174512</td>\n",
              "      <td>0.037891</td>\n",
              "      <td>0.047548</td>\n",
              "      <td>0.276495</td>\n",
              "      <td>0.265085</td>\n",
              "      <td>0.240179</td>\n",
              "      <td>0.121982</td>\n",
              "      <td>0.240448</td>\n",
              "      <td>-0.172127</td>\n",
              "      <td>-0.204381</td>\n",
              "      <td>0.216384</td>\n",
              "      <td>0.181148</td>\n",
              "      <td>0.653867</td>\n",
              "      <td>-0.144304</td>\n",
              "      <td>0.558028</td>\n",
              "      <td>-0.290160</td>\n",
              "      <td>-0.089322</td>\n",
              "      <td>-0.047649</td>\n",
              "      <td>-0.118058</td>\n",
              "      <td>0.032081</td>\n",
              "      <td>-0.104082</td>\n",
              "      <td>0.046842</td>\n",
              "      <td>-0.029964</td>\n",
              "      <td>-0.514549</td>\n",
              "      <td>0.057987</td>\n",
              "      <td>-0.115259</td>\n",
              "      <td>0.224609</td>\n",
              "      <td>0.359204</td>\n",
              "      <td>-0.294915</td>\n",
              "      <td>-0.363483</td>\n",
              "      <td>-0.051290</td>\n",
              "      <td>0.363466</td>\n",
              "      <td>0.152344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.156222</td>\n",
              "      <td>0.466047</td>\n",
              "      <td>0.407739</td>\n",
              "      <td>-0.488406</td>\n",
              "      <td>-0.134691</td>\n",
              "      <td>0.180689</td>\n",
              "      <td>0.119333</td>\n",
              "      <td>0.555957</td>\n",
              "      <td>-0.380613</td>\n",
              "      <td>0.106727</td>\n",
              "      <td>0.350529</td>\n",
              "      <td>-0.594996</td>\n",
              "      <td>-0.018465</td>\n",
              "      <td>0.219168</td>\n",
              "      <td>-0.017954</td>\n",
              "      <td>0.668042</td>\n",
              "      <td>0.372010</td>\n",
              "      <td>0.225265</td>\n",
              "      <td>-0.009484</td>\n",
              "      <td>0.251160</td>\n",
              "      <td>0.478516</td>\n",
              "      <td>0.029257</td>\n",
              "      <td>0.119113</td>\n",
              "      <td>0.528693</td>\n",
              "      <td>0.074440</td>\n",
              "      <td>0.295118</td>\n",
              "      <td>-0.032307</td>\n",
              "      <td>-0.149913</td>\n",
              "      <td>-0.470186</td>\n",
              "      <td>0.174785</td>\n",
              "      <td>0.256214</td>\n",
              "      <td>0.150042</td>\n",
              "      <td>0.303352</td>\n",
              "      <td>-0.476231</td>\n",
              "      <td>0.053985</td>\n",
              "      <td>-0.321143</td>\n",
              "      <td>0.158450</td>\n",
              "      <td>-0.342911</td>\n",
              "      <td>0.354510</td>\n",
              "      <td>0.329658</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.254499</td>\n",
              "      <td>-0.511102</td>\n",
              "      <td>-0.289797</td>\n",
              "      <td>-0.663957</td>\n",
              "      <td>-0.208240</td>\n",
              "      <td>-0.269668</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.476862</td>\n",
              "      <td>-0.435070</td>\n",
              "      <td>-0.319236</td>\n",
              "      <td>-0.132682</td>\n",
              "      <td>0.485799</td>\n",
              "      <td>0.169192</td>\n",
              "      <td>-0.452174</td>\n",
              "      <td>0.311044</td>\n",
              "      <td>-0.247674</td>\n",
              "      <td>-0.211432</td>\n",
              "      <td>0.119443</td>\n",
              "      <td>0.363887</td>\n",
              "      <td>0.758694</td>\n",
              "      <td>-0.099674</td>\n",
              "      <td>0.347741</td>\n",
              "      <td>-0.247450</td>\n",
              "      <td>-0.375582</td>\n",
              "      <td>-0.018978</td>\n",
              "      <td>-0.099293</td>\n",
              "      <td>0.056441</td>\n",
              "      <td>-0.230396</td>\n",
              "      <td>-0.302153</td>\n",
              "      <td>-0.292718</td>\n",
              "      <td>-0.558637</td>\n",
              "      <td>-0.064437</td>\n",
              "      <td>0.064220</td>\n",
              "      <td>-0.150074</td>\n",
              "      <td>0.565909</td>\n",
              "      <td>-0.179520</td>\n",
              "      <td>-0.388975</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>0.498361</td>\n",
              "      <td>0.313834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.187530</td>\n",
              "      <td>0.268600</td>\n",
              "      <td>0.311052</td>\n",
              "      <td>-0.200505</td>\n",
              "      <td>0.015567</td>\n",
              "      <td>-0.144803</td>\n",
              "      <td>-0.014919</td>\n",
              "      <td>0.373741</td>\n",
              "      <td>0.093312</td>\n",
              "      <td>-0.321745</td>\n",
              "      <td>0.163310</td>\n",
              "      <td>-0.396472</td>\n",
              "      <td>-0.016497</td>\n",
              "      <td>-0.006123</td>\n",
              "      <td>-0.377307</td>\n",
              "      <td>0.720480</td>\n",
              "      <td>0.312520</td>\n",
              "      <td>0.613831</td>\n",
              "      <td>0.084854</td>\n",
              "      <td>-0.234721</td>\n",
              "      <td>1.083672</td>\n",
              "      <td>-0.332469</td>\n",
              "      <td>0.131527</td>\n",
              "      <td>0.448986</td>\n",
              "      <td>0.228013</td>\n",
              "      <td>-0.003657</td>\n",
              "      <td>0.131388</td>\n",
              "      <td>-0.161173</td>\n",
              "      <td>-0.393312</td>\n",
              "      <td>0.170627</td>\n",
              "      <td>0.593013</td>\n",
              "      <td>-0.103269</td>\n",
              "      <td>0.474441</td>\n",
              "      <td>-0.234904</td>\n",
              "      <td>0.048970</td>\n",
              "      <td>-0.413106</td>\n",
              "      <td>0.394481</td>\n",
              "      <td>-0.358634</td>\n",
              "      <td>0.347066</td>\n",
              "      <td>-0.008929</td>\n",
              "      <td>...</td>\n",
              "      <td>0.117894</td>\n",
              "      <td>-0.644592</td>\n",
              "      <td>-0.153507</td>\n",
              "      <td>-0.261847</td>\n",
              "      <td>-0.348074</td>\n",
              "      <td>-0.581157</td>\n",
              "      <td>-0.168289</td>\n",
              "      <td>0.061962</td>\n",
              "      <td>0.115857</td>\n",
              "      <td>0.043849</td>\n",
              "      <td>-0.130765</td>\n",
              "      <td>0.537401</td>\n",
              "      <td>0.136393</td>\n",
              "      <td>-0.616925</td>\n",
              "      <td>-0.219375</td>\n",
              "      <td>-0.171489</td>\n",
              "      <td>0.045843</td>\n",
              "      <td>0.463768</td>\n",
              "      <td>0.320474</td>\n",
              "      <td>0.664438</td>\n",
              "      <td>-0.564877</td>\n",
              "      <td>0.214166</td>\n",
              "      <td>0.121975</td>\n",
              "      <td>-0.381541</td>\n",
              "      <td>0.032417</td>\n",
              "      <td>0.066512</td>\n",
              "      <td>0.479395</td>\n",
              "      <td>-0.364574</td>\n",
              "      <td>-0.432845</td>\n",
              "      <td>-0.817666</td>\n",
              "      <td>-0.713633</td>\n",
              "      <td>-0.180551</td>\n",
              "      <td>-0.376038</td>\n",
              "      <td>0.114734</td>\n",
              "      <td>0.641779</td>\n",
              "      <td>-0.507370</td>\n",
              "      <td>-0.481888</td>\n",
              "      <td>-0.259569</td>\n",
              "      <td>0.059434</td>\n",
              "      <td>0.451487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>0.091006</td>\n",
              "      <td>0.561350</td>\n",
              "      <td>0.220087</td>\n",
              "      <td>-0.757771</td>\n",
              "      <td>0.133639</td>\n",
              "      <td>-0.109520</td>\n",
              "      <td>0.180277</td>\n",
              "      <td>0.367699</td>\n",
              "      <td>-0.224234</td>\n",
              "      <td>-0.332058</td>\n",
              "      <td>-0.089731</td>\n",
              "      <td>-0.218915</td>\n",
              "      <td>-0.109006</td>\n",
              "      <td>0.048059</td>\n",
              "      <td>-0.263213</td>\n",
              "      <td>0.863571</td>\n",
              "      <td>0.212466</td>\n",
              "      <td>0.389487</td>\n",
              "      <td>0.614161</td>\n",
              "      <td>-0.086711</td>\n",
              "      <td>0.788468</td>\n",
              "      <td>-0.060691</td>\n",
              "      <td>-0.076992</td>\n",
              "      <td>0.318297</td>\n",
              "      <td>0.327359</td>\n",
              "      <td>0.172609</td>\n",
              "      <td>0.494178</td>\n",
              "      <td>-0.093250</td>\n",
              "      <td>-0.642839</td>\n",
              "      <td>-0.063243</td>\n",
              "      <td>0.781365</td>\n",
              "      <td>0.175113</td>\n",
              "      <td>-0.152739</td>\n",
              "      <td>-0.280006</td>\n",
              "      <td>-0.169909</td>\n",
              "      <td>-0.313993</td>\n",
              "      <td>0.450466</td>\n",
              "      <td>-0.352787</td>\n",
              "      <td>-0.029192</td>\n",
              "      <td>0.211810</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026387</td>\n",
              "      <td>-0.291474</td>\n",
              "      <td>-0.140296</td>\n",
              "      <td>-0.030668</td>\n",
              "      <td>-0.095701</td>\n",
              "      <td>-0.493935</td>\n",
              "      <td>0.110218</td>\n",
              "      <td>-0.414005</td>\n",
              "      <td>-0.178922</td>\n",
              "      <td>0.224465</td>\n",
              "      <td>-0.055236</td>\n",
              "      <td>0.047705</td>\n",
              "      <td>0.480967</td>\n",
              "      <td>-0.190057</td>\n",
              "      <td>0.224529</td>\n",
              "      <td>-0.089975</td>\n",
              "      <td>0.056657</td>\n",
              "      <td>0.022135</td>\n",
              "      <td>-0.075964</td>\n",
              "      <td>0.397923</td>\n",
              "      <td>-0.350319</td>\n",
              "      <td>0.295857</td>\n",
              "      <td>0.259457</td>\n",
              "      <td>-0.614115</td>\n",
              "      <td>0.262725</td>\n",
              "      <td>-0.269345</td>\n",
              "      <td>-0.008568</td>\n",
              "      <td>-0.338333</td>\n",
              "      <td>-0.507381</td>\n",
              "      <td>-0.184507</td>\n",
              "      <td>-0.567601</td>\n",
              "      <td>0.110983</td>\n",
              "      <td>-0.044716</td>\n",
              "      <td>-0.272419</td>\n",
              "      <td>0.226073</td>\n",
              "      <td>-0.190810</td>\n",
              "      <td>-0.389146</td>\n",
              "      <td>-0.013880</td>\n",
              "      <td>0.462799</td>\n",
              "      <td>0.254994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>0.106377</td>\n",
              "      <td>0.364273</td>\n",
              "      <td>0.648710</td>\n",
              "      <td>-0.651089</td>\n",
              "      <td>0.579585</td>\n",
              "      <td>-0.194366</td>\n",
              "      <td>-0.166921</td>\n",
              "      <td>0.816909</td>\n",
              "      <td>-0.206333</td>\n",
              "      <td>-0.141815</td>\n",
              "      <td>0.311151</td>\n",
              "      <td>-0.382003</td>\n",
              "      <td>0.178676</td>\n",
              "      <td>0.188375</td>\n",
              "      <td>-0.377538</td>\n",
              "      <td>0.937112</td>\n",
              "      <td>0.124876</td>\n",
              "      <td>0.456146</td>\n",
              "      <td>0.147312</td>\n",
              "      <td>-0.224139</td>\n",
              "      <td>0.760303</td>\n",
              "      <td>0.072268</td>\n",
              "      <td>0.033559</td>\n",
              "      <td>0.086394</td>\n",
              "      <td>0.094563</td>\n",
              "      <td>0.491350</td>\n",
              "      <td>0.366786</td>\n",
              "      <td>0.298446</td>\n",
              "      <td>-0.476386</td>\n",
              "      <td>-0.132563</td>\n",
              "      <td>0.493288</td>\n",
              "      <td>0.055220</td>\n",
              "      <td>-0.000392</td>\n",
              "      <td>-0.430928</td>\n",
              "      <td>0.127056</td>\n",
              "      <td>-0.686848</td>\n",
              "      <td>0.537682</td>\n",
              "      <td>-0.260890</td>\n",
              "      <td>-0.077890</td>\n",
              "      <td>0.505422</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073203</td>\n",
              "      <td>-0.474479</td>\n",
              "      <td>-0.314126</td>\n",
              "      <td>-0.324037</td>\n",
              "      <td>-0.047044</td>\n",
              "      <td>-0.735415</td>\n",
              "      <td>0.272512</td>\n",
              "      <td>-0.134246</td>\n",
              "      <td>-0.338725</td>\n",
              "      <td>0.124791</td>\n",
              "      <td>0.085217</td>\n",
              "      <td>0.354539</td>\n",
              "      <td>-0.002648</td>\n",
              "      <td>-0.183943</td>\n",
              "      <td>0.093347</td>\n",
              "      <td>-0.081096</td>\n",
              "      <td>-0.133964</td>\n",
              "      <td>0.267154</td>\n",
              "      <td>0.396371</td>\n",
              "      <td>0.388802</td>\n",
              "      <td>-0.493829</td>\n",
              "      <td>0.509024</td>\n",
              "      <td>0.112676</td>\n",
              "      <td>-0.273665</td>\n",
              "      <td>0.272412</td>\n",
              "      <td>0.059530</td>\n",
              "      <td>0.241940</td>\n",
              "      <td>-0.275573</td>\n",
              "      <td>-0.493048</td>\n",
              "      <td>-0.192752</td>\n",
              "      <td>-0.309807</td>\n",
              "      <td>-0.018762</td>\n",
              "      <td>0.060558</td>\n",
              "      <td>-0.009681</td>\n",
              "      <td>0.366008</td>\n",
              "      <td>-0.486010</td>\n",
              "      <td>-0.402157</td>\n",
              "      <td>0.202666</td>\n",
              "      <td>0.450038</td>\n",
              "      <td>0.294662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>0.200587</td>\n",
              "      <td>0.632470</td>\n",
              "      <td>0.452327</td>\n",
              "      <td>-0.333864</td>\n",
              "      <td>0.105404</td>\n",
              "      <td>0.116012</td>\n",
              "      <td>0.251767</td>\n",
              "      <td>0.328868</td>\n",
              "      <td>-0.012447</td>\n",
              "      <td>-0.314254</td>\n",
              "      <td>0.149082</td>\n",
              "      <td>-0.242813</td>\n",
              "      <td>-0.076636</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.213088</td>\n",
              "      <td>0.834758</td>\n",
              "      <td>0.320840</td>\n",
              "      <td>0.100508</td>\n",
              "      <td>0.091595</td>\n",
              "      <td>-0.229268</td>\n",
              "      <td>1.150250</td>\n",
              "      <td>-0.167489</td>\n",
              "      <td>0.320666</td>\n",
              "      <td>0.302161</td>\n",
              "      <td>0.288873</td>\n",
              "      <td>0.288023</td>\n",
              "      <td>0.329725</td>\n",
              "      <td>-0.048066</td>\n",
              "      <td>-0.749308</td>\n",
              "      <td>-0.145215</td>\n",
              "      <td>0.506286</td>\n",
              "      <td>-0.147755</td>\n",
              "      <td>0.375724</td>\n",
              "      <td>-0.401739</td>\n",
              "      <td>-0.195842</td>\n",
              "      <td>-0.297437</td>\n",
              "      <td>-0.012209</td>\n",
              "      <td>-0.290802</td>\n",
              "      <td>-0.012745</td>\n",
              "      <td>-0.086297</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340454</td>\n",
              "      <td>-0.293618</td>\n",
              "      <td>-0.314073</td>\n",
              "      <td>-0.433923</td>\n",
              "      <td>0.122052</td>\n",
              "      <td>-0.360461</td>\n",
              "      <td>-0.107032</td>\n",
              "      <td>-0.139771</td>\n",
              "      <td>-0.065770</td>\n",
              "      <td>0.072193</td>\n",
              "      <td>-0.287180</td>\n",
              "      <td>0.174737</td>\n",
              "      <td>0.141031</td>\n",
              "      <td>0.121455</td>\n",
              "      <td>0.405234</td>\n",
              "      <td>0.022682</td>\n",
              "      <td>0.163822</td>\n",
              "      <td>0.264478</td>\n",
              "      <td>0.184069</td>\n",
              "      <td>0.484682</td>\n",
              "      <td>-0.139608</td>\n",
              "      <td>0.799220</td>\n",
              "      <td>-0.029043</td>\n",
              "      <td>-0.470777</td>\n",
              "      <td>-0.035307</td>\n",
              "      <td>0.199051</td>\n",
              "      <td>0.209762</td>\n",
              "      <td>-0.564824</td>\n",
              "      <td>-0.887952</td>\n",
              "      <td>-0.082044</td>\n",
              "      <td>-0.386754</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.367012</td>\n",
              "      <td>-0.454167</td>\n",
              "      <td>0.118667</td>\n",
              "      <td>-0.097619</td>\n",
              "      <td>-0.270266</td>\n",
              "      <td>0.187779</td>\n",
              "      <td>0.342686</td>\n",
              "      <td>-0.030291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>-0.275840</td>\n",
              "      <td>0.431549</td>\n",
              "      <td>0.353404</td>\n",
              "      <td>-0.389437</td>\n",
              "      <td>-0.153087</td>\n",
              "      <td>-0.148158</td>\n",
              "      <td>0.149882</td>\n",
              "      <td>0.477485</td>\n",
              "      <td>-0.354752</td>\n",
              "      <td>-0.184685</td>\n",
              "      <td>0.010818</td>\n",
              "      <td>-0.363396</td>\n",
              "      <td>0.134015</td>\n",
              "      <td>0.318329</td>\n",
              "      <td>-0.007664</td>\n",
              "      <td>0.815095</td>\n",
              "      <td>0.340491</td>\n",
              "      <td>0.229355</td>\n",
              "      <td>0.201636</td>\n",
              "      <td>0.155619</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.026976</td>\n",
              "      <td>0.108195</td>\n",
              "      <td>0.706576</td>\n",
              "      <td>0.184612</td>\n",
              "      <td>0.110016</td>\n",
              "      <td>-0.107746</td>\n",
              "      <td>-0.405019</td>\n",
              "      <td>-0.477389</td>\n",
              "      <td>0.105523</td>\n",
              "      <td>0.329453</td>\n",
              "      <td>0.043848</td>\n",
              "      <td>0.128855</td>\n",
              "      <td>-0.314362</td>\n",
              "      <td>-0.489506</td>\n",
              "      <td>-0.226411</td>\n",
              "      <td>0.420743</td>\n",
              "      <td>-0.313750</td>\n",
              "      <td>0.194902</td>\n",
              "      <td>0.510837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.182181</td>\n",
              "      <td>-0.530322</td>\n",
              "      <td>-0.110891</td>\n",
              "      <td>-0.312774</td>\n",
              "      <td>-0.071164</td>\n",
              "      <td>-0.630336</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>0.257743</td>\n",
              "      <td>-0.055170</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>-0.172036</td>\n",
              "      <td>0.161880</td>\n",
              "      <td>0.196169</td>\n",
              "      <td>-0.337393</td>\n",
              "      <td>0.292819</td>\n",
              "      <td>-0.008794</td>\n",
              "      <td>-0.377003</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>-0.163677</td>\n",
              "      <td>0.576054</td>\n",
              "      <td>-0.423863</td>\n",
              "      <td>0.678619</td>\n",
              "      <td>0.131139</td>\n",
              "      <td>-0.296466</td>\n",
              "      <td>-0.155286</td>\n",
              "      <td>-0.280105</td>\n",
              "      <td>-0.025960</td>\n",
              "      <td>-0.580991</td>\n",
              "      <td>-0.219401</td>\n",
              "      <td>-0.463358</td>\n",
              "      <td>-0.647055</td>\n",
              "      <td>0.039960</td>\n",
              "      <td>0.287399</td>\n",
              "      <td>-0.079697</td>\n",
              "      <td>-0.009502</td>\n",
              "      <td>-0.003524</td>\n",
              "      <td>-0.030494</td>\n",
              "      <td>0.129787</td>\n",
              "      <td>0.480699</td>\n",
              "      <td>0.319759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>-0.351146</td>\n",
              "      <td>0.597660</td>\n",
              "      <td>0.309986</td>\n",
              "      <td>-0.888673</td>\n",
              "      <td>0.306189</td>\n",
              "      <td>0.201990</td>\n",
              "      <td>0.245330</td>\n",
              "      <td>0.635296</td>\n",
              "      <td>-0.194669</td>\n",
              "      <td>-0.165458</td>\n",
              "      <td>0.411324</td>\n",
              "      <td>-0.347652</td>\n",
              "      <td>-0.116542</td>\n",
              "      <td>0.376998</td>\n",
              "      <td>-0.176046</td>\n",
              "      <td>0.629920</td>\n",
              "      <td>0.096958</td>\n",
              "      <td>0.495873</td>\n",
              "      <td>0.306107</td>\n",
              "      <td>-0.099550</td>\n",
              "      <td>0.643617</td>\n",
              "      <td>0.033749</td>\n",
              "      <td>0.153753</td>\n",
              "      <td>0.064789</td>\n",
              "      <td>-0.041080</td>\n",
              "      <td>0.367131</td>\n",
              "      <td>0.451226</td>\n",
              "      <td>0.085414</td>\n",
              "      <td>-0.393418</td>\n",
              "      <td>0.162472</td>\n",
              "      <td>0.260070</td>\n",
              "      <td>0.231759</td>\n",
              "      <td>0.031699</td>\n",
              "      <td>-0.222851</td>\n",
              "      <td>0.035163</td>\n",
              "      <td>-0.419068</td>\n",
              "      <td>0.300631</td>\n",
              "      <td>-0.404352</td>\n",
              "      <td>0.188232</td>\n",
              "      <td>0.195429</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.162446</td>\n",
              "      <td>-0.482416</td>\n",
              "      <td>-0.320390</td>\n",
              "      <td>-0.072718</td>\n",
              "      <td>-0.120588</td>\n",
              "      <td>-0.660606</td>\n",
              "      <td>0.211060</td>\n",
              "      <td>0.095862</td>\n",
              "      <td>0.119164</td>\n",
              "      <td>0.142719</td>\n",
              "      <td>-0.049428</td>\n",
              "      <td>0.285637</td>\n",
              "      <td>0.355071</td>\n",
              "      <td>-0.222420</td>\n",
              "      <td>0.502982</td>\n",
              "      <td>-0.417510</td>\n",
              "      <td>-0.000320</td>\n",
              "      <td>-0.180309</td>\n",
              "      <td>0.200359</td>\n",
              "      <td>0.606395</td>\n",
              "      <td>-0.421940</td>\n",
              "      <td>0.573564</td>\n",
              "      <td>-0.035178</td>\n",
              "      <td>-0.399351</td>\n",
              "      <td>0.028797</td>\n",
              "      <td>-0.024448</td>\n",
              "      <td>0.003119</td>\n",
              "      <td>-0.325046</td>\n",
              "      <td>-0.297072</td>\n",
              "      <td>0.084704</td>\n",
              "      <td>-0.389994</td>\n",
              "      <td>-0.048743</td>\n",
              "      <td>0.438872</td>\n",
              "      <td>-0.022448</td>\n",
              "      <td>0.618735</td>\n",
              "      <td>-0.465972</td>\n",
              "      <td>-0.122363</td>\n",
              "      <td>-0.467356</td>\n",
              "      <td>0.332870</td>\n",
              "      <td>0.483163</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       765       766       767\n",
              "0     0.089255  0.474704  0.171598  ...  0.036834  0.211623  0.179178\n",
              "1    -0.340932  0.221050  0.782805  ... -0.177580  0.440142  0.300442\n",
              "2    -0.335891  0.215750  0.134236  ... -0.051290  0.363466  0.152344\n",
              "3    -0.156222  0.466047  0.407739  ...  0.025525  0.498361  0.313834\n",
              "4    -0.187530  0.268600  0.311052  ... -0.259569  0.059434  0.451487\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "8995  0.091006  0.561350  0.220087  ... -0.013880  0.462799  0.254994\n",
              "8996  0.106377  0.364273  0.648710  ...  0.202666  0.450038  0.294662\n",
              "8997  0.200587  0.632470  0.452327  ...  0.187779  0.342686 -0.030291\n",
              "8998 -0.275840  0.431549  0.353404  ...  0.129787  0.480699  0.319759\n",
              "8999 -0.351146  0.597660  0.309986  ... -0.467356  0.332870  0.483163\n",
              "\n",
              "[9000 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "633ee717-0880-467e-b58b-3da40efa63ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "# X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUmYQdKgdUq",
        "colab_type": "code",
        "outputId": "a0048ae2-42af-4ab8-86df-3f79fb6785e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5614</th>\n",
              "      <td>-0.340161</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>0.602034</td>\n",
              "      <td>-0.272003</td>\n",
              "      <td>-0.341873</td>\n",
              "      <td>-0.120357</td>\n",
              "      <td>0.224259</td>\n",
              "      <td>0.206777</td>\n",
              "      <td>-0.050708</td>\n",
              "      <td>-0.413362</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.422804</td>\n",
              "      <td>-0.132413</td>\n",
              "      <td>0.146387</td>\n",
              "      <td>-0.105816</td>\n",
              "      <td>0.753637</td>\n",
              "      <td>0.414343</td>\n",
              "      <td>0.122540</td>\n",
              "      <td>0.179269</td>\n",
              "      <td>0.088668</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>-0.148916</td>\n",
              "      <td>0.095241</td>\n",
              "      <td>0.706351</td>\n",
              "      <td>0.180122</td>\n",
              "      <td>0.449534</td>\n",
              "      <td>0.147201</td>\n",
              "      <td>-0.237596</td>\n",
              "      <td>-0.151617</td>\n",
              "      <td>-0.037453</td>\n",
              "      <td>0.579897</td>\n",
              "      <td>0.074145</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>-0.265257</td>\n",
              "      <td>-0.118240</td>\n",
              "      <td>-0.280390</td>\n",
              "      <td>0.099969</td>\n",
              "      <td>-0.501312</td>\n",
              "      <td>0.068412</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.236783</td>\n",
              "      <td>-0.722360</td>\n",
              "      <td>-0.142746</td>\n",
              "      <td>-0.070288</td>\n",
              "      <td>-0.142651</td>\n",
              "      <td>-0.857589</td>\n",
              "      <td>-0.207489</td>\n",
              "      <td>0.172034</td>\n",
              "      <td>-0.133808</td>\n",
              "      <td>-0.344236</td>\n",
              "      <td>-0.061278</td>\n",
              "      <td>0.064547</td>\n",
              "      <td>0.122214</td>\n",
              "      <td>-0.097235</td>\n",
              "      <td>0.183206</td>\n",
              "      <td>-0.092817</td>\n",
              "      <td>-0.457735</td>\n",
              "      <td>0.098827</td>\n",
              "      <td>-0.223504</td>\n",
              "      <td>0.734053</td>\n",
              "      <td>-0.446106</td>\n",
              "      <td>0.604875</td>\n",
              "      <td>0.058997</td>\n",
              "      <td>-0.558629</td>\n",
              "      <td>-0.211270</td>\n",
              "      <td>-0.087967</td>\n",
              "      <td>-0.125183</td>\n",
              "      <td>-0.588484</td>\n",
              "      <td>-0.453498</td>\n",
              "      <td>-0.176318</td>\n",
              "      <td>-0.628320</td>\n",
              "      <td>0.045665</td>\n",
              "      <td>0.183669</td>\n",
              "      <td>-0.280129</td>\n",
              "      <td>-0.195073</td>\n",
              "      <td>-0.313454</td>\n",
              "      <td>-0.230915</td>\n",
              "      <td>-0.017511</td>\n",
              "      <td>0.452152</td>\n",
              "      <td>0.481604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2383</th>\n",
              "      <td>-0.391934</td>\n",
              "      <td>0.589659</td>\n",
              "      <td>0.081654</td>\n",
              "      <td>-0.364856</td>\n",
              "      <td>0.272116</td>\n",
              "      <td>-0.185212</td>\n",
              "      <td>-0.179081</td>\n",
              "      <td>0.248599</td>\n",
              "      <td>-0.116149</td>\n",
              "      <td>-0.085957</td>\n",
              "      <td>0.279195</td>\n",
              "      <td>-0.495050</td>\n",
              "      <td>-0.024453</td>\n",
              "      <td>0.470089</td>\n",
              "      <td>0.149684</td>\n",
              "      <td>1.075918</td>\n",
              "      <td>0.142027</td>\n",
              "      <td>-0.185910</td>\n",
              "      <td>0.287008</td>\n",
              "      <td>0.135054</td>\n",
              "      <td>0.798489</td>\n",
              "      <td>-0.099341</td>\n",
              "      <td>0.331279</td>\n",
              "      <td>0.542002</td>\n",
              "      <td>0.268069</td>\n",
              "      <td>0.279488</td>\n",
              "      <td>0.068858</td>\n",
              "      <td>-0.253427</td>\n",
              "      <td>-0.456926</td>\n",
              "      <td>0.170671</td>\n",
              "      <td>0.415524</td>\n",
              "      <td>-0.215536</td>\n",
              "      <td>-0.112373</td>\n",
              "      <td>-0.604337</td>\n",
              "      <td>0.137619</td>\n",
              "      <td>-0.606019</td>\n",
              "      <td>0.378203</td>\n",
              "      <td>-0.173980</td>\n",
              "      <td>0.747436</td>\n",
              "      <td>0.213383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.194351</td>\n",
              "      <td>-0.251167</td>\n",
              "      <td>-0.103786</td>\n",
              "      <td>-0.614198</td>\n",
              "      <td>-0.427270</td>\n",
              "      <td>-0.536838</td>\n",
              "      <td>-0.142121</td>\n",
              "      <td>0.175018</td>\n",
              "      <td>0.257659</td>\n",
              "      <td>0.063984</td>\n",
              "      <td>0.204867</td>\n",
              "      <td>0.264982</td>\n",
              "      <td>0.090459</td>\n",
              "      <td>-0.156936</td>\n",
              "      <td>0.328172</td>\n",
              "      <td>-0.267422</td>\n",
              "      <td>0.004332</td>\n",
              "      <td>0.437211</td>\n",
              "      <td>0.271880</td>\n",
              "      <td>0.837500</td>\n",
              "      <td>-0.222474</td>\n",
              "      <td>0.509900</td>\n",
              "      <td>-0.052957</td>\n",
              "      <td>-0.122060</td>\n",
              "      <td>0.306961</td>\n",
              "      <td>0.155461</td>\n",
              "      <td>0.102520</td>\n",
              "      <td>-0.191638</td>\n",
              "      <td>-0.222842</td>\n",
              "      <td>-0.347603</td>\n",
              "      <td>-0.569109</td>\n",
              "      <td>0.100797</td>\n",
              "      <td>-0.224095</td>\n",
              "      <td>0.094102</td>\n",
              "      <td>0.375749</td>\n",
              "      <td>0.166803</td>\n",
              "      <td>-0.509638</td>\n",
              "      <td>0.045654</td>\n",
              "      <td>0.267326</td>\n",
              "      <td>0.322551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>-0.311391</td>\n",
              "      <td>0.246223</td>\n",
              "      <td>0.322206</td>\n",
              "      <td>-0.626673</td>\n",
              "      <td>-0.120535</td>\n",
              "      <td>-0.328576</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>0.150335</td>\n",
              "      <td>-0.228633</td>\n",
              "      <td>-0.119519</td>\n",
              "      <td>0.234018</td>\n",
              "      <td>-0.455078</td>\n",
              "      <td>0.094305</td>\n",
              "      <td>0.039104</td>\n",
              "      <td>-0.354940</td>\n",
              "      <td>1.053825</td>\n",
              "      <td>0.437839</td>\n",
              "      <td>0.352679</td>\n",
              "      <td>0.163303</td>\n",
              "      <td>-0.222175</td>\n",
              "      <td>0.575262</td>\n",
              "      <td>-0.151778</td>\n",
              "      <td>0.039601</td>\n",
              "      <td>0.237160</td>\n",
              "      <td>0.136593</td>\n",
              "      <td>0.441844</td>\n",
              "      <td>0.216453</td>\n",
              "      <td>-0.411612</td>\n",
              "      <td>-0.499689</td>\n",
              "      <td>-0.125523</td>\n",
              "      <td>0.927454</td>\n",
              "      <td>-0.064041</td>\n",
              "      <td>0.280551</td>\n",
              "      <td>0.071708</td>\n",
              "      <td>-0.018094</td>\n",
              "      <td>-0.288303</td>\n",
              "      <td>0.428391</td>\n",
              "      <td>-0.406295</td>\n",
              "      <td>0.066329</td>\n",
              "      <td>0.261219</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063150</td>\n",
              "      <td>-0.204630</td>\n",
              "      <td>-0.137450</td>\n",
              "      <td>-0.170344</td>\n",
              "      <td>-0.314548</td>\n",
              "      <td>-0.759352</td>\n",
              "      <td>-0.347153</td>\n",
              "      <td>-0.266005</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0.089698</td>\n",
              "      <td>-0.136013</td>\n",
              "      <td>0.242091</td>\n",
              "      <td>0.530148</td>\n",
              "      <td>-0.121507</td>\n",
              "      <td>0.408584</td>\n",
              "      <td>-0.177793</td>\n",
              "      <td>-0.448586</td>\n",
              "      <td>-0.175234</td>\n",
              "      <td>0.130837</td>\n",
              "      <td>0.919979</td>\n",
              "      <td>-0.481680</td>\n",
              "      <td>0.259219</td>\n",
              "      <td>0.216071</td>\n",
              "      <td>-0.403066</td>\n",
              "      <td>-0.032478</td>\n",
              "      <td>0.047672</td>\n",
              "      <td>0.106018</td>\n",
              "      <td>-0.227465</td>\n",
              "      <td>-0.409935</td>\n",
              "      <td>-0.288559</td>\n",
              "      <td>-0.625146</td>\n",
              "      <td>0.146311</td>\n",
              "      <td>0.043609</td>\n",
              "      <td>0.026973</td>\n",
              "      <td>0.094333</td>\n",
              "      <td>-0.698623</td>\n",
              "      <td>-0.382027</td>\n",
              "      <td>-0.658689</td>\n",
              "      <td>0.190388</td>\n",
              "      <td>0.197619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8412</th>\n",
              "      <td>-0.157567</td>\n",
              "      <td>0.439942</td>\n",
              "      <td>0.159633</td>\n",
              "      <td>-0.716131</td>\n",
              "      <td>0.394165</td>\n",
              "      <td>-0.170059</td>\n",
              "      <td>0.290004</td>\n",
              "      <td>0.776767</td>\n",
              "      <td>-0.385152</td>\n",
              "      <td>-0.022196</td>\n",
              "      <td>0.137164</td>\n",
              "      <td>-0.146209</td>\n",
              "      <td>0.309307</td>\n",
              "      <td>-0.086701</td>\n",
              "      <td>-0.312970</td>\n",
              "      <td>0.535559</td>\n",
              "      <td>0.378450</td>\n",
              "      <td>0.155951</td>\n",
              "      <td>0.159643</td>\n",
              "      <td>-0.078866</td>\n",
              "      <td>0.229330</td>\n",
              "      <td>-0.042130</td>\n",
              "      <td>-0.046688</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>-0.052326</td>\n",
              "      <td>0.334858</td>\n",
              "      <td>0.294528</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>-0.752542</td>\n",
              "      <td>0.043778</td>\n",
              "      <td>0.562868</td>\n",
              "      <td>0.093217</td>\n",
              "      <td>-0.056954</td>\n",
              "      <td>-0.345783</td>\n",
              "      <td>-0.071456</td>\n",
              "      <td>-0.615525</td>\n",
              "      <td>0.299450</td>\n",
              "      <td>-0.361907</td>\n",
              "      <td>0.153372</td>\n",
              "      <td>0.102023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.160457</td>\n",
              "      <td>-0.595206</td>\n",
              "      <td>-0.231311</td>\n",
              "      <td>-0.040880</td>\n",
              "      <td>0.016651</td>\n",
              "      <td>-0.327501</td>\n",
              "      <td>-0.104689</td>\n",
              "      <td>-0.158801</td>\n",
              "      <td>-0.196048</td>\n",
              "      <td>0.014901</td>\n",
              "      <td>-0.336172</td>\n",
              "      <td>-0.002332</td>\n",
              "      <td>0.202591</td>\n",
              "      <td>-0.242723</td>\n",
              "      <td>0.059663</td>\n",
              "      <td>-0.198993</td>\n",
              "      <td>0.126893</td>\n",
              "      <td>0.143854</td>\n",
              "      <td>0.008069</td>\n",
              "      <td>0.378277</td>\n",
              "      <td>-0.390169</td>\n",
              "      <td>0.389232</td>\n",
              "      <td>0.494898</td>\n",
              "      <td>-0.393445</td>\n",
              "      <td>0.279485</td>\n",
              "      <td>0.022502</td>\n",
              "      <td>0.140657</td>\n",
              "      <td>0.021231</td>\n",
              "      <td>-0.466449</td>\n",
              "      <td>0.030109</td>\n",
              "      <td>-0.550439</td>\n",
              "      <td>0.104452</td>\n",
              "      <td>-0.086756</td>\n",
              "      <td>-0.266146</td>\n",
              "      <td>0.756947</td>\n",
              "      <td>-0.396499</td>\n",
              "      <td>-0.247555</td>\n",
              "      <td>-0.413154</td>\n",
              "      <td>0.299626</td>\n",
              "      <td>0.228981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>-0.166418</td>\n",
              "      <td>0.318898</td>\n",
              "      <td>0.330421</td>\n",
              "      <td>-0.459612</td>\n",
              "      <td>-0.162131</td>\n",
              "      <td>0.267957</td>\n",
              "      <td>0.236269</td>\n",
              "      <td>0.415457</td>\n",
              "      <td>-0.052450</td>\n",
              "      <td>-0.242916</td>\n",
              "      <td>0.365095</td>\n",
              "      <td>-0.367487</td>\n",
              "      <td>-0.144583</td>\n",
              "      <td>0.366747</td>\n",
              "      <td>-0.179655</td>\n",
              "      <td>0.657255</td>\n",
              "      <td>0.487739</td>\n",
              "      <td>0.288062</td>\n",
              "      <td>-0.025196</td>\n",
              "      <td>-0.025123</td>\n",
              "      <td>1.028283</td>\n",
              "      <td>0.087602</td>\n",
              "      <td>0.023006</td>\n",
              "      <td>0.461438</td>\n",
              "      <td>0.195829</td>\n",
              "      <td>0.166410</td>\n",
              "      <td>-0.029758</td>\n",
              "      <td>-0.221556</td>\n",
              "      <td>-0.417432</td>\n",
              "      <td>0.024154</td>\n",
              "      <td>0.216279</td>\n",
              "      <td>-0.060858</td>\n",
              "      <td>0.727997</td>\n",
              "      <td>-0.428053</td>\n",
              "      <td>0.269999</td>\n",
              "      <td>-0.264617</td>\n",
              "      <td>0.174458</td>\n",
              "      <td>-0.049764</td>\n",
              "      <td>0.479403</td>\n",
              "      <td>0.235735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.167074</td>\n",
              "      <td>-0.798584</td>\n",
              "      <td>-0.346666</td>\n",
              "      <td>-0.463683</td>\n",
              "      <td>-0.125071</td>\n",
              "      <td>-0.276027</td>\n",
              "      <td>-0.208865</td>\n",
              "      <td>0.138834</td>\n",
              "      <td>-0.086230</td>\n",
              "      <td>-0.017895</td>\n",
              "      <td>-0.248692</td>\n",
              "      <td>0.168226</td>\n",
              "      <td>0.456018</td>\n",
              "      <td>-0.261686</td>\n",
              "      <td>0.062923</td>\n",
              "      <td>-0.336427</td>\n",
              "      <td>-0.276823</td>\n",
              "      <td>0.119404</td>\n",
              "      <td>0.571929</td>\n",
              "      <td>0.441417</td>\n",
              "      <td>-0.024522</td>\n",
              "      <td>0.412292</td>\n",
              "      <td>-0.085683</td>\n",
              "      <td>-0.287694</td>\n",
              "      <td>-0.030651</td>\n",
              "      <td>0.387611</td>\n",
              "      <td>0.239172</td>\n",
              "      <td>-0.372846</td>\n",
              "      <td>-0.460792</td>\n",
              "      <td>-0.505841</td>\n",
              "      <td>-0.717796</td>\n",
              "      <td>0.123918</td>\n",
              "      <td>0.077422</td>\n",
              "      <td>0.259605</td>\n",
              "      <td>0.282041</td>\n",
              "      <td>-0.028263</td>\n",
              "      <td>-0.214877</td>\n",
              "      <td>-0.100879</td>\n",
              "      <td>0.318270</td>\n",
              "      <td>-0.004839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2512</th>\n",
              "      <td>-0.095390</td>\n",
              "      <td>0.449202</td>\n",
              "      <td>0.224358</td>\n",
              "      <td>-0.576839</td>\n",
              "      <td>0.287336</td>\n",
              "      <td>-0.409430</td>\n",
              "      <td>-0.004248</td>\n",
              "      <td>0.780078</td>\n",
              "      <td>-0.096692</td>\n",
              "      <td>-0.372438</td>\n",
              "      <td>-0.105739</td>\n",
              "      <td>-0.333202</td>\n",
              "      <td>0.027714</td>\n",
              "      <td>0.324660</td>\n",
              "      <td>-0.215557</td>\n",
              "      <td>0.960718</td>\n",
              "      <td>0.474020</td>\n",
              "      <td>0.017450</td>\n",
              "      <td>0.015001</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>0.432339</td>\n",
              "      <td>-0.218721</td>\n",
              "      <td>-0.218295</td>\n",
              "      <td>0.222339</td>\n",
              "      <td>0.328858</td>\n",
              "      <td>0.313048</td>\n",
              "      <td>0.265837</td>\n",
              "      <td>-0.011704</td>\n",
              "      <td>-0.654497</td>\n",
              "      <td>-0.086932</td>\n",
              "      <td>0.333828</td>\n",
              "      <td>0.368232</td>\n",
              "      <td>0.113449</td>\n",
              "      <td>-0.388320</td>\n",
              "      <td>-0.254458</td>\n",
              "      <td>-0.130834</td>\n",
              "      <td>0.342358</td>\n",
              "      <td>-0.447215</td>\n",
              "      <td>0.014278</td>\n",
              "      <td>-0.027058</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.537184</td>\n",
              "      <td>-0.904238</td>\n",
              "      <td>-0.234918</td>\n",
              "      <td>-0.608328</td>\n",
              "      <td>0.132864</td>\n",
              "      <td>-0.577511</td>\n",
              "      <td>-0.558329</td>\n",
              "      <td>-0.132951</td>\n",
              "      <td>-0.165845</td>\n",
              "      <td>-0.032519</td>\n",
              "      <td>-0.064881</td>\n",
              "      <td>0.362410</td>\n",
              "      <td>-0.008946</td>\n",
              "      <td>-0.385552</td>\n",
              "      <td>0.028843</td>\n",
              "      <td>-0.635229</td>\n",
              "      <td>-0.100598</td>\n",
              "      <td>0.389611</td>\n",
              "      <td>0.144955</td>\n",
              "      <td>0.429051</td>\n",
              "      <td>-0.211813</td>\n",
              "      <td>0.540881</td>\n",
              "      <td>0.100470</td>\n",
              "      <td>-0.189920</td>\n",
              "      <td>0.252190</td>\n",
              "      <td>0.060027</td>\n",
              "      <td>0.083821</td>\n",
              "      <td>-0.448317</td>\n",
              "      <td>-0.622622</td>\n",
              "      <td>-0.158499</td>\n",
              "      <td>-0.493352</td>\n",
              "      <td>0.116451</td>\n",
              "      <td>-0.185150</td>\n",
              "      <td>-0.215702</td>\n",
              "      <td>0.311338</td>\n",
              "      <td>0.104676</td>\n",
              "      <td>-0.270898</td>\n",
              "      <td>-0.086899</td>\n",
              "      <td>0.236654</td>\n",
              "      <td>0.390968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>-0.145079</td>\n",
              "      <td>0.475734</td>\n",
              "      <td>0.365407</td>\n",
              "      <td>-0.620131</td>\n",
              "      <td>0.328844</td>\n",
              "      <td>0.151111</td>\n",
              "      <td>0.108706</td>\n",
              "      <td>0.852761</td>\n",
              "      <td>-0.457996</td>\n",
              "      <td>-0.247563</td>\n",
              "      <td>-0.029036</td>\n",
              "      <td>-0.127879</td>\n",
              "      <td>0.112376</td>\n",
              "      <td>0.168221</td>\n",
              "      <td>-0.282487</td>\n",
              "      <td>0.590009</td>\n",
              "      <td>0.499115</td>\n",
              "      <td>0.254812</td>\n",
              "      <td>0.427291</td>\n",
              "      <td>-0.123747</td>\n",
              "      <td>0.638318</td>\n",
              "      <td>-0.070385</td>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.335780</td>\n",
              "      <td>0.097913</td>\n",
              "      <td>0.173838</td>\n",
              "      <td>0.614560</td>\n",
              "      <td>0.106443</td>\n",
              "      <td>-0.349500</td>\n",
              "      <td>-0.058919</td>\n",
              "      <td>0.505293</td>\n",
              "      <td>-0.064001</td>\n",
              "      <td>-0.172986</td>\n",
              "      <td>-0.075850</td>\n",
              "      <td>-0.132227</td>\n",
              "      <td>-0.384453</td>\n",
              "      <td>0.370201</td>\n",
              "      <td>-0.468317</td>\n",
              "      <td>-0.201656</td>\n",
              "      <td>0.048853</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.265857</td>\n",
              "      <td>-0.223197</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.323132</td>\n",
              "      <td>0.073406</td>\n",
              "      <td>-0.371595</td>\n",
              "      <td>-0.044837</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>0.018509</td>\n",
              "      <td>0.258279</td>\n",
              "      <td>-0.317874</td>\n",
              "      <td>0.332849</td>\n",
              "      <td>0.212638</td>\n",
              "      <td>0.056430</td>\n",
              "      <td>0.370784</td>\n",
              "      <td>-0.076809</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.098970</td>\n",
              "      <td>0.058247</td>\n",
              "      <td>0.772120</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.695418</td>\n",
              "      <td>0.071432</td>\n",
              "      <td>-0.349727</td>\n",
              "      <td>0.203655</td>\n",
              "      <td>0.144502</td>\n",
              "      <td>0.156430</td>\n",
              "      <td>-0.250324</td>\n",
              "      <td>-0.374293</td>\n",
              "      <td>-0.250994</td>\n",
              "      <td>-0.330648</td>\n",
              "      <td>-0.096069</td>\n",
              "      <td>0.097628</td>\n",
              "      <td>-0.629242</td>\n",
              "      <td>0.276352</td>\n",
              "      <td>-0.184843</td>\n",
              "      <td>-0.126158</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.549695</td>\n",
              "      <td>0.128526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>-0.090581</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.374877</td>\n",
              "      <td>-0.471052</td>\n",
              "      <td>0.101587</td>\n",
              "      <td>0.137666</td>\n",
              "      <td>0.020140</td>\n",
              "      <td>0.438660</td>\n",
              "      <td>-0.227439</td>\n",
              "      <td>-0.421619</td>\n",
              "      <td>-0.015464</td>\n",
              "      <td>-0.266985</td>\n",
              "      <td>-0.196794</td>\n",
              "      <td>0.108737</td>\n",
              "      <td>-0.082186</td>\n",
              "      <td>0.811929</td>\n",
              "      <td>0.190508</td>\n",
              "      <td>0.046835</td>\n",
              "      <td>-0.133266</td>\n",
              "      <td>-0.312746</td>\n",
              "      <td>0.837726</td>\n",
              "      <td>-0.376351</td>\n",
              "      <td>0.239629</td>\n",
              "      <td>0.392617</td>\n",
              "      <td>0.293395</td>\n",
              "      <td>0.253092</td>\n",
              "      <td>0.570215</td>\n",
              "      <td>-0.252462</td>\n",
              "      <td>-0.398234</td>\n",
              "      <td>-0.221244</td>\n",
              "      <td>0.518364</td>\n",
              "      <td>0.045404</td>\n",
              "      <td>0.120460</td>\n",
              "      <td>-0.335099</td>\n",
              "      <td>-0.117464</td>\n",
              "      <td>-0.292256</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>-0.173357</td>\n",
              "      <td>0.191883</td>\n",
              "      <td>0.055143</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.203764</td>\n",
              "      <td>-0.207143</td>\n",
              "      <td>-0.417275</td>\n",
              "      <td>-0.421830</td>\n",
              "      <td>0.229466</td>\n",
              "      <td>-0.182841</td>\n",
              "      <td>0.209093</td>\n",
              "      <td>-0.340755</td>\n",
              "      <td>-0.520002</td>\n",
              "      <td>0.335023</td>\n",
              "      <td>0.064961</td>\n",
              "      <td>0.460892</td>\n",
              "      <td>0.196377</td>\n",
              "      <td>0.173352</td>\n",
              "      <td>0.518383</td>\n",
              "      <td>0.143448</td>\n",
              "      <td>0.168927</td>\n",
              "      <td>-0.019226</td>\n",
              "      <td>-0.087348</td>\n",
              "      <td>0.389946</td>\n",
              "      <td>-0.294117</td>\n",
              "      <td>0.699093</td>\n",
              "      <td>0.011120</td>\n",
              "      <td>-0.384840</td>\n",
              "      <td>-0.081319</td>\n",
              "      <td>0.028739</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>-0.147006</td>\n",
              "      <td>-0.560598</td>\n",
              "      <td>-0.503186</td>\n",
              "      <td>-0.468980</td>\n",
              "      <td>0.193140</td>\n",
              "      <td>0.157212</td>\n",
              "      <td>-0.501378</td>\n",
              "      <td>0.166521</td>\n",
              "      <td>-0.202339</td>\n",
              "      <td>-0.203073</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>0.301909</td>\n",
              "      <td>0.507903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3886</th>\n",
              "      <td>-0.490045</td>\n",
              "      <td>0.403811</td>\n",
              "      <td>0.233433</td>\n",
              "      <td>-1.101597</td>\n",
              "      <td>0.230473</td>\n",
              "      <td>-0.140833</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>0.423244</td>\n",
              "      <td>-0.396777</td>\n",
              "      <td>-0.049143</td>\n",
              "      <td>0.206543</td>\n",
              "      <td>-0.689884</td>\n",
              "      <td>0.505751</td>\n",
              "      <td>-0.306384</td>\n",
              "      <td>-0.464576</td>\n",
              "      <td>0.924472</td>\n",
              "      <td>0.342408</td>\n",
              "      <td>0.543019</td>\n",
              "      <td>0.400223</td>\n",
              "      <td>-0.173314</td>\n",
              "      <td>0.342301</td>\n",
              "      <td>-0.150971</td>\n",
              "      <td>-0.109750</td>\n",
              "      <td>-0.179589</td>\n",
              "      <td>0.230435</td>\n",
              "      <td>0.035653</td>\n",
              "      <td>0.412052</td>\n",
              "      <td>-0.130912</td>\n",
              "      <td>-0.566213</td>\n",
              "      <td>-0.381750</td>\n",
              "      <td>0.798290</td>\n",
              "      <td>0.468334</td>\n",
              "      <td>0.196522</td>\n",
              "      <td>0.037070</td>\n",
              "      <td>0.054352</td>\n",
              "      <td>-0.617227</td>\n",
              "      <td>0.368253</td>\n",
              "      <td>-0.336719</td>\n",
              "      <td>0.157408</td>\n",
              "      <td>0.318117</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136145</td>\n",
              "      <td>-0.158449</td>\n",
              "      <td>0.145648</td>\n",
              "      <td>-0.330110</td>\n",
              "      <td>0.095605</td>\n",
              "      <td>-0.911029</td>\n",
              "      <td>-0.152521</td>\n",
              "      <td>-0.277227</td>\n",
              "      <td>-0.283700</td>\n",
              "      <td>0.005756</td>\n",
              "      <td>-0.165566</td>\n",
              "      <td>0.285174</td>\n",
              "      <td>0.500378</td>\n",
              "      <td>-0.387482</td>\n",
              "      <td>0.691950</td>\n",
              "      <td>-0.060325</td>\n",
              "      <td>-0.550582</td>\n",
              "      <td>0.222612</td>\n",
              "      <td>0.019106</td>\n",
              "      <td>1.069099</td>\n",
              "      <td>-0.530565</td>\n",
              "      <td>0.136196</td>\n",
              "      <td>0.366861</td>\n",
              "      <td>-0.426256</td>\n",
              "      <td>0.226655</td>\n",
              "      <td>0.038184</td>\n",
              "      <td>0.256335</td>\n",
              "      <td>0.157677</td>\n",
              "      <td>-0.489276</td>\n",
              "      <td>-0.687277</td>\n",
              "      <td>-0.630584</td>\n",
              "      <td>-0.016984</td>\n",
              "      <td>-0.350730</td>\n",
              "      <td>-0.172616</td>\n",
              "      <td>0.260950</td>\n",
              "      <td>-0.729053</td>\n",
              "      <td>-0.367372</td>\n",
              "      <td>-0.624519</td>\n",
              "      <td>0.301135</td>\n",
              "      <td>0.294430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7315</th>\n",
              "      <td>0.167208</td>\n",
              "      <td>0.299286</td>\n",
              "      <td>0.491610</td>\n",
              "      <td>-0.444910</td>\n",
              "      <td>-0.113778</td>\n",
              "      <td>-0.170792</td>\n",
              "      <td>0.454244</td>\n",
              "      <td>0.180629</td>\n",
              "      <td>-0.194229</td>\n",
              "      <td>-0.268134</td>\n",
              "      <td>-0.049930</td>\n",
              "      <td>-0.278949</td>\n",
              "      <td>0.090974</td>\n",
              "      <td>0.260803</td>\n",
              "      <td>-0.046972</td>\n",
              "      <td>0.854258</td>\n",
              "      <td>0.046183</td>\n",
              "      <td>0.218709</td>\n",
              "      <td>-0.057021</td>\n",
              "      <td>-0.069000</td>\n",
              "      <td>0.872898</td>\n",
              "      <td>-0.121805</td>\n",
              "      <td>0.250340</td>\n",
              "      <td>0.507785</td>\n",
              "      <td>0.031016</td>\n",
              "      <td>0.500577</td>\n",
              "      <td>0.406960</td>\n",
              "      <td>-0.329940</td>\n",
              "      <td>-0.440141</td>\n",
              "      <td>0.126961</td>\n",
              "      <td>0.761491</td>\n",
              "      <td>0.009389</td>\n",
              "      <td>0.311035</td>\n",
              "      <td>-0.478103</td>\n",
              "      <td>-0.139362</td>\n",
              "      <td>-0.231615</td>\n",
              "      <td>0.203378</td>\n",
              "      <td>-0.122280</td>\n",
              "      <td>0.229558</td>\n",
              "      <td>0.164038</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.222196</td>\n",
              "      <td>-0.372110</td>\n",
              "      <td>-0.287214</td>\n",
              "      <td>-0.214754</td>\n",
              "      <td>-0.091732</td>\n",
              "      <td>-0.311470</td>\n",
              "      <td>0.151199</td>\n",
              "      <td>-0.145242</td>\n",
              "      <td>-0.185514</td>\n",
              "      <td>0.069422</td>\n",
              "      <td>-0.001048</td>\n",
              "      <td>0.263794</td>\n",
              "      <td>0.176082</td>\n",
              "      <td>-0.525270</td>\n",
              "      <td>0.330416</td>\n",
              "      <td>-0.291158</td>\n",
              "      <td>-0.033593</td>\n",
              "      <td>0.270450</td>\n",
              "      <td>0.091955</td>\n",
              "      <td>0.530226</td>\n",
              "      <td>-0.025298</td>\n",
              "      <td>0.296416</td>\n",
              "      <td>0.235689</td>\n",
              "      <td>-0.678318</td>\n",
              "      <td>0.360752</td>\n",
              "      <td>0.281592</td>\n",
              "      <td>-0.151794</td>\n",
              "      <td>-0.625438</td>\n",
              "      <td>-0.146655</td>\n",
              "      <td>0.218224</td>\n",
              "      <td>-0.572947</td>\n",
              "      <td>-0.211265</td>\n",
              "      <td>0.197571</td>\n",
              "      <td>-0.295656</td>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.073510</td>\n",
              "      <td>-0.218864</td>\n",
              "      <td>-0.075352</td>\n",
              "      <td>0.194545</td>\n",
              "      <td>0.245993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7200 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       765       766       767\n",
              "5614 -0.340161 -0.148656  0.602034  ... -0.017511  0.452152  0.481604\n",
              "2383 -0.391934  0.589659  0.081654  ...  0.045654  0.267326  0.322551\n",
              "5448 -0.311391  0.246223  0.322206  ... -0.658689  0.190388  0.197619\n",
              "8412 -0.157567  0.439942  0.159633  ... -0.413154  0.299626  0.228981\n",
              "1448 -0.166418  0.318898  0.330421  ... -0.100879  0.318270 -0.004839\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "2512 -0.095390  0.449202  0.224358  ... -0.086899  0.236654  0.390968\n",
              "8694 -0.145079  0.475734  0.365407  ...  0.001324  0.549695  0.128526\n",
              "8368 -0.090581  0.432085  0.374877  ...  0.063565  0.301909  0.507903\n",
              "3886 -0.490045  0.403811  0.233433  ... -0.624519  0.301135  0.294430\n",
              "7315  0.167208  0.299286  0.491610  ... -0.075352  0.194545  0.245993\n",
              "\n",
              "[7200 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "outputId": "87fb78ca-c846-47d3-ee3b-5b5a11147d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "outputId": "63f59174-dc76-41ea-9d09-bb2756febbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "outputId": "d0bd4443-5959-4302-c4e6-3e2b5b5f3d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "units = 1024\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "c = concatenate([x, c])\n",
        "\n",
        "def FFUnit(c):\n",
        "  x = Dense(units)(c)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  c = concatenate([x, c])\n",
        "  return c\n",
        "\n",
        "for i in range(10):\n",
        "  c = FFUnit(c)\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=32)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=32,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=32,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         787456      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024)         4096        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1024)         0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2048)         0           activation_1[0][0]               \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         2098176     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1024)         0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3072)         0           activation_2[0][0]               \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         3146752     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024)         4096        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1024)         0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 4096)         0           activation_3[0][0]               \n",
            "                                                                 concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1024)         0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 1024)         0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5120)         0           activation_4[0][0]               \n",
            "                                                                 concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         5243904     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 1024)         4096        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1024)         0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 1024)         0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 6144)         0           activation_5[0][0]               \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1024)         6292480     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 1024)         4096        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 1024)         0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 1024)         0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 7168)         0           activation_6[0][0]               \n",
            "                                                                 concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         7341056     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 1024)         4096        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1024)         0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 1024)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8192)         0           activation_7[0][0]               \n",
            "                                                                 concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1024)         8389632     concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 1024)         4096        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 1024)         0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 1024)         0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 9216)         0           activation_8[0][0]               \n",
            "                                                                 concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1024)         9438208     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 1024)         4096        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 1024)         0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 1024)         0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 10240)        0           activation_9[0][0]               \n",
            "                                                                 concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 1024)         10486784    concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 1024)         4096        dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 1024)         0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 1024)         0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 11264)        0           activation_10[0][0]              \n",
            "                                                                 concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1024)         11535360    concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 1024)         4096        dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 1024)         0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 1024)         0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 12288)        0           activation_11[0][0]              \n",
            "                                                                 concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 3)            36867       concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 3)            12          dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 3)            0           batch_normalization_12[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 69,037,071\n",
            "Trainable params: 69,014,537\n",
            "Non-trainable params: 22,534\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7200/7200 [==============================] - 23s 3ms/step - loss: 0.3484 - acc: 0.9339 - val_loss: 0.2847 - val_acc: 0.9444\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2394 - acc: 0.9657 - val_loss: 0.2252 - val_acc: 0.9689\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2065 - acc: 0.9694 - val_loss: 0.2339 - val_acc: 0.9589\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1723 - acc: 0.9762 - val_loss: 0.2097 - val_acc: 0.9456\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1506 - acc: 0.9808 - val_loss: 0.1580 - val_acc: 0.9711\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 15s 2ms/step - loss: 0.1307 - acc: 0.9839 - val_loss: 0.1450 - val_acc: 0.9811\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1173 - acc: 0.9881 - val_loss: 0.1375 - val_acc: 0.9789\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1149 - acc: 0.9878 - val_loss: 0.1325 - val_acc: 0.9778\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.1005 - acc: 0.9914 - val_loss: 0.1112 - val_acc: 0.9833\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0996 - acc: 0.9900 - val_loss: 0.1222 - val_acc: 0.9778\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0890 - acc: 0.9919 - val_loss: 0.1118 - val_acc: 0.9778\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0858 - acc: 0.9935 - val_loss: 0.1086 - val_acc: 0.9833\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0828 - acc: 0.9919 - val_loss: 0.0927 - val_acc: 0.9867\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0743 - acc: 0.9947 - val_loss: 0.0898 - val_acc: 0.9878\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0738 - acc: 0.9931 - val_loss: 0.0959 - val_acc: 0.9833\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0722 - acc: 0.9932 - val_loss: 0.0893 - val_acc: 0.9856\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0626 - acc: 0.9962 - val_loss: 0.1047 - val_acc: 0.9767\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0604 - acc: 0.9956 - val_loss: 0.0898 - val_acc: 0.9811\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0588 - acc: 0.9960 - val_loss: 0.0838 - val_acc: 0.9811\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0514 - acc: 0.9969 - val_loss: 0.0867 - val_acc: 0.9811\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0512 - acc: 0.9958 - val_loss: 0.0813 - val_acc: 0.9800\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0474 - acc: 0.9971 - val_loss: 0.0816 - val_acc: 0.9844\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0497 - acc: 0.9957 - val_loss: 0.0885 - val_acc: 0.9789\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0459 - acc: 0.9971 - val_loss: 0.0763 - val_acc: 0.9833\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0420 - acc: 0.9972 - val_loss: 0.0722 - val_acc: 0.9811\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0394 - acc: 0.9979 - val_loss: 0.0697 - val_acc: 0.9856\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0395 - acc: 0.9962 - val_loss: 0.0743 - val_acc: 0.9822\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0374 - acc: 0.9975 - val_loss: 0.0685 - val_acc: 0.9856\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0347 - acc: 0.9981 - val_loss: 0.0561 - val_acc: 0.9900\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0315 - acc: 0.9982 - val_loss: 0.0566 - val_acc: 0.9856\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0295 - acc: 0.9985 - val_loss: 0.0680 - val_acc: 0.9844\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0304 - acc: 0.9981 - val_loss: 0.0615 - val_acc: 0.9833\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0250 - acc: 0.9994 - val_loss: 0.0558 - val_acc: 0.9889\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0268 - acc: 0.9985 - val_loss: 0.0541 - val_acc: 0.9900\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0244 - acc: 0.9986 - val_loss: 0.0503 - val_acc: 0.9878\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0247 - acc: 0.9978 - val_loss: 0.0575 - val_acc: 0.9822\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0231 - acc: 0.9988 - val_loss: 0.0581 - val_acc: 0.9833\n",
            "Epoch 33/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0262 - acc: 0.9972 - val_loss: 0.0571 - val_acc: 0.9856\n",
            "Epoch 34/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0240 - acc: 0.9978 - val_loss: 0.0834 - val_acc: 0.9800\n",
            "Epoch 35/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0198 - acc: 0.9986 - val_loss: 0.0470 - val_acc: 0.9889\n",
            "Epoch 36/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0197 - acc: 0.9981 - val_loss: 0.0661 - val_acc: 0.9833\n",
            "Epoch 37/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0187 - acc: 0.9992 - val_loss: 0.0572 - val_acc: 0.9856\n",
            "Epoch 38/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0189 - acc: 0.9986 - val_loss: 0.0629 - val_acc: 0.9833\n",
            "Epoch 39/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0155 - acc: 0.9994 - val_loss: 0.0582 - val_acc: 0.9856\n",
            "Epoch 40/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0169 - acc: 0.9986 - val_loss: 0.0724 - val_acc: 0.9800\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 15s 2ms/step - loss: 0.0186 - acc: 0.9992 - val_loss: 0.0465 - val_acc: 0.9867\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0190 - acc: 0.9988 - val_loss: 0.0457 - val_acc: 0.9889\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0170 - acc: 0.9988 - val_loss: 0.0515 - val_acc: 0.9844\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0164 - acc: 0.9993 - val_loss: 0.0499 - val_acc: 0.9889\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0151 - acc: 0.9996 - val_loss: 0.0518 - val_acc: 0.9867\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.0171 - acc: 0.9988 - val_loss: 0.0565 - val_acc: 0.9856\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0154 - acc: 0.9994 - val_loss: 0.0572 - val_acc: 0.9822\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 157us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06356117938955624, 0.9788888888888889]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}