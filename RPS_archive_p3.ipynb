{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/volkfox/CS221-Final_Project/blob/master/RPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6o7cfFrJqOn"
   },
   "outputs": [],
   "source": [
    "# AWS p3.xlarge instance, notebook v6 \n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHn_26BHO5C5"
   },
   "source": [
    "This jupyter sheet is an AWS-based RPS generator. \n",
    "For it to work, runtime has to be supported by GPU with sufficient memory (does not happen often on free tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyVYZnZW7JVo"
   },
   "outputs": [],
   "source": [
    "# you need 2 files to run the cells:\n",
    "# Nabokov-all.txt (or other style source)\n",
    "# donor.csv (content in csv with columns 'text' and 'author'), also see below that style author is dropped\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @kernel-restart\n",
    "# Ayne Rand \"Atlas shrugged\" short 1\n",
    "\n",
    "input = \"\"\"She sat at the window of the train, her head thrown back, one leg stretched across to the empty seat\n",
    "before her. The window frame trembled with the speed of the motion, the pane hung over empty\n",
    "darkness, and dots of light slashed across the glass as luminous streaks, once in a while.\n",
    "\n",
    "Her leg, sculptured by the tight sheen of the stocking, its long line running straight, over an arched\n",
    "instep, to the tip of a foot in a high-heeled pump, had a feminine elegance that seemed out of place in\n",
    "the dusty train car and oddly incongruous with the rest of her. She wore a battered camel's hair coat\n",
    "that had been expensive, wrapped shapelessly about her slender, nervous body. The coat collar was\n",
    "raised to the slanting brim of her hat. A sweep of brown hair fell back, almost touching the line of her\n",
    "shoulders. Her face was made of angular planes, the shape of her mouth clear-cut, a sensual mouth\n",
    "held closed with inflexible precision. She kept her hands in the coat pockets, her posture taut, as if\n",
    "she resented immobility, and unfeminine, as if she were unconscious of her own body and that it was\n",
    "a woman's body. She sat listening to the music. It was a symphony of triumph. The notes flowed up,\n",
    "they spoke of rising and they were the rising itself, they were the essence and the form of upward\n",
    "motion, they seemed to embody every human act and thought that had ascent as its motive. It was a\n",
    "sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the\n",
    "tension of purpose. It swept space clean, and left nothing but the joy of an unobstructed effort. Only a\n",
    "faint echo within the sounds spoke of that from which the music had escaped, but spoke in laughing\n",
    "astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was\n",
    "the song of an immense deliverance.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "5YaJs3eY9FL5",
    "outputId": "7f24633a-9846-43d2-d450-9e3f5cef1ff3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one-time\n",
    "# install dependencies if needed \n",
    "\n",
    "!pip install sacremoses\n",
    "!pip install gpt-2-simple \n",
    "!pip install nltk\n",
    "!pip install wrapt --upgrade --ignore-installed\n",
    "!pip install --upgrade tensorflow-gpu==1.14\n",
    "!pip install tensorflow-hub \n",
    "\n",
    "# instructions to enable 774M model:\n",
    "#\n",
    "# vi /home/ubuntu/anaconda3/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\n",
    "# comment out line: assert model_name not in ['774M', '1558M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ztLigiKA9Tle",
    "outputId": "da0955e4-01fe-4575-feef-dc179f071406",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @kernel restart\n",
    "\n",
    "import sys, getopt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from sacremoses import MosesDetokenizer\n",
    "import gpt_2_simple as gpt2\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bh5M0YtAbTLt",
    "outputId": "372e1752-ef7b-42df-dfec-0454da8467eb"
   },
   "outputs": [],
   "source": [
    "# one-time\n",
    "# download tokenizer file\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5sSsJ7--ERt"
   },
   "outputs": [],
   "source": [
    "# @kernel restart\n",
    "# GPT-2 released three models\n",
    "#model_name=\"117M\"\n",
    "model_name=\"774M\"\n",
    "\n",
    "#model_name=\"345M\"\n",
    "\n",
    "# hyperparameters\n",
    "minsimilarity = 0.68 # rejection threshold\n",
    "# lexeme bounds\n",
    "minwords = 2         # min lexeme length\n",
    "softmaxwords = 4     # min lexeme ending in -and-\n",
    "maxwords = 7        # max lexeme without a clear end\n",
    "# generator parameters\n",
    "temperature = 1.0    # generator madness\n",
    "seedLexemes = 10     # how many lexemes to put in a prior\n",
    "nsamples = 10000      # number of samples generated to choose from\n",
    "\n",
    "\n",
    "bestLexemes = []\n",
    "output = \"\"\n",
    "\n",
    "# this is your style for GPT-2 finetuning\n",
    "style = \"Nabokov-All.txt\"\n",
    "\n",
    "# content input and output\n",
    "generator_name = 'Trained-8K'\n",
    "outfile = generator_name+\"_\"+model_name+\"_\"+str(nsamples)+\"_\"+style\n",
    "sfile = outfile+\".stats\"\n",
    "\n",
    "# get around resource exhaustion for embeddings\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "        #device_count = {'GPU': 1}\n",
    "    )\n",
    "\n",
    "# read text data to transform\n",
    "data = pd.read_csv(\"donor.csv\")\n",
    "data = data[data['author']!='Nabokov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "RCuLOVes-Jvo",
    "outputId": "9b0f226b-84ee-4131-cdd2-d77155e800fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# @kernel restart\n",
    "# init tokenizer and sentence embedder\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "dt = MosesDetokenizer()\n",
    "\n",
    "# Google Sentence encoder v2 appears worse than V1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
    "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "embed = hub.Module(module_url)\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "B3XQ_r2RD3MN",
    "outputId": "a5fd5aa0-8f24-4738-9491-0fd5e828f17e"
   },
   "outputs": [],
   "source": [
    "# download GPT-2 model (only needed once)\n",
    "gpt2.download_gpt2(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 24993
    },
    "colab_type": "code",
    "id": "UO6-wr7dGs67",
    "outputId": "6ed2870e-1b59-4ced-86c1-6fb0dff58a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/345M/model.ckpt\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 2363594 tokens\n",
      "Training...\n",
      "Saving checkpoint/run1/model-0\n",
      "[1 | 13.38] loss=3.51 avg=3.51\n",
      "[2 | 13.88] loss=3.67 avg=3.59\n",
      "[3 | 14.39] loss=3.71 avg=3.63\n",
      "[4 | 14.89] loss=3.44 avg=3.58\n",
      "[5 | 15.39] loss=3.19 avg=3.50\n",
      "[6 | 15.89] loss=3.29 avg=3.47\n",
      "[7 | 16.40] loss=3.50 avg=3.47\n",
      "[8 | 16.90] loss=4.28 avg=3.58\n",
      "[9 | 17.40] loss=3.83 avg=3.61\n",
      "[10 | 17.91] loss=4.22 avg=3.67\n",
      "[11 | 18.41] loss=3.87 avg=3.69\n",
      "[12 | 18.91] loss=2.72 avg=3.60\n",
      "[13 | 19.42] loss=2.83 avg=3.54\n",
      "[14 | 19.92] loss=4.13 avg=3.58\n",
      "[15 | 20.43] loss=2.97 avg=3.54\n",
      "[16 | 20.93] loss=3.49 avg=3.54\n",
      "[17 | 21.43] loss=3.87 avg=3.56\n",
      "[18 | 21.94] loss=4.01 avg=3.59\n",
      "[19 | 22.44] loss=4.64 avg=3.65\n",
      "[20 | 22.95] loss=2.95 avg=3.61\n",
      "[21 | 23.45] loss=3.67 avg=3.61\n",
      "[22 | 23.95] loss=4.03 avg=3.63\n",
      "[23 | 24.45] loss=3.85 avg=3.64\n",
      "[24 | 24.96] loss=3.26 avg=3.63\n",
      "[25 | 25.46] loss=3.12 avg=3.60\n",
      "[26 | 25.97] loss=3.23 avg=3.59\n",
      "[27 | 26.47] loss=3.92 avg=3.60\n",
      "[28 | 26.98] loss=3.96 avg=3.62\n",
      "[29 | 27.48] loss=3.46 avg=3.61\n",
      "[30 | 27.99] loss=3.39 avg=3.60\n",
      "[31 | 28.49] loss=3.33 avg=3.59\n",
      "[32 | 29.00] loss=4.20 avg=3.61\n",
      "[33 | 29.50] loss=3.01 avg=3.59\n",
      "[34 | 30.01] loss=3.16 avg=3.58\n",
      "[35 | 30.52] loss=3.31 avg=3.57\n",
      "[36 | 31.02] loss=3.68 avg=3.57\n",
      "[37 | 31.53] loss=3.63 avg=3.57\n",
      "[38 | 32.03] loss=3.43 avg=3.57\n",
      "[39 | 32.54] loss=3.91 avg=3.58\n",
      "[40 | 33.05] loss=3.62 avg=3.58\n",
      "[41 | 33.55] loss=4.03 avg=3.59\n",
      "[42 | 34.06] loss=3.38 avg=3.59\n",
      "[43 | 34.56] loss=3.65 avg=3.59\n",
      "[44 | 35.07] loss=3.46 avg=3.59\n",
      "[45 | 35.58] loss=3.90 avg=3.59\n",
      "[46 | 36.08] loss=3.89 avg=3.60\n",
      "[47 | 36.59] loss=3.54 avg=3.60\n",
      "[48 | 37.10] loss=4.10 avg=3.61\n",
      "[49 | 37.60] loss=4.06 avg=3.63\n",
      "[50 | 38.11] loss=3.16 avg=3.61\n",
      "[51 | 38.61] loss=3.73 avg=3.62\n",
      "[52 | 39.12] loss=3.29 avg=3.61\n",
      "[53 | 39.63] loss=3.55 avg=3.61\n",
      "[54 | 40.13] loss=3.20 avg=3.60\n",
      "[55 | 40.64] loss=3.41 avg=3.59\n",
      "[56 | 41.15] loss=3.26 avg=3.59\n",
      "[57 | 41.66] loss=3.20 avg=3.58\n",
      "[58 | 42.16] loss=3.48 avg=3.57\n",
      "[59 | 42.67] loss=3.11 avg=3.56\n",
      "[60 | 43.18] loss=4.26 avg=3.58\n",
      "[61 | 43.69] loss=4.23 avg=3.59\n",
      "[62 | 44.19] loss=3.39 avg=3.59\n",
      "[63 | 44.70] loss=3.63 avg=3.59\n",
      "[64 | 45.20] loss=3.63 avg=3.59\n",
      "[65 | 45.71] loss=3.78 avg=3.59\n",
      "[66 | 46.22] loss=3.53 avg=3.59\n",
      "[67 | 46.73] loss=3.08 avg=3.58\n",
      "[68 | 47.23] loss=4.13 avg=3.59\n",
      "[69 | 47.74] loss=3.84 avg=3.60\n",
      "[70 | 48.25] loss=2.91 avg=3.59\n",
      "[71 | 48.75] loss=3.94 avg=3.59\n",
      "[72 | 49.26] loss=3.77 avg=3.60\n",
      "[73 | 49.77] loss=3.57 avg=3.60\n",
      "[74 | 50.27] loss=3.79 avg=3.60\n",
      "[75 | 50.78] loss=3.61 avg=3.60\n",
      "[76 | 51.29] loss=3.50 avg=3.60\n",
      "[77 | 51.79] loss=3.77 avg=3.60\n",
      "[78 | 52.30] loss=2.86 avg=3.59\n",
      "[79 | 52.81] loss=3.39 avg=3.58\n",
      "[80 | 53.32] loss=4.13 avg=3.59\n",
      "[81 | 53.82] loss=2.61 avg=3.58\n",
      "[82 | 54.33] loss=3.47 avg=3.57\n",
      "[83 | 54.83] loss=4.04 avg=3.58\n",
      "[84 | 55.34] loss=3.44 avg=3.58\n",
      "[85 | 55.85] loss=3.78 avg=3.58\n",
      "[86 | 56.35] loss=4.34 avg=3.60\n",
      "[87 | 56.86] loss=3.78 avg=3.60\n",
      "[88 | 57.36] loss=3.35 avg=3.59\n",
      "[89 | 57.87] loss=3.93 avg=3.60\n",
      "[90 | 58.38] loss=3.64 avg=3.60\n",
      "[91 | 58.89] loss=3.50 avg=3.60\n",
      "[92 | 59.39] loss=3.11 avg=3.59\n",
      "[93 | 59.90] loss=3.38 avg=3.59\n",
      "[94 | 60.41] loss=2.95 avg=3.58\n",
      "[95 | 60.91] loss=3.93 avg=3.58\n",
      "[96 | 61.42] loss=4.15 avg=3.59\n",
      "[97 | 61.93] loss=3.59 avg=3.59\n",
      "[98 | 62.44] loss=3.78 avg=3.60\n",
      "[99 | 62.94] loss=3.16 avg=3.59\n",
      "[100 | 63.45] loss=4.16 avg=3.60\n",
      "======== SAMPLE 1 ========\n",
      " of one's life has not to be such difficult an affair, as to be confined under such oppressive influences, or (how to be so exact) even to have to be so cruel, as to have to be killed by a human being, whose death must surely happen on this earth, so long as all men are equally alive, so long as every one in this world is immortal and immortal: I do not deny, however, that life can be so cruel a thing. However, the only thing better than Death, as well as The New Man, is Life itself, and that I think one is a living thing. I do not think a boy boy of twelve has an opinion as to how much life in youth is but a very minute and limited one. There is very little life in men except the life which the soul of the young man has enjoyed. The only life which that is—and that must have been something new— is Life. That is all there is; life. Life is the only life we have; life can be cruel as long as many men are immortal and immortal. Life can be as bad as the worst things. That is all. You must not make any mistake, and I cannot speak against you on this point. That should also lie under your head, that is Life. In life—and let us forget the good part of it—there is no life at all, except that you have got some, just a little. There is Life in your heart, just as there is Life in the life of a child. That is the only life. You take your life off Earth, there must be some living human within it. Of course, those are your enemies. Life, I cannot say exactly what it is, but I know that it is just as bad on Earth; but, in the end, when all life is extinct, there will come a day we shall again have a chance of enjoying a better one. Life is so hard, you have to bear the pain of it; you must go through it too, and the pain is just as good or equally as great. It can be much better than that.\n",
      "\n",
      "But I cannot give a word on this matter. You are so good. You must have read Life, and you must have read what other men have written about Life. I am not saying anything I have not already said. I know that life really is hard. I am not saying life is easy. You have got to give your life for our country, and to live in a good country is more important than it ever was before. That is a fact. The country must have such a life, the world must have such a life, and, if it is to exist at all, this will only come when all men are immortal, when all men are immortal beings. In the end it will mean only one thing: it will mean that, before all men are immortal, we shall have a little more chance to enjoy a truly better life than we have now; and I am going to tell you something to which I have read so many books about Life. The same thing I read about in those books. Just go out, and try to find the author. How about this: if you find one (a famous author) who writes about Life, if you want to know when there is something new—let him write your biography. If a book is not new he is an old book, an old book. A novelist makes a new book. Let me give you a hint: if you find one writing about Life, tell him, \"Let us make that novel.\" Then he can write your biography, which is the hardest and the best thing he could do in life. Then you are in Heaven and he does not have to go through the same sorrow and the same hardships of death. Well, how can you imagine the life of a novelist if he has to write an autobiography? So, you see—and here is where I have to come back—there is Life in the novelist. It is simply the greatest art, if one is alive. How should I put it? The novelist is the most skilled and most sensitive man in the land of our childhood. His art is exactly the same as that which life itself is. You read in Life that when one was young one was killed. But when you are old, even with the help of a friend, and when everything is over, Death will put you out of your misery, all right, you will be all right again. The same thing. Well, give me an example; I have some with me. Life is only temporary. When the world has ended we shall have to meet it again for a while. That is what death is about. Well, what you must say to that is, that there are other things, such as a boy—well, I was told he has lost his mind. I suppose he has been dead a week since and then some other horrible thing happened. Well, that is what I mean.\n",
      "\n",
      "[101 | 84.54] loss=3.42 avg=3.59\n",
      "[102 | 85.04] loss=3.76 avg=3.60\n",
      "[103 | 85.55] loss=3.70 avg=3.60\n",
      "[104 | 86.06] loss=3.79 avg=3.60\n",
      "[105 | 86.56] loss=4.21 avg=3.61\n",
      "[106 | 87.07] loss=3.65 avg=3.61\n",
      "[107 | 87.58] loss=3.26 avg=3.61\n",
      "[108 | 88.09] loss=3.39 avg=3.60\n",
      "[109 | 88.59] loss=3.05 avg=3.59\n",
      "[110 | 89.10] loss=3.09 avg=3.59\n",
      "[111 | 89.60] loss=3.53 avg=3.59\n",
      "[112 | 90.11] loss=2.98 avg=3.58\n",
      "[113 | 90.61] loss=3.40 avg=3.57\n",
      "[114 | 91.12] loss=3.57 avg=3.57\n",
      "[115 | 91.63] loss=3.80 avg=3.58\n",
      "[116 | 92.14] loss=3.16 avg=3.57\n",
      "[117 | 92.64] loss=4.04 avg=3.58\n",
      "[118 | 93.15] loss=2.62 avg=3.57\n",
      "[119 | 93.66] loss=2.88 avg=3.56\n",
      "[120 | 94.17] loss=4.35 avg=3.57\n",
      "[121 | 94.67] loss=4.23 avg=3.58\n",
      "[122 | 95.18] loss=2.98 avg=3.57\n",
      "[123 | 95.69] loss=3.44 avg=3.57\n",
      "[124 | 96.20] loss=3.03 avg=3.56\n",
      "[125 | 96.70] loss=3.74 avg=3.56\n",
      "[126 | 97.21] loss=3.28 avg=3.56\n",
      "[127 | 97.72] loss=3.54 avg=3.56\n",
      "[128 | 98.23] loss=2.98 avg=3.55\n",
      "[129 | 98.73] loss=2.64 avg=3.54\n",
      "[130 | 99.24] loss=3.07 avg=3.53\n",
      "[131 | 99.75] loss=3.42 avg=3.53\n",
      "[132 | 100.26] loss=4.51 avg=3.54\n",
      "[133 | 100.77] loss=3.28 avg=3.54\n",
      "[134 | 101.27] loss=3.27 avg=3.53\n",
      "[135 | 101.78] loss=2.61 avg=3.52\n",
      "[136 | 102.29] loss=3.16 avg=3.52\n",
      "[137 | 102.80] loss=3.90 avg=3.52\n",
      "[138 | 103.30] loss=3.63 avg=3.52\n",
      "[139 | 103.81] loss=3.97 avg=3.53\n",
      "[140 | 104.32] loss=3.82 avg=3.53\n",
      "[141 | 104.83] loss=3.10 avg=3.53\n",
      "[142 | 105.33] loss=4.13 avg=3.54\n",
      "[143 | 105.84] loss=4.21 avg=3.54\n",
      "[144 | 106.35] loss=3.62 avg=3.55\n",
      "[145 | 106.85] loss=3.08 avg=3.54\n",
      "[146 | 107.36] loss=3.51 avg=3.54\n",
      "[147 | 107.87] loss=3.28 avg=3.54\n",
      "[148 | 108.37] loss=3.38 avg=3.53\n",
      "[149 | 108.88] loss=3.31 avg=3.53\n",
      "[150 | 109.39] loss=3.91 avg=3.54\n",
      "[151 | 109.90] loss=3.15 avg=3.53\n",
      "[152 | 110.41] loss=2.97 avg=3.52\n",
      "[153 | 110.91] loss=3.90 avg=3.53\n",
      "[154 | 111.42] loss=2.84 avg=3.52\n",
      "[155 | 111.92] loss=3.55 avg=3.52\n",
      "[156 | 112.43] loss=3.57 avg=3.52\n",
      "[157 | 112.94] loss=2.94 avg=3.51\n",
      "[158 | 113.44] loss=2.97 avg=3.51\n",
      "[159 | 113.95] loss=3.70 avg=3.51\n",
      "[160 | 114.46] loss=3.61 avg=3.51\n",
      "[161 | 114.96] loss=3.56 avg=3.51\n",
      "[162 | 115.47] loss=3.70 avg=3.51\n",
      "[163 | 115.98] loss=3.52 avg=3.51\n",
      "[164 | 116.48] loss=3.51 avg=3.51\n",
      "[165 | 116.99] loss=3.16 avg=3.51\n",
      "[166 | 117.49] loss=3.65 avg=3.51\n",
      "[167 | 118.00] loss=3.41 avg=3.51\n",
      "[168 | 118.51] loss=3.23 avg=3.51\n",
      "[169 | 119.02] loss=3.49 avg=3.51\n",
      "[170 | 119.52] loss=3.17 avg=3.50\n",
      "[171 | 120.03] loss=4.13 avg=3.51\n",
      "[172 | 120.53] loss=3.20 avg=3.51\n",
      "[173 | 121.04] loss=3.43 avg=3.50\n",
      "[174 | 121.55] loss=2.85 avg=3.50\n",
      "[175 | 122.05] loss=2.87 avg=3.49\n",
      "[176 | 122.56] loss=4.18 avg=3.50\n",
      "[177 | 123.07] loss=4.12 avg=3.51\n",
      "[178 | 123.57] loss=2.73 avg=3.50\n",
      "[179 | 124.08] loss=3.36 avg=3.49\n",
      "[180 | 124.59] loss=4.07 avg=3.50\n",
      "[181 | 125.09] loss=3.09 avg=3.50\n",
      "[182 | 125.60] loss=3.72 avg=3.50\n",
      "[183 | 126.11] loss=2.92 avg=3.49\n",
      "[184 | 126.62] loss=3.76 avg=3.50\n",
      "[185 | 127.12] loss=3.36 avg=3.49\n",
      "[186 | 127.63] loss=3.38 avg=3.49\n",
      "[187 | 128.14] loss=2.52 avg=3.48\n",
      "[188 | 128.65] loss=4.03 avg=3.49\n",
      "[189 | 129.15] loss=3.38 avg=3.49\n",
      "[190 | 129.66] loss=3.87 avg=3.49\n",
      "[191 | 130.17] loss=4.11 avg=3.50\n",
      "[192 | 130.67] loss=3.47 avg=3.50\n",
      "[193 | 131.18] loss=3.79 avg=3.50\n",
      "[194 | 131.69] loss=2.10 avg=3.48\n",
      "[195 | 132.20] loss=3.72 avg=3.49\n",
      "[196 | 132.70] loss=4.26 avg=3.50\n",
      "[197 | 133.21] loss=3.54 avg=3.50\n",
      "[198 | 133.72] loss=3.42 avg=3.50\n",
      "[199 | 134.22] loss=3.45 avg=3.50\n",
      "[200 | 134.73] loss=3.59 avg=3.50\n",
      "======== SAMPLE 1 ========\n",
      " repeating his course, she said gently:\n",
      "\n",
      "'There's one last thing I want to mention;'\n",
      "\n",
      "'Yes, yes, I can understand that. The last thing I want to mention is…?'\n",
      "\n",
      "She didn't need to tell her partner that they were almost out of breath.\n",
      "\n",
      "'The last thing?' asked the first time.\n",
      "\n",
      "The first was the very last thing. When you see a woman, especially one whose skin is as hard as yours, and whose hair is as fine as hers… it should come in handy if she has a secret, if she will be the secret of her life.\n",
      "\n",
      "She asked herself, as she looked, if this was the proper time to make this remark, and she knew right away, already in one of her cheeks and as if on the verge, she put up the smallest blush.\n",
      "\n",
      "… but let us be frank, I don't think it would have come in handy.\n",
      "\n",
      "'You can't fool me. I'm not going to laugh at you at all. I know you're not going to laugh at me. I mean it, I understand that you feel the same way, you have been through this all before. The last thing I want to say is this.'\n",
      "\n",
      "She started in earnest, and the first part was cut out in pieces of the second. When she came to, however, the first part would not be quite finished, the next part had already been added to, and the last part had been cut out just like that, and there was an error along the way. This could easily have been corrected by taking out those parts.\n",
      "\n",
      "With a look at the whole, she asked him: 'Why did you have to come back so soon?'\n",
      "\n",
      "The first part had been started out when she got the invitation from the first man, and her response to his call was a little before she would have had a chance to think about what she should do.\n",
      "\n",
      "He had come to see her as a visitor, so as to be present at her wedding.\n",
      "\n",
      "She had known him too long to be a fool.\n",
      "\n",
      "The day after her first trip to the country the woman had gotten up for breakfast, and he had not had time to get up for breakfast, and in the morning the woman had come for breakfast, and he had not had time to say hello either.\n",
      "\n",
      "At the end of the trip, in the midst of this strange and strange mood, something happened that made her feel happy\n",
      "\n",
      "And at the end of the trip, in the midst of this strange and strange mood, something happened that made her feel happy—and at the end of the trip, in the midst of this strange and strange mood, something happened that made her feel happy, the sun was rising, the woman came at last.\n",
      "\n",
      "In this manner, a new moon had started to rise.\n",
      "\n",
      "This was the day the woman had got up for breakfast.\n",
      "\n",
      "He was in the country—the woman herself had not been in her own house for some days—and that night he had gone for a night with his mother in the country. She had not seen him since…\n",
      "\n",
      "This was the last time his mother had to see her.\n",
      "\n",
      "This was the last time his mother had a chance to ask her questions\n",
      "\n",
      "And this was the last time his mother had a chance to speak to him.\n",
      "\n",
      "He said in his sleep:\n",
      "\n",
      "'I don't see you, and I never will\n",
      "\n",
      "When we go out together, and when we come home together\n",
      "\n",
      "But it takes a certain amount\n",
      "\n",
      "And some days\n",
      "\n",
      "Not to come at all\n",
      "\n",
      "And in my sleep I dream\n",
      "\n",
      "Of love with you\n",
      "\n",
      "And I dreamed for hours\n",
      "\n",
      "In an unknown dream world\n",
      "\n",
      "And\n",
      "\n",
      "In the dream-world\n",
      "\n",
      "I dream\n",
      "\n",
      "Of love with you\n",
      "\n",
      "And not long after that his mother, the first person he ever saw at supper, he had given his card of death to, and from now on, for a while she always seemed waiting for him at the roadside. And the next morning he would go to the airport, a man of forty, he was a little nervous\n",
      "\n",
      "'I think he might have been sent away from this country as a child,'\n",
      "\n",
      "and, the next day, at the last minute, he would be taken away with her. After two weeks, two years\n",
      "\n",
      "He was brought to this country\n",
      "\n",
      "That she took him away with\n",
      "\n",
      "For a period of time\n",
      "\n",
      "…\n",
      "\n",
      "That she knew he could not come back\n",
      "\n",
      "And, the day, the next day, the next day\n",
      "\n",
      "A day of many happy days\n",
      "\n",
      "And many a pleasant dream\n",
      "\n",
      "\n",
      "But at last they decided, after all\n",
      "\n",
      "\n",
      "He had come he would be returned\n",
      "\n",
      "And his mother was not so sad\n",
      "\n",
      "As when she had first met him\n",
      "\n",
      "\n",
      "\n",
      "[201 | 151.91] loss=2.53 avg=3.49\n",
      "[202 | 152.42] loss=3.20 avg=3.48\n",
      "[203 | 152.92] loss=3.96 avg=3.49\n",
      "[204 | 153.43] loss=3.73 avg=3.49\n",
      "[205 | 153.94] loss=3.80 avg=3.49\n",
      "[206 | 154.44] loss=3.06 avg=3.49\n",
      "[207 | 154.95] loss=4.18 avg=3.50\n",
      "[208 | 155.46] loss=3.67 avg=3.50\n",
      "[209 | 155.96] loss=4.09 avg=3.51\n",
      "[210 | 156.47] loss=4.14 avg=3.51\n",
      "[211 | 156.98] loss=3.73 avg=3.52\n",
      "[212 | 157.48] loss=3.67 avg=3.52\n",
      "[213 | 157.99] loss=3.53 avg=3.52\n",
      "[214 | 158.50] loss=2.62 avg=3.51\n",
      "[215 | 159.00] loss=3.34 avg=3.51\n",
      "[216 | 159.51] loss=3.52 avg=3.51\n",
      "[217 | 160.02] loss=3.03 avg=3.50\n",
      "[218 | 160.53] loss=3.11 avg=3.50\n",
      "[219 | 161.04] loss=3.74 avg=3.50\n",
      "[220 | 161.55] loss=2.92 avg=3.49\n",
      "[221 | 162.06] loss=2.45 avg=3.48\n",
      "[222 | 162.57] loss=3.27 avg=3.48\n",
      "[223 | 163.07] loss=3.51 avg=3.48\n",
      "[224 | 163.58] loss=3.61 avg=3.48\n",
      "[225 | 164.09] loss=3.31 avg=3.48\n",
      "[226 | 164.60] loss=3.37 avg=3.48\n",
      "[227 | 165.10] loss=3.52 avg=3.48\n",
      "[228 | 165.61] loss=4.18 avg=3.49\n",
      "[229 | 166.12] loss=4.19 avg=3.49\n",
      "[230 | 166.63] loss=3.11 avg=3.49\n",
      "[231 | 167.14] loss=3.44 avg=3.49\n",
      "[232 | 167.64] loss=3.63 avg=3.49\n",
      "[233 | 168.15] loss=3.42 avg=3.49\n",
      "[234 | 168.66] loss=3.93 avg=3.49\n",
      "[235 | 169.16] loss=3.48 avg=3.49\n",
      "[236 | 169.67] loss=3.31 avg=3.49\n",
      "[237 | 170.18] loss=3.78 avg=3.49\n",
      "[238 | 170.69] loss=3.64 avg=3.50\n",
      "[239 | 171.19] loss=3.21 avg=3.49\n",
      "[240 | 171.70] loss=3.52 avg=3.49\n",
      "[241 | 172.21] loss=3.89 avg=3.50\n",
      "[242 | 172.71] loss=3.96 avg=3.50\n",
      "[243 | 173.22] loss=3.67 avg=3.50\n",
      "[244 | 173.73] loss=3.32 avg=3.50\n",
      "[245 | 174.24] loss=3.12 avg=3.50\n",
      "[246 | 174.74] loss=2.15 avg=3.48\n",
      "[247 | 175.25] loss=3.62 avg=3.49\n",
      "[248 | 175.76] loss=2.58 avg=3.48\n",
      "[249 | 176.27] loss=3.62 avg=3.48\n",
      "[250 | 176.77] loss=3.12 avg=3.47\n",
      "[251 | 177.28] loss=2.88 avg=3.47\n",
      "[252 | 177.79] loss=2.96 avg=3.46\n",
      "[253 | 178.29] loss=3.09 avg=3.46\n",
      "[254 | 178.80] loss=3.64 avg=3.46\n",
      "[255 | 179.31] loss=3.20 avg=3.46\n",
      "[256 | 179.81] loss=3.33 avg=3.46\n",
      "[257 | 180.32] loss=3.45 avg=3.46\n",
      "[258 | 180.82] loss=3.74 avg=3.46\n",
      "[259 | 181.33] loss=3.20 avg=3.46\n",
      "[260 | 181.84] loss=3.74 avg=3.46\n",
      "[261 | 182.34] loss=3.10 avg=3.45\n",
      "[262 | 182.85] loss=3.57 avg=3.46\n",
      "[263 | 183.35] loss=3.56 avg=3.46\n",
      "[264 | 183.86] loss=3.43 avg=3.46\n",
      "[265 | 184.36] loss=2.87 avg=3.45\n",
      "[266 | 184.87] loss=3.35 avg=3.45\n",
      "[267 | 185.37] loss=3.40 avg=3.45\n",
      "[268 | 185.88] loss=3.15 avg=3.45\n",
      "[269 | 186.39] loss=4.30 avg=3.45\n",
      "[270 | 186.89] loss=4.19 avg=3.46\n",
      "[271 | 187.40] loss=4.44 avg=3.47\n",
      "[272 | 187.90] loss=3.13 avg=3.47\n",
      "[273 | 188.41] loss=3.35 avg=3.47\n",
      "[274 | 188.91] loss=3.69 avg=3.47\n",
      "[275 | 189.42] loss=2.29 avg=3.46\n",
      "[276 | 189.93] loss=3.89 avg=3.46\n",
      "[277 | 190.43] loss=3.16 avg=3.46\n",
      "[278 | 190.94] loss=4.40 avg=3.47\n",
      "[279 | 191.44] loss=3.60 avg=3.47\n",
      "[280 | 191.95] loss=3.52 avg=3.47\n",
      "[281 | 192.46] loss=2.91 avg=3.47\n",
      "[282 | 192.97] loss=3.00 avg=3.46\n",
      "[283 | 193.47] loss=2.93 avg=3.45\n",
      "[284 | 193.98] loss=2.90 avg=3.45\n",
      "[285 | 194.48] loss=4.02 avg=3.45\n",
      "[286 | 194.99] loss=3.77 avg=3.46\n",
      "[287 | 195.50] loss=3.96 avg=3.46\n",
      "[288 | 196.01] loss=2.42 avg=3.45\n",
      "[289 | 196.51] loss=3.31 avg=3.45\n",
      "[290 | 197.02] loss=3.83 avg=3.45\n",
      "[291 | 197.52] loss=3.54 avg=3.46\n",
      "[292 | 198.03] loss=3.73 avg=3.46\n",
      "[293 | 198.54] loss=3.92 avg=3.46\n",
      "[294 | 199.04] loss=3.98 avg=3.47\n",
      "[295 | 199.55] loss=3.27 avg=3.47\n",
      "[296 | 200.06] loss=4.16 avg=3.47\n",
      "[297 | 200.57] loss=3.03 avg=3.47\n",
      "[298 | 201.07] loss=3.35 avg=3.47\n",
      "[299 | 201.58] loss=4.32 avg=3.48\n",
      "[300 | 202.09] loss=4.38 avg=3.49\n",
      "======== SAMPLE 1 ========\n",
      ", of course, that I would be completely at my ease if he showed some signs of anxiety or discomfort—and that he would certainly want to go. In the meantime I would like to show him some pictures of the house and my garden.\n",
      "\n",
      "In the afternoon I will come to pick up my sister, a relative of mine, and will be delighted to know if he would be willing to come to the park to talk with you guys.\n",
      "\n",
      "Thanks very much for all the lovely emails, I am very busy in these kind of matters but we will do our best to help you find him.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Very respectfully yours,\n",
      "\n",
      "(Mrs. Margaret Sanger)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TO: ELIZABETH BISHIO, KATHY GRACE, KAREN P. SZOLSKI\n",
      "\n",
      "TL(XI)1(V)1(J): To the Editor\n",
      "\n",
      "Date: Tuesday 19 November 1971\n",
      "\n",
      "Re: Les délétrons des écuds à la rue d'Afrique [SINISTAISSA, 1 a. v. pp. 23–5, p. 28–7, 32–6]\n",
      "\n",
      "Dear Ms. Grace (I received a very encouraging letter),\n",
      "\n",
      "I have just received the copy of your Letter of October 11, 1971 from the Editor of the New York Times. I am looking forward to hearing from you if it becomes necessary to put me on the right track of the story (which I hope is very much preferable to delay and prolong the story).\n",
      "\n",
      "The reader is reminded that you have in recent years become the leading voice of the opposition to Stalinism; that you publicly called on him to be executed, and your famous \"dubbing\" of the name of Stalin on his face; and that we can see your face in the picture of the meeting between Stalin and the Pravda editorialists (a \"narrow-eyed\" picture which you have given to a great many people) in Berlin, where you met Mr. Zhirinovsky. I have also been in touch with him.\n",
      "\n",
      "As I write to you I am a little dizzy but am beginning to write a little better: I have started to write well and have found a rhythm for my poems and I am also growing accustomed to the \"drum\" which I used to play with the boys in our little village. I have been very happy at the suggestion of your lovely mother to take you to the P.N. (Pravda) interview you gave the other day. As you know, I am fond of the \"dunce of the tongue\" and am very glad that he is so sure of me and understands me better than I could have expected. If he thinks I might give in—because perhaps he ought to be so sure!—he can see that it would be better to call me at the office immediately, and in that case to say that I don't know when he will get a chance to write anything.\n",
      "\n",
      "On your way to the interview I might suggest, if you want, to call me immediately back for one thing: he would surely like to be given some information by his lawyer of what is necessary to stop me taking the case to Court, since (as you know) I will not get anything in return from the law that you can possibly understand, much less be pleased with. I am sure it would be most appreciated.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I am very glad that you have given me such an inspiring letter. I cannot thank you enough. I thank you for the wonderful gift with the nice words you used.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It is very sad I cannot do something for this mother. If she feels she cannot help herself and would not take responsibility (for not sending her message to me first), then you don't worry: I will do it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yours ever,\n",
      "\n",
      "Vladimir Nabokov\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TO: ELIZABETH BISHIO, KATHY GRACE, KAREN P. SZOLSKI\n",
      "\n",
      "TL(XI)1(V)1(J): To the Editor\n",
      "\n",
      "1. Dear Ms. Grace,\n",
      "\n",
      "It is sad to see you leave your desk job. I have been very sad too about all that. It might be thought all that you do is do away with the need for a lawyer is that you call a lawyer when somebody is worried, not for the sake of a lawyer's reputation for honesty, which there is no such thing, nor for your own reasons: you should have known that it would be much better if his name was not on your file and the lawyer was not paid; but I should not think I could have prevented you from doing it, either with the lawyer or by being of sufficient dignity to not do it.\n",
      "\n",
      "So I am sorry to write so angrily. It was one of my earliest recollections:\n",
      "\n",
      "[301 | 219.53] loss=3.15 avg=3.48\n",
      "[302 | 220.04] loss=3.79 avg=3.49\n",
      "[303 | 220.54] loss=3.63 avg=3.49\n",
      "[304 | 221.05] loss=3.83 avg=3.49\n",
      "[305 | 221.56] loss=3.60 avg=3.49\n",
      "[306 | 222.06] loss=3.55 avg=3.49\n",
      "[307 | 222.57] loss=3.91 avg=3.50\n",
      "[308 | 223.08] loss=4.33 avg=3.51\n",
      "[309 | 223.59] loss=3.70 avg=3.51\n",
      "[310 | 224.09] loss=4.31 avg=3.52\n",
      "[311 | 224.60] loss=3.08 avg=3.51\n",
      "[312 | 225.10] loss=3.17 avg=3.51\n",
      "[313 | 225.61] loss=3.60 avg=3.51\n",
      "[314 | 226.12] loss=3.81 avg=3.51\n",
      "[315 | 226.63] loss=3.85 avg=3.52\n",
      "[316 | 227.14] loss=3.12 avg=3.51\n",
      "[317 | 227.64] loss=2.76 avg=3.50\n",
      "[318 | 228.15] loss=2.96 avg=3.50\n",
      "[319 | 228.66] loss=3.42 avg=3.50\n",
      "[320 | 229.17] loss=3.85 avg=3.50\n",
      "[321 | 229.67] loss=3.74 avg=3.50\n",
      "[322 | 230.18] loss=2.97 avg=3.50\n",
      "[323 | 230.69] loss=2.84 avg=3.49\n",
      "[324 | 231.20] loss=3.44 avg=3.49\n",
      "[325 | 231.71] loss=3.47 avg=3.49\n",
      "[326 | 232.21] loss=3.32 avg=3.49\n",
      "[327 | 232.72] loss=3.39 avg=3.49\n",
      "[328 | 233.23] loss=3.31 avg=3.49\n",
      "[329 | 233.74] loss=3.08 avg=3.48\n",
      "[330 | 234.24] loss=3.65 avg=3.48\n",
      "[331 | 234.75] loss=4.12 avg=3.49\n",
      "[332 | 235.26] loss=3.28 avg=3.49\n",
      "[333 | 235.77] loss=3.87 avg=3.49\n",
      "[334 | 236.27] loss=3.83 avg=3.50\n",
      "[335 | 236.78] loss=3.87 avg=3.50\n",
      "[336 | 237.29] loss=3.37 avg=3.50\n",
      "[337 | 237.80] loss=3.60 avg=3.50\n",
      "[338 | 238.30] loss=3.65 avg=3.50\n",
      "[339 | 238.81] loss=3.74 avg=3.50\n",
      "[340 | 239.32] loss=3.54 avg=3.50\n",
      "[341 | 239.82] loss=2.77 avg=3.50\n",
      "[342 | 240.33] loss=3.89 avg=3.50\n",
      "[343 | 240.84] loss=3.83 avg=3.50\n",
      "[344 | 241.35] loss=3.77 avg=3.51\n",
      "[345 | 241.86] loss=3.66 avg=3.51\n",
      "[346 | 242.36] loss=2.91 avg=3.50\n",
      "[347 | 242.87] loss=3.35 avg=3.50\n",
      "[348 | 243.38] loss=3.51 avg=3.50\n",
      "[349 | 243.89] loss=3.82 avg=3.50\n",
      "[350 | 244.39] loss=4.35 avg=3.51\n",
      "[351 | 244.90] loss=3.53 avg=3.51\n",
      "[352 | 245.41] loss=3.87 avg=3.52\n",
      "[353 | 245.92] loss=3.62 avg=3.52\n",
      "[354 | 246.42] loss=3.59 avg=3.52\n",
      "[355 | 246.93] loss=4.27 avg=3.53\n",
      "[356 | 247.44] loss=3.06 avg=3.52\n",
      "[357 | 247.94] loss=3.40 avg=3.52\n",
      "[358 | 248.45] loss=3.66 avg=3.52\n",
      "[359 | 248.95] loss=3.17 avg=3.52\n",
      "[360 | 249.46] loss=3.06 avg=3.51\n",
      "[361 | 249.97] loss=2.88 avg=3.51\n",
      "[362 | 250.47] loss=3.16 avg=3.50\n",
      "[363 | 250.98] loss=2.98 avg=3.50\n",
      "[364 | 251.48] loss=3.67 avg=3.50\n",
      "[365 | 251.99] loss=3.35 avg=3.50\n",
      "[366 | 252.50] loss=2.83 avg=3.49\n",
      "[367 | 253.00] loss=3.55 avg=3.49\n",
      "[368 | 253.51] loss=4.04 avg=3.50\n",
      "[369 | 254.02] loss=3.27 avg=3.49\n",
      "[370 | 254.53] loss=3.75 avg=3.50\n",
      "[371 | 255.03] loss=3.97 avg=3.50\n",
      "[372 | 255.54] loss=3.55 avg=3.50\n",
      "[373 | 256.05] loss=2.89 avg=3.50\n",
      "[374 | 256.55] loss=2.64 avg=3.49\n",
      "[375 | 257.06] loss=2.56 avg=3.48\n",
      "[376 | 257.57] loss=3.13 avg=3.47\n",
      "[377 | 258.08] loss=3.20 avg=3.47\n",
      "[378 | 258.58] loss=3.73 avg=3.47\n",
      "[379 | 259.09] loss=2.88 avg=3.47\n",
      "[380 | 259.60] loss=3.96 avg=3.47\n",
      "[381 | 260.10] loss=3.82 avg=3.48\n",
      "[382 | 260.61] loss=4.40 avg=3.49\n",
      "[383 | 261.12] loss=3.36 avg=3.48\n",
      "[384 | 261.63] loss=3.91 avg=3.49\n",
      "[385 | 262.13] loss=3.43 avg=3.49\n",
      "[386 | 262.64] loss=3.57 avg=3.49\n",
      "[387 | 263.15] loss=3.24 avg=3.49\n",
      "[388 | 263.65] loss=2.47 avg=3.48\n",
      "[389 | 264.16] loss=3.74 avg=3.48\n",
      "[390 | 264.67] loss=3.08 avg=3.47\n",
      "[391 | 265.17] loss=3.12 avg=3.47\n",
      "[392 | 265.68] loss=3.33 avg=3.47\n",
      "[393 | 266.19] loss=2.70 avg=3.46\n",
      "[394 | 266.69] loss=3.51 avg=3.46\n",
      "[395 | 267.20] loss=3.28 avg=3.46\n",
      "[396 | 267.71] loss=3.53 avg=3.46\n",
      "[397 | 268.22] loss=3.67 avg=3.46\n",
      "[398 | 268.72] loss=4.64 avg=3.48\n",
      "[399 | 269.23] loss=3.74 avg=3.48\n",
      "[400 | 269.74] loss=3.11 avg=3.47\n",
      "======== SAMPLE 1 ========\n",
      " keep the knife pointed at me. He was not that type anyway, and I liked that way of eating. There were some very peculiar things about the chicken. He put it back. The other one was really quite good. It was made to order. A bit too sweet, with a touch of sweetness (a sort of lemonade) that lingered on the tongue. Not exactly good for the head.\n",
      "\n",
      "My boy had also enjoyed a very special meal, and by the time he had had it he had changed the subject quite a little, but when I explained to the waiter and went up one way in and left in the other that he was a friend of mine, he replied that he was a great lover and that the only reason why he had to give me this plate was \"because he's my only friend in the universe.\" I thanked him and left my little boy with me.\n",
      "\n",
      "As soon as the waiter entered, the first thing he asked was where was my coat button. It stuck in his throat but he said that he would show him. He was a young man of about thirty and a good-looking fellow. My young boy (now a little small-shouldered, still a kid) did not know a thing about the subject. The only thing he could do was point it out. \"Well,\" he muttered with a grin.\n",
      "\n",
      "That was not all, however. Another waiter appeared; very pale, old-fashionedly and with a little air of earnestness. He was also a very respectable and respectable old man, a handsome fellow who was also my best man. He was in the way when the boy turned back, but at the same moment the waiter gave him in, pointed a pair of legs out by his ear, passed the boy to one of the clerks, and showed them the handbook. I went on reading while the boy looked in the other direction to the right; and the boy looked at him in turn—but after a while the old man pulled out the book.\n",
      "\n",
      "\"This is a little late,\" said the boy. \"I was getting ready but it might go out too late.\" \"This is my book.\" There was an exchange of words: the boy pointed out he was the one who bore the mark, and the boy nodded in agreement.\n",
      "\n",
      "\"Well how is that?\"\n",
      "\n",
      "\"Very good.\" The book, with the mark, slipped off the hand-kerf (the small man held it out to one of the clerks, who did not catch it, but was about to throw it in the bin). \"This book has many faults. But it's a very pretty book.\" \"You're a little on the edge for sure.\"\n",
      "\n",
      "\"I'm on the edge as well. But it does belong to a family that has been reading for ages. It's not particularly good because the pages are a bit flabby and the handwriting of the author is not exactly accurate. You have to remember—you're on the edge. I'm not. But a very good book. What can I say about it? It was a delight to me, although it does seem a little bit much.\"\n",
      "\n",
      "The boy looked at the handbag. \"Can you give me a little to drink?\"\n",
      "\n",
      "\"Of course—of course.\" He was served. \"I'm really sorry.\"\n",
      "\n",
      "\"No need because it's nothing for you\"— I laughed faintly while taking the last of the glass. \"You were right. You didn't make an ass of yourself.\"\n",
      "\n",
      "\"I wasn't trying to give him anything. You saw I was on the edge, though, and that's what happened. You got the book out of there with all the effort of an artless kid and didn't do it.\"\n",
      "\n",
      "\"Pardon me a moment, my young boy. But you must tell me which is the best in the collection. The first is called 'Trolls in the Sky,' the second called 'I'm Going on a Voyage'?\"\n",
      "\n",
      "\"My pleasure, yes.\"\n",
      "\n",
      "\"And the former? 'The Golden Road?' No, 'The Golden Road, a Traveller's Paradise'—how do you spell that in that language—\"\n",
      "\n",
      "\"Traveller's Paradise. A Traveller's Paradise Paradise.\"\n",
      "\n",
      "\"Yes. And that last thing? 'The City, by the City'—\"\n",
      "\n",
      "\"Yes. And—'I Love The City'—No, I don't, because, in real life, not all that 'The City' does, actually, means the 'The City' of the people who live in the Country. Not the People of The City. A very modern kind of 'City. The city— 'The city, and not the city— the city' in English.' But let me see—how does it sound to you, the boy— the city— 'The city, but not my city'? How would you pronounce that, the boy? I imagine\n",
      "\n",
      "[401 | 287.17] loss=4.27 avg=3.48\n",
      "[402 | 287.68] loss=3.34 avg=3.48\n",
      "[403 | 288.19] loss=3.50 avg=3.48\n",
      "[404 | 288.69] loss=3.43 avg=3.48\n",
      "[405 | 289.20] loss=3.98 avg=3.49\n",
      "[406 | 289.71] loss=3.55 avg=3.49\n",
      "[407 | 290.22] loss=2.61 avg=3.48\n",
      "[408 | 290.72] loss=3.17 avg=3.47\n",
      "[409 | 291.23] loss=3.08 avg=3.47\n",
      "[410 | 291.74] loss=2.97 avg=3.47\n",
      "[411 | 292.25] loss=3.63 avg=3.47\n",
      "[412 | 292.75] loss=3.29 avg=3.47\n",
      "[413 | 293.26] loss=3.18 avg=3.46\n",
      "[414 | 293.77] loss=2.61 avg=3.45\n",
      "[415 | 294.28] loss=3.90 avg=3.46\n",
      "[416 | 294.78] loss=3.35 avg=3.46\n",
      "[417 | 295.29] loss=3.56 avg=3.46\n",
      "[418 | 295.80] loss=3.88 avg=3.46\n",
      "[419 | 296.31] loss=3.07 avg=3.46\n",
      "[420 | 296.81] loss=3.01 avg=3.45\n",
      "[421 | 297.32] loss=3.02 avg=3.45\n",
      "[422 | 297.83] loss=3.44 avg=3.45\n",
      "[423 | 298.34] loss=3.96 avg=3.45\n",
      "[424 | 298.85] loss=3.13 avg=3.45\n",
      "[425 | 299.36] loss=2.80 avg=3.44\n",
      "[426 | 299.86] loss=3.67 avg=3.45\n",
      "[427 | 300.37] loss=3.07 avg=3.44\n",
      "[428 | 300.88] loss=3.34 avg=3.44\n",
      "[429 | 301.38] loss=3.74 avg=3.44\n",
      "[430 | 301.89] loss=3.30 avg=3.44\n",
      "[431 | 302.40] loss=3.66 avg=3.45\n",
      "[432 | 302.91] loss=2.85 avg=3.44\n",
      "[433 | 303.41] loss=3.49 avg=3.44\n",
      "[434 | 303.92] loss=3.53 avg=3.44\n",
      "[435 | 304.43] loss=3.07 avg=3.44\n",
      "[436 | 304.94] loss=3.62 avg=3.44\n",
      "[437 | 305.44] loss=3.58 avg=3.44\n",
      "[438 | 305.95] loss=3.36 avg=3.44\n",
      "[439 | 306.46] loss=2.87 avg=3.43\n",
      "[440 | 306.96] loss=3.91 avg=3.44\n",
      "[441 | 307.47] loss=3.54 avg=3.44\n",
      "[442 | 307.98] loss=3.05 avg=3.44\n",
      "[443 | 308.49] loss=3.34 avg=3.43\n",
      "[444 | 308.99] loss=4.04 avg=3.44\n",
      "[445 | 309.50] loss=3.04 avg=3.44\n",
      "[446 | 310.01] loss=2.79 avg=3.43\n",
      "[447 | 310.52] loss=3.49 avg=3.43\n",
      "[448 | 311.02] loss=3.42 avg=3.43\n",
      "[449 | 311.53] loss=3.68 avg=3.43\n",
      "[450 | 312.04] loss=2.89 avg=3.43\n",
      "[451 | 312.54] loss=3.43 avg=3.43\n",
      "[452 | 313.05] loss=3.10 avg=3.42\n",
      "[453 | 313.55] loss=3.24 avg=3.42\n",
      "[454 | 314.06] loss=3.36 avg=3.42\n",
      "[455 | 314.57] loss=3.35 avg=3.42\n",
      "[456 | 315.07] loss=3.60 avg=3.42\n",
      "[457 | 315.58] loss=3.52 avg=3.42\n",
      "[458 | 316.09] loss=3.51 avg=3.42\n",
      "[459 | 316.59] loss=3.40 avg=3.42\n",
      "[460 | 317.10] loss=3.48 avg=3.43\n",
      "[461 | 317.61] loss=3.77 avg=3.43\n",
      "[462 | 318.11] loss=3.70 avg=3.43\n",
      "[463 | 318.62] loss=3.73 avg=3.43\n",
      "[464 | 319.13] loss=3.01 avg=3.43\n",
      "[465 | 319.64] loss=3.29 avg=3.43\n",
      "[466 | 320.14] loss=3.61 avg=3.43\n",
      "[467 | 320.65] loss=3.19 avg=3.43\n",
      "[468 | 321.16] loss=3.52 avg=3.43\n",
      "[469 | 321.67] loss=3.61 avg=3.43\n",
      "[470 | 322.17] loss=3.53 avg=3.43\n",
      "[471 | 322.68] loss=3.29 avg=3.43\n",
      "[472 | 323.18] loss=3.76 avg=3.43\n",
      "[473 | 323.69] loss=3.82 avg=3.44\n",
      "[474 | 324.20] loss=3.70 avg=3.44\n",
      "[475 | 324.71] loss=4.07 avg=3.45\n",
      "[476 | 325.21] loss=3.68 avg=3.45\n",
      "[477 | 325.72] loss=3.01 avg=3.44\n",
      "[478 | 326.23] loss=3.68 avg=3.45\n",
      "[479 | 326.73] loss=3.74 avg=3.45\n",
      "[480 | 327.24] loss=3.55 avg=3.45\n",
      "[481 | 327.75] loss=3.07 avg=3.45\n",
      "[482 | 328.25] loss=2.65 avg=3.44\n",
      "[483 | 328.76] loss=3.93 avg=3.44\n",
      "[484 | 329.27] loss=2.67 avg=3.44\n",
      "[485 | 329.77] loss=3.56 avg=3.44\n",
      "[486 | 330.28] loss=3.53 avg=3.44\n",
      "[487 | 330.79] loss=3.36 avg=3.44\n",
      "[488 | 331.29] loss=2.73 avg=3.43\n",
      "[489 | 331.80] loss=3.44 avg=3.43\n",
      "[490 | 332.31] loss=2.95 avg=3.43\n",
      "[491 | 332.81] loss=4.25 avg=3.43\n",
      "[492 | 333.32] loss=3.81 avg=3.44\n",
      "[493 | 333.83] loss=4.09 avg=3.44\n",
      "[494 | 334.34] loss=3.57 avg=3.45\n",
      "[495 | 334.84] loss=2.93 avg=3.44\n",
      "[496 | 335.35] loss=3.64 avg=3.44\n",
      "[497 | 335.86] loss=3.64 avg=3.44\n",
      "[498 | 336.36] loss=3.07 avg=3.44\n",
      "[499 | 336.87] loss=3.30 avg=3.44\n",
      "[500 | 337.37] loss=3.45 avg=3.44\n",
      "======== SAMPLE 1 ========\n",
      " at no longer exist?\n",
      "\n",
      "The night was dark and still as if nothing could be done.\n",
      "\n",
      "He knew, however, that it had become evening.\n",
      "\n",
      "After a moment in his new hotel.\n",
      "\n",
      "—\n",
      "\n",
      "But the darkness had nothing at all to do with him, in spite of an inexplicable, and quite natural, desire to see the night, to feel for its dark, shadowy beauty—\n",
      "\n",
      "He looked at the night with the same intensity with which, in the course of a sleepless night, he had looked at anything that had the night with it.\n",
      "\n",
      "He had been waiting for an hour in the dark. It was still dark, and yet it seemed to him to have spread through his soul a light, a freshness, a newness. His body was stiff, but he saw the sun, and the night—and a stranger man, and a city, and an empty street, and the sun, shining with his white hair, in his black shoes, in his blue petticoat, in his black shirt. He saw the red and yellow florins, and the little bells, and the bright puffs of smoke, and the red roses that were slowly dying, and the red-red-white-red of the city lights,\n",
      "\n",
      "all was still, and nothing else could possibly exist, except for himself, and nothing other than him. Now there was no other person in the world but himself, the sun, the red lights, the red puffs of smoke, the red roses, and the red florins.\n",
      "\n",
      "And now nothing could exist, except for him. He saw the streets, the little bells, the night—the black streets were the blackest, wettest, and most miserable.\n",
      "\n",
      "The air was still. The sun remained in the sky. In the darkness and pain in his soul one could hear the rustle of the wind as it passed along the streets of the city. It was in every city all around that night, every one, that he was in which he had the nightless, the unquiet, the nightly, the nightly voice. The black puffs of smoke could still be seen on the terrace of the Hotel Le Givon, but now there were only the pale and red faces of a hundred women, so pale they looked like tiny butterflies. A woman beside a table was eating and listening; she was a little pale, with a little grey-blond hair. She sat with her back to a wall, and at once her back turned to him, and his face twitched at her hand. She took his hand, as if she were going to play hide-and-seek; but he stopped and sat still for a moment, trembling still from the cold, trembling from the darkness. Then she leaned over and began to whisper, and he began to hear her murmural voice as if some great secret had been opened, but she was not sure he could hear her. In desperation he reached out for her hand, and all around him the darkness grew thick and dense, for a moment he knew that he had made contact, that she was there in front of him—and only then, all at once, that he heard her whisper again, her voice muffled, he saw her, she there—and he was not sure whether he would remain in her place, in the darkness, in the dark, in the dark, in the dark.\n",
      "\n",
      "This is what she must have said:\n",
      "\n",
      "“I love, oh, oh, I love that sun, and a little flower in the street’s garden, and you here in the park, and the way the leaves flew and fluttered—”\n",
      "\n",
      "She had spoken in her sleep.\n",
      "\n",
      "He began to understand now. He felt the burning sun in his heart, the joy that had for a time passed into something else, like some invisible wind in the air. He felt a great light that had left his soul, that had moved from one place to another in his mind—and he was still not sure what had come over him, what was he to take home? He could no longer talk. He had suddenly grown sleepy, and had started to feel faint. But she had already disappeared, and the light remained.\n",
      "\n",
      "Now all he could hear—and could not resist, or at least did not resist, her voice or the sweet fragrance that drifted from its mouth—then the voices of his mother, his son, the two maids sitting around the dining room table—he could not help touching the back of one hand with the other, holding the back of his face, looking at her with a sudden warmth that left him as if he had taken a huge step through a door.\n",
      "\n",
      "Then he saw his mother and that woman, and, slowly, he heard what he did not know how to define any more. He heard his mother coming, and the lights dimmed, and the\n",
      "\n",
      "[501 | 354.90] loss=3.34 avg=3.44\n",
      "[502 | 355.41] loss=3.27 avg=3.44\n",
      "[503 | 355.92] loss=2.87 avg=3.43\n",
      "[504 | 356.42] loss=3.11 avg=3.43\n",
      "[505 | 356.93] loss=3.59 avg=3.43\n",
      "[506 | 357.44] loss=4.08 avg=3.44\n",
      "[507 | 357.94] loss=3.33 avg=3.43\n",
      "[508 | 358.45] loss=3.63 avg=3.44\n",
      "[509 | 358.96] loss=2.63 avg=3.43\n",
      "[510 | 359.47] loss=3.68 avg=3.43\n",
      "[511 | 359.97] loss=3.34 avg=3.43\n",
      "[512 | 360.48] loss=2.89 avg=3.42\n",
      "[513 | 360.99] loss=3.13 avg=3.42\n",
      "[514 | 361.50] loss=3.04 avg=3.42\n",
      "[515 | 362.00] loss=3.03 avg=3.41\n",
      "[516 | 362.51] loss=2.81 avg=3.41\n",
      "[517 | 363.02] loss=3.82 avg=3.41\n",
      "[518 | 363.53] loss=3.57 avg=3.41\n",
      "[519 | 364.04] loss=4.29 avg=3.42\n",
      "[520 | 364.55] loss=3.25 avg=3.42\n",
      "[521 | 365.05] loss=4.17 avg=3.43\n",
      "[522 | 365.56] loss=2.81 avg=3.42\n",
      "[523 | 366.07] loss=3.52 avg=3.42\n",
      "[524 | 366.58] loss=3.81 avg=3.43\n",
      "[525 | 367.09] loss=3.10 avg=3.42\n",
      "[526 | 367.59] loss=4.22 avg=3.43\n",
      "[527 | 368.10] loss=3.77 avg=3.44\n",
      "[528 | 368.61] loss=3.40 avg=3.43\n",
      "[529 | 369.12] loss=3.79 avg=3.44\n",
      "[530 | 369.62] loss=3.74 avg=3.44\n",
      "[531 | 370.13] loss=3.10 avg=3.44\n",
      "[532 | 370.64] loss=3.95 avg=3.44\n",
      "[533 | 371.15] loss=3.03 avg=3.44\n",
      "[534 | 371.66] loss=3.04 avg=3.43\n",
      "[535 | 372.16] loss=3.59 avg=3.44\n",
      "[536 | 372.67] loss=3.06 avg=3.43\n",
      "[537 | 373.18] loss=3.82 avg=3.44\n",
      "[538 | 373.68] loss=2.66 avg=3.43\n",
      "[539 | 374.19] loss=3.96 avg=3.43\n",
      "[540 | 374.70] loss=4.22 avg=3.44\n",
      "[541 | 375.21] loss=3.81 avg=3.45\n",
      "[542 | 375.71] loss=2.65 avg=3.44\n",
      "[543 | 376.22] loss=3.74 avg=3.44\n",
      "[544 | 376.72] loss=2.65 avg=3.43\n",
      "[545 | 377.23] loss=3.25 avg=3.43\n",
      "[546 | 377.74] loss=3.83 avg=3.44\n",
      "[547 | 378.24] loss=2.93 avg=3.43\n",
      "[548 | 378.75] loss=2.70 avg=3.42\n",
      "[549 | 379.26] loss=3.33 avg=3.42\n",
      "[550 | 379.76] loss=3.58 avg=3.42\n",
      "[551 | 380.27] loss=3.23 avg=3.42\n",
      "[552 | 380.78] loss=2.14 avg=3.41\n",
      "[553 | 381.29] loss=4.16 avg=3.42\n",
      "[554 | 381.79] loss=3.35 avg=3.42\n",
      "[555 | 382.30] loss=2.86 avg=3.41\n",
      "[556 | 382.81] loss=3.68 avg=3.41\n",
      "[557 | 383.31] loss=2.86 avg=3.41\n",
      "[558 | 383.82] loss=4.04 avg=3.41\n",
      "[559 | 384.33] loss=3.00 avg=3.41\n",
      "[560 | 384.83] loss=4.09 avg=3.42\n",
      "[561 | 385.34] loss=3.54 avg=3.42\n",
      "[562 | 385.85] loss=2.60 avg=3.41\n",
      "[563 | 386.35] loss=3.40 avg=3.41\n",
      "[564 | 386.86] loss=2.62 avg=3.40\n",
      "[565 | 387.37] loss=3.00 avg=3.40\n",
      "[566 | 387.87] loss=3.49 avg=3.40\n",
      "[567 | 388.38] loss=3.59 avg=3.40\n",
      "[568 | 388.89] loss=2.54 avg=3.39\n",
      "[569 | 389.39] loss=3.35 avg=3.39\n",
      "[570 | 389.90] loss=3.70 avg=3.39\n",
      "[571 | 390.41] loss=3.62 avg=3.40\n",
      "[572 | 390.92] loss=3.19 avg=3.39\n",
      "[573 | 391.42] loss=2.47 avg=3.39\n",
      "[574 | 391.93] loss=2.89 avg=3.38\n",
      "[575 | 392.44] loss=3.68 avg=3.38\n",
      "[576 | 392.94] loss=2.83 avg=3.38\n",
      "[577 | 393.45] loss=3.32 avg=3.38\n",
      "[578 | 393.96] loss=3.75 avg=3.38\n",
      "[579 | 394.46] loss=3.05 avg=3.38\n",
      "[580 | 394.97] loss=3.16 avg=3.38\n",
      "[581 | 395.47] loss=2.89 avg=3.37\n",
      "[582 | 395.98] loss=2.38 avg=3.36\n",
      "[583 | 396.49] loss=3.55 avg=3.36\n",
      "[584 | 396.99] loss=3.97 avg=3.37\n",
      "[585 | 397.50] loss=3.70 avg=3.37\n",
      "[586 | 398.01] loss=2.71 avg=3.36\n",
      "[587 | 398.52] loss=3.58 avg=3.37\n",
      "[588 | 399.03] loss=3.42 avg=3.37\n",
      "[589 | 399.53] loss=3.40 avg=3.37\n",
      "[590 | 400.04] loss=2.56 avg=3.36\n",
      "[591 | 400.55] loss=3.62 avg=3.36\n",
      "[592 | 401.05] loss=3.42 avg=3.36\n",
      "[593 | 401.56] loss=3.37 avg=3.36\n",
      "[594 | 402.07] loss=3.04 avg=3.36\n",
      "[595 | 402.58] loss=3.85 avg=3.36\n",
      "[596 | 403.08] loss=3.03 avg=3.36\n",
      "[597 | 403.59] loss=3.25 avg=3.36\n",
      "[598 | 404.10] loss=3.40 avg=3.36\n",
      "[599 | 404.61] loss=3.00 avg=3.36\n",
      "[600 | 405.11] loss=3.10 avg=3.35\n",
      "======== SAMPLE 1 ========\n",
      "how? We want your response to:\n",
      "\n",
      "\"Do you realize how many times you would have to repeat in order to get your readers to care about your book?\"\n",
      "\n",
      "To which I might respond with a kind of: I have not read anything by that author.\n",
      "\n",
      "For a while I may have been obliged to return the manuscript to you but I now find myself completely satisfied with the outcome. As I write this I have had time to reflect on my own response to this letter and to the following questions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. Could you do another review at a later date? You wrote to us two years ago.\n",
      "\n",
      "2. Does the title change seem quite obvious (to you at least)?\n",
      "\n",
      "3. Would you be interested in adding some additional stuff to my review of the book?\n",
      "\n",
      "4. To summarise briefly:\n",
      "\n",
      "\n",
      "\n",
      "1. \"Forbidden\" has a certain poetic quality which is not possible with any novelist.\n",
      "\n",
      "2. I feel that the title is very clever.\n",
      "\n",
      "\n",
      "\n",
      "You were, it seems, not a little impressed with the book I am writing and writing (by the end of this year) to. However, I have an inkling of what I am writing about: the mysterious \"forbidden,\" to my mind. Perhaps you will notice in the next years of our love affair (which I begin to consider more and more as a sort of \"vampire\" that the book which you wrote is an \"artificial creation\" which is not mine!).\n",
      "\n",
      "I am now approaching two-three years of writing the kind of work which seems to have been so frequently and satisfactorily delivered to me from you in the past: a thriller (with such a dull and pointless title as \"Forbidden\"?) and a novel about a young person, \"Lizzie,\" (yes, a novel, by the way, for the first four pages, to be sure.) and a book which might well be one of the latter, namely: \"Lizzie\" by Luzhin Yalta, \"The Other Side of Love,\" a memoir of the period from 1926 to 1930.\n",
      "\n",
      "When I began to write it, I hoped that it would be a serious thriller, which would at least tell a serious story, and perhaps also a historical novel, whose purpose would be to explain (in the most general way possible) what the \"secret\" of a certain mysterious \"lady\" did not seem to her, in the first place. I am convinced that I have come to the conclusion that even now some of the book's most characteristic characteristics stand for the following lines:\n",
      "\n",
      "\n",
      "the book as a novel contains many mysteries\n",
      "\n",
      "\n",
      "isn't as much a mystery (to a certain extent), but a story told in a certain voice in a certain story.\n",
      "\n",
      "The author is either not a novelist but a poet who, with good taste, keeps at the very least \"obscures\" the truth of what he writes, or, on the contrary, his writing, by which lies a hidden \"taste.\" (And who will ever read a thriller, written by an author who writes under the pretence of a story, with that pretense, without \"opening the book?\")\n",
      "\n",
      "\n",
      "\n",
      "(The writer could write anything! A work written by an \"author\" of his choosing, and with the \"obscures\" being those that have been revealed, is still an author's work; to be sure it will turn out to have some \"taste\"—even if it is never going to be read by a modern reader!)\n",
      "\n",
      "\n",
      "\n",
      "The book may not concern itself with the \"secret\" that \"Lizzie\" was being kept in—but on the contrary, I am aware of, and I am delighted that someone will think, on the very first sentence, that the book is actually about something or other, and not about her—and, indeed, it is very necessary to realize this before jumping to the conclusion that the book has nothing to do with anything at all.\n",
      "\n",
      "The author (\"for his own sake and in his own way\") in the third paragraph of the book seems in some respects to be a \"selfish\" person who tries vainly to be more generous and \"more humane,\" to be more \"giver and kind\" (if the word can apply to him) but at the end of the book he is the very opposite of generous and kind. That author, whose entire life has been spent in this manner (in the first place, as a man, working to pay for an apartment that came to him in order to \"procreate\" in a way that he did not find in his \"inner\" world, then again coming to the conclusion that his life was a mere fairy tale), can have no sympathy for the lonely widower who has had nothing but an imaginary dream and then, through the very act of taking that dream away, found himself in the midst of a society which\n",
      "\n",
      "[601 | 422.54] loss=3.83 avg=3.36\n",
      "[602 | 423.04] loss=3.77 avg=3.36\n",
      "[603 | 423.55] loss=3.08 avg=3.36\n",
      "[604 | 424.06] loss=2.44 avg=3.35\n",
      "[605 | 424.56] loss=3.14 avg=3.35\n",
      "[606 | 425.07] loss=3.60 avg=3.35\n",
      "[607 | 425.58] loss=2.91 avg=3.35\n",
      "[608 | 426.08] loss=2.77 avg=3.34\n",
      "[609 | 426.59] loss=3.94 avg=3.35\n",
      "[610 | 427.09] loss=3.32 avg=3.35\n",
      "[611 | 427.60] loss=3.24 avg=3.35\n",
      "[612 | 428.11] loss=3.81 avg=3.35\n",
      "[613 | 428.62] loss=3.51 avg=3.35\n",
      "[614 | 429.13] loss=3.28 avg=3.35\n",
      "[615 | 429.63] loss=2.84 avg=3.35\n",
      "[616 | 430.14] loss=2.49 avg=3.34\n",
      "[617 | 430.65] loss=3.43 avg=3.34\n",
      "[618 | 431.16] loss=3.20 avg=3.34\n",
      "[619 | 431.66] loss=3.56 avg=3.34\n",
      "[620 | 432.17] loss=4.04 avg=3.35\n",
      "[621 | 432.68] loss=3.78 avg=3.35\n",
      "[622 | 433.19] loss=3.68 avg=3.35\n",
      "[623 | 433.69] loss=3.90 avg=3.36\n",
      "[624 | 434.20] loss=4.29 avg=3.37\n",
      "[625 | 434.71] loss=3.01 avg=3.37\n",
      "[626 | 435.22] loss=3.84 avg=3.37\n",
      "[627 | 435.72] loss=3.48 avg=3.37\n",
      "[628 | 436.23] loss=4.11 avg=3.38\n",
      "[629 | 436.74] loss=3.48 avg=3.38\n",
      "[630 | 437.25] loss=3.17 avg=3.38\n",
      "[631 | 437.75] loss=3.78 avg=3.38\n",
      "[632 | 438.26] loss=3.15 avg=3.38\n",
      "[633 | 438.77] loss=3.39 avg=3.38\n",
      "[634 | 439.28] loss=3.79 avg=3.38\n",
      "[635 | 439.78] loss=3.13 avg=3.38\n",
      "[636 | 440.29] loss=3.24 avg=3.38\n",
      "[637 | 440.80] loss=3.80 avg=3.38\n",
      "[638 | 441.30] loss=2.88 avg=3.38\n",
      "[639 | 441.81] loss=3.50 avg=3.38\n",
      "[640 | 442.32] loss=3.53 avg=3.38\n",
      "[641 | 442.83] loss=3.69 avg=3.38\n",
      "[642 | 443.33] loss=4.32 avg=3.39\n",
      "[643 | 443.84] loss=3.59 avg=3.40\n",
      "[644 | 444.35] loss=2.81 avg=3.39\n",
      "[645 | 444.86] loss=3.55 avg=3.39\n",
      "[646 | 445.37] loss=2.54 avg=3.38\n",
      "[647 | 445.87] loss=3.10 avg=3.38\n",
      "[648 | 446.38] loss=3.62 avg=3.38\n",
      "[649 | 446.89] loss=2.98 avg=3.38\n",
      "[650 | 447.40] loss=3.20 avg=3.38\n",
      "[651 | 447.90] loss=4.22 avg=3.39\n",
      "[652 | 448.41] loss=3.74 avg=3.39\n",
      "[653 | 448.91] loss=1.92 avg=3.37\n",
      "[654 | 449.42] loss=3.46 avg=3.38\n",
      "[655 | 449.93] loss=2.27 avg=3.36\n",
      "[656 | 450.43] loss=2.24 avg=3.35\n",
      "[657 | 450.94] loss=3.53 avg=3.35\n",
      "[658 | 451.45] loss=2.50 avg=3.35\n",
      "[659 | 451.95] loss=3.74 avg=3.35\n",
      "[660 | 452.46] loss=1.84 avg=3.34\n",
      "[661 | 452.97] loss=3.68 avg=3.34\n",
      "[662 | 453.47] loss=3.53 avg=3.34\n",
      "[663 | 453.98] loss=2.10 avg=3.33\n",
      "[664 | 454.49] loss=3.36 avg=3.33\n",
      "[665 | 454.99] loss=2.79 avg=3.32\n",
      "[666 | 455.50] loss=2.05 avg=3.31\n",
      "[667 | 456.01] loss=4.16 avg=3.32\n",
      "[668 | 456.51] loss=2.84 avg=3.31\n",
      "[669 | 457.02] loss=4.44 avg=3.33\n",
      "[670 | 457.52] loss=3.24 avg=3.32\n",
      "[671 | 458.03] loss=3.12 avg=3.32\n",
      "[672 | 458.54] loss=3.92 avg=3.33\n",
      "[673 | 459.04] loss=3.68 avg=3.33\n",
      "[674 | 459.55] loss=3.90 avg=3.34\n",
      "[675 | 460.06] loss=3.94 avg=3.34\n",
      "[676 | 460.56] loss=3.30 avg=3.34\n",
      "[677 | 461.07] loss=3.59 avg=3.35\n",
      "[678 | 461.57] loss=3.89 avg=3.35\n",
      "[679 | 462.08] loss=3.07 avg=3.35\n",
      "[680 | 462.59] loss=2.99 avg=3.34\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "# set number of steps for finetuning or load the pretrained model\n",
    "# if a model is already mounted, no training required\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "# load an existing, fine-tuned model\n",
    "# gpt2.load_gpt2(sess)\n",
    "#\n",
    "# finetuning frequently fails in collab based on resource exhaustion\n",
    "# Good training duration results in a loss between 0.2 and 0.9 (about 5-10K training iterations)\n",
    "#\n",
    "# GPT-2 can generate coherent output even on low finetuning, but with generic style (default)\n",
    "\n",
    "# fine-tune from scratch\n",
    "\n",
    "gpt2.finetune(sess, style, model_name=model_name, restore_from = 'latest', overwrite=True, steps=8000)\n",
    "\n",
    "# continue training from a saved model\n",
    "#gpt2.finetune(sess, style, model_name=model_name, overwrite=True, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePqv72z4Sn8_"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# take a list of unstructured expansions and tidy them up\n",
    "# expansion formation rules: seed phrase deleted, truncation at first end \n",
    "# of sentence, the last comma, or allow to run off unclipped otherwise\n",
    "# \n",
    "def cleaned(expansions, seed):\n",
    "\n",
    "   seedbag = word_tokenize(seed)\n",
    "   cutseed = dt.detokenize(seedbag[1:])\n",
    "   symlen = len(cutseed)\n",
    "   cleaned = []\n",
    "   expansion = \"\"\n",
    "   for exp in expansions:\n",
    "      pos = exp.find(cutseed)\n",
    "      if pos > 0:\n",
    "         expansion = exp[pos+symlen:].strip()\n",
    "      else:\n",
    "         expansion = exp\n",
    "      pos = expansion.find(\".\")\n",
    "      if pos > 0:\n",
    "         expansion = expansion[:pos]+\".\"\n",
    "      else:\n",
    "         pos = expansion[::-1].find(\",\")\n",
    "         if pos > 0:\n",
    "            expansion = expansion[:-pos]\n",
    "      cleaned.append(expansion)\n",
    "      \n",
    "   return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OL9417-w-dLJ"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# define the next piece to replace in the donor text\n",
    "def generateLexeme(sent):\n",
    "\n",
    "      if sent == \"\":\n",
    "         return\n",
    "\n",
    "      words = word_tokenize(sent)\n",
    "\n",
    "      lexeme = \"\"\n",
    "      count = 0\n",
    "\n",
    "      if len(words) <= maxwords:\n",
    "         processLexeme(sent)\n",
    "\n",
    "      else:\n",
    "         for word in words:\n",
    "            count += 1\n",
    "            if (word in [\";\", \",\",\"-\",\"--\"] and count > minwords) or (word in [\"and\", \"for\", \"at\"] and count > softmaxwords) or count > maxwords:\n",
    "               if (len(words)-count) >  minwords:\n",
    "                  processLexeme(dt.detokenize(words[:count]))\n",
    "                  sentence = dt.detokenize(words[count:])\n",
    "                  generateLexeme(sentence)\n",
    "               else:\n",
    "                  processLexeme(dt.detokenize(words))\n",
    "\n",
    "               break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDmLlzZo-vpj"
   },
   "outputs": [],
   "source": [
    "# @every kernel restart\n",
    "# main RPS routine: extract a lexeme, generate candidates, replace with a best one\n",
    "def processLexeme(sent):\n",
    "\n",
    "   # always start from the first phrase\n",
    "   if not bestLexemes:\n",
    "      bestLexemes.append(sent)\n",
    "      print(\"Leading output with seed phrase: {}\".format(sent))\n",
    "      return\n",
    "    \n",
    "   seed = dt.detokenize(bestLexemes[-seedLexemes:])\n",
    "   print(\"seed: \\\"{}\\\"\".format(seed))\n",
    "   print(\"\\t  processing lexeme: \\\"{}\\\"\".format(sent))\n",
    "   \n",
    "   expansions = gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, return_as_list=True, include_prefix=False)\n",
    "   #expansions = gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, truncate=\".\", return_as_list=True, include_prefix=False)\n",
    "   #expansions += gpt2.generate(sess, nsamples=nsamples, batch_size=10, length=20, prefix=seed, truncate=\",\", return_as_list=True, include_prefix=False)\n",
    "   expansions = cleaned(expansions, seed)\n",
    "   #print(\"  expansion set: {}\".format(expansions))\n",
    "  \n",
    "   messages = [sent]+expansions\n",
    "   bestLexeme = sent\n",
    "  \n",
    "   gc.collect()\n",
    "\n",
    "   with tf.Session(config=config) as session:\n",
    "      session.run(tf.global_variables_initializer())\n",
    "      session.run(tf.tables_initializer())\n",
    "      message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})\n",
    "\n",
    "      corr = np.inner(message_embeddings_, message_embeddings_)\n",
    "      embeddings = corr[0,1:]\n",
    "      #print(\"embeddings: {}\".format(embeddings))\n",
    "      \n",
    "      bestIndex = np.argmax(embeddings)\n",
    "      \n",
    "      if embeddings[bestIndex] >  minsimilarity:\n",
    "        \n",
    "         bestLexeme = expansions[bestIndex]\n",
    "            \n",
    "         # stats       \n",
    "         # calculate running average for accepted scores\n",
    "\n",
    "         acceptedLexemes[\"accepted\"] = acceptedLexemes[\"accepted\"]+1\n",
    "         acceptedLexemes[\"candidate_percent\"].append(np.sum(embeddings > minsimilarity)/nsamples)\n",
    "            \n",
    "         # join expansion with future lexeme smoothly\n",
    "        \n",
    "         originalEnding = sent[-1]\n",
    "         end = originalEnding if originalEnding in [\".\",\",\", \";\", \"-\", \"--\", \"?\", \"!\", \"...\", \"and\",\"for\", \"at\"] else \"\"\n",
    "        \n",
    "         lexemeEnding = bestLexeme[-1]\n",
    "         \n",
    "         # replace punctuation at end of lexeme if original punctuation existed\n",
    "         # if donor sentence not ending yet, ignore the expansion markers\n",
    "         if lexemeEnding in [\".\", \",\", \";\", \"?\", \"!\", \"and\", \"or\", \"for\", \"at\"]:\n",
    "            bestLexeme = bestLexeme[:-1]+end\n",
    "         # trying a plug from the content. Removing this line allows a lot of freedom to style engine.\n",
    "         else:\n",
    "            bestLexeme = bestLexeme+end\n",
    "            \n",
    "      else:\n",
    "         acceptedLexemes[\"rejected\"] = acceptedLexemes[\"rejected\"]+1  \n",
    "        \n",
    "   print(\"  expanding with: \\\"{}\\\" bestscore: {} bestphrase: {}\".format(bestLexeme, embeddings[bestIndex], expansions[bestIndex]))\n",
    "   bestLexemes.append(bestLexeme)\n",
    "\n",
    "   output = dt.detokenize(bestLexemes)\n",
    "   Path(outputfile).write_text(output)\n",
    "    \n",
    "   #stats \n",
    "   acceptedLexemes[\"total_score\"] = acceptedLexemes[\"total_score\"]+embeddings[bestIndex]\n",
    "   print(acceptedLexemes)\n",
    "   Path(statsfile).write_text(str(acceptedLexemes))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1363
    },
    "colab_type": "code",
    "id": "hNj7LbB7JsMU",
    "outputId": "0d2b102d-8d83-4b91-a6bc-40edc04b94e8"
   },
   "outputs": [],
   "source": [
    "# launch RPS from here.\n",
    "# \n",
    "\n",
    "transformed = []\n",
    "\n",
    "for ind, row in data.iterrows():\n",
    "    \n",
    "    outputfile =  outfile + str(ind)\n",
    "    statsfile = sfile + str(ind)\n",
    "    input = row['text']\n",
    "    \n",
    "    bestLexemes = []\n",
    "    acceptedLexemes = {\"accepted\":0, \"rejected\":0, \"total_score\": 0, \"candidate_percent\":[]}\n",
    "\n",
    "    sentences = tokenizer.tokenize(input)\n",
    "\n",
    "    for sent in sentences:\n",
    "       print(\"Input sentence: {}\".format(sent))\n",
    "       generateLexeme(sent)\n",
    "\n",
    "    output = dt.detokenize(bestLexemes)\n",
    "    #Path(outputfile).write_text(output)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(output)\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(input)\n",
    "    transformed.append(output)\n",
    "    np.savetxt(\"processed.csv\", transformed, delimiter=\",\", fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestLexemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import BytesInUse\n",
    "with tf.device('/device:GPU:0'):  # Replace with device you are interested in\n",
    "  bytes_in_use = BytesInUse()\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run(bytes_in_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "RPS.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
